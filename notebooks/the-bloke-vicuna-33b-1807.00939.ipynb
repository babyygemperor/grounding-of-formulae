{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63e50065-dd79-48b6-9bc3-87eb6522a15f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unidecode\n",
      "  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m235.9/235.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tiktoken\n",
      "  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting transformers\n",
      "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting einops\n",
      "  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting auto-gptq\n",
      "  Downloading auto_gptq-0.3.2.tar.gz (63 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m63.5/63.5 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hDiscarding \u001b[4;34mhttps://files.pythonhosted.org/packages/1b/79/5a3a7d877a9b0a72f528e9977ec65cdb9fad800fa4f5110f87f2acaaf6fe/auto_gptq-0.3.2.tar.gz (from https://pypi.org/simple/auto-gptq/) (requires-python:>=3.8.0)\u001b[0m: \u001b[33mRequested auto-gptq from https://files.pythonhosted.org/packages/1b/79/5a3a7d877a9b0a72f528e9977ec65cdb9fad800fa4f5110f87f2acaaf6fe/auto_gptq-0.3.2.tar.gz has inconsistent version: expected '0.3.2', but metadata has '0.3.2+cu118'\u001b[0m\n",
      "  Downloading auto_gptq-0.3.1.tar.gz (63 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m63.5/63.5 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hDiscarding \u001b[4;34mhttps://files.pythonhosted.org/packages/3f/5c/28d57f833498e014e9d066ea0199a503b5e59d4c8e8e701a460223e143da/auto_gptq-0.3.1.tar.gz (from https://pypi.org/simple/auto-gptq/) (requires-python:>=3.8.0)\u001b[0m: \u001b[33mRequested auto-gptq from https://files.pythonhosted.org/packages/3f/5c/28d57f833498e014e9d066ea0199a503b5e59d4c8e8e701a460223e143da/auto_gptq-0.3.1.tar.gz has inconsistent version: expected '0.3.1', but metadata has '0.3.1+cu1180'\u001b[0m\n",
      "  Downloading auto_gptq-0.3.0.tar.gz (62 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting regex>=2022.1.18 (from tiktoken)\n",
      "  Downloading regex-2023.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (770 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m770.4/770.4 kB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.28.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.9.0)\n",
      "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
      "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
      "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m126.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tqdm>=4.27 (from transformers)\n",
      "  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting accelerate>=0.19.0 (from auto-gptq)\n",
      "  Downloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting datasets (from auto-gptq)\n",
      "  Downloading datasets-2.14.3-py3-none-any.whl (519 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m519.1/519.1 kB\u001b[0m \u001b[31m81.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting rouge (from auto-gptq)\n",
      "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (2.0.1+cu118)\n",
      "Collecting peft (from auto-gptq)\n",
      "  Downloading peft-0.4.0-py3-none-any.whl (72 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.19.0->auto-gptq) (5.9.5)\n",
      "Collecting fsspec (from huggingface-hub<1.0,>=0.14.1->transformers)\n",
      "  Downloading fsspec-2023.6.0-py3-none-any.whl (163 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2022.12.7)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (1.11.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (2.0.0)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.13.0->auto-gptq) (3.25.0)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.13.0->auto-gptq) (15.0.7)\n",
      "Collecting pyarrow>=8.0.0 (from datasets->auto-gptq)\n",
      "  Downloading pyarrow-12.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.9 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m38.9/38.9 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting dill<0.3.8,>=0.3.0 (from datasets->auto-gptq)\n",
      "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pandas (from datasets->auto-gptq)\n",
      "  Downloading pandas-2.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m123.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting xxhash (from datasets->auto-gptq)\n",
      "  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multiprocess (from datasets->auto-gptq)\n",
      "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiohttp (from datasets->auto-gptq)\n",
      "  Downloading aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m98.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six in /usr/lib/python3/dist-packages (from rouge->auto-gptq) (1.16.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (23.1.0)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets->auto-gptq)\n",
      "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets->auto-gptq)\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets->auto-gptq)\n",
      "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->datasets->auto-gptq)\n",
      "  Downloading frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (225 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m225.7/225.7 kB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->datasets->auto-gptq)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->auto-gptq) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->auto-gptq) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets->auto-gptq)\n",
      "  Downloading pytz-2023.3-py2.py3-none-any.whl (502 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m502.3/502.3 kB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tzdata>=2022.1 (from pandas->datasets->auto-gptq)\n",
      "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->auto-gptq) (1.2.1)\n",
      "Building wheels for collected packages: auto-gptq\n",
      "  Building wheel for auto-gptq (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for auto-gptq: filename=auto_gptq-0.3.0-cp310-cp310-linux_x86_64.whl size=5664438 sha256=4e7e987e449022a25a112145c129d74293fa0b9cbf5edac908016835d4ec60e5\n",
      "  Stored in directory: /root/.cache/pip/wheels/62/ca/da/632464db8c2071b514d3b659652383c789f53cc2fc917737bb\n",
      "Successfully built auto-gptq\n",
      "Installing collected packages: tokenizers, safetensors, pytz, xxhash, unidecode, tzdata, tqdm, rouge, regex, pyarrow, multidict, fsspec, frozenlist, einops, dill, async-timeout, yarl, tiktoken, pandas, multiprocess, huggingface-hub, aiosignal, transformers, aiohttp, datasets, accelerate, peft, auto-gptq\n",
      "Successfully installed accelerate-0.21.0 aiohttp-3.8.5 aiosignal-1.3.1 async-timeout-4.0.2 auto-gptq-0.3.0 datasets-2.14.3 dill-0.3.7 einops-0.6.1 frozenlist-1.4.0 fsspec-2023.6.0 huggingface-hub-0.16.4 multidict-6.0.4 multiprocess-0.70.15 pandas-2.0.3 peft-0.4.0 pyarrow-12.0.1 pytz-2023.3 regex-2023.6.3 rouge-1.0.1 safetensors-0.3.1 tiktoken-0.4.0 tokenizers-0.13.3 tqdm-4.65.0 transformers-4.31.0 tzdata-2023.3 unidecode-1.3.6 xxhash-3.3.0 yarl-1.9.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install unidecode tiktoken transformers einops auto-gptq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b5a187-7c86-4ea4-863a-55817112cfb5",
   "metadata": {},
   "source": [
    "## Time to install packages 4 mins 0 seconds\n",
    "## Time started: 23:59:00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4255ace1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, NavigableString\n",
    "from unidecode import unidecode\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime\n",
    "import unicodedata\n",
    "import itertools\n",
    "import tiktoken\n",
    "import ast\n",
    "import pprint\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af9777c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The safetensors archive passed at /root/.cache/huggingface/hub/models--TheBloke--Vicuna-33B-1-3-SuperHOT-8K-GPTQ/snapshots/6bd46c2caeac69d4329b48f66bbd05f8546998a8/vicuna-33b-1.3-superhot-8k-GPTQ-4bit--1g.act.order.safetensors does not contain metadata. Make sure to save your model with the `save_pretrained` method. Defaulting to 'pt' metadata.\n",
      "skip module injection for FusedLlamaMLPForQuantizedModel not support integrate without triton yet.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, pipeline, logging\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "import argparse\n",
    "\n",
    "model_name_or_path = \"TheBloke/Vicuna-33B-1-3-SuperHOT-8K-GPTQ\"\n",
    "model_basename = \"vicuna-33b-1.3-superhot-8k-GPTQ-4bit--1g.act.order\"\n",
    "model_name = 'vicuna-33b'\n",
    "file_code = '1807.00939'\n",
    "\n",
    "use_triton = False\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "\n",
    "model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n",
    "        model_basename=model_basename,\n",
    "        use_safetensors=True,\n",
    "        trust_remote_code=True,\n",
    "        device_map='auto',\n",
    "        use_triton=use_triton,\n",
    "        quantize_config=None)\n",
    "\n",
    "model.seqlen = 8192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1f3d52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_messages(prompt_template, model=\"gpt-3.5-turbo\"):\n",
    "    return len(tokenizer(prompt_template, return_tensors='pt').input_ids.cuda().tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "604d244c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_messages(text):\n",
    "    # This is a placeholder for your actual implementation\n",
    "    return text.split(\"\\n\")\n",
    "\n",
    "def split_into_chunks(text, max_tokens=2000):\n",
    "    messages = split_into_messages(text)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_tokens = 0\n",
    "    for message in messages:\n",
    "        message_tokens = num_tokens_from_messages(message, model=\"gpt-3.5-turbo\")\n",
    "        if current_tokens + message_tokens > max_tokens:\n",
    "            # If adding this message would exceed the max tokens, start a new chunk\n",
    "            chunks.append('\\n'.join(current_chunk))\n",
    "            current_chunk = [message]\n",
    "            current_tokens = message_tokens\n",
    "        else:\n",
    "            # Otherwise, add the message to the current chunk\n",
    "            current_chunk.append(message)\n",
    "            current_tokens += message_tokens\n",
    "    # Don't forget the last chunk!\n",
    "    if current_chunk:\n",
    "        chunks.append('\\n'.join(current_chunk))\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e285df1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_string(input_str):\n",
    "    if \"</s>\" in input_str:\n",
    "        return input_str\n",
    "    else:\n",
    "        last_comma_index = input_str.rfind(',')\n",
    "        if last_comma_index == -1:\n",
    "            return input_str  # No comma found, return the string as is\n",
    "        else:\n",
    "            return input_str[:last_comma_index] + '}' + input_str[last_comma_index+1:]\n",
    "        \n",
    "def get_json_of_string(incorrect, pattern=r'\\{[^\\}]*\\}'):\n",
    "    match = re.search(r'{(.*)}', incorrect, re.DOTALL)\n",
    "    if match:\n",
    "        return \"{\" + match.group(1).replace('{', '[').replace('}', ']') + \"}\"\n",
    "    else:\n",
    "        return \"{}\"\n",
    "\n",
    "\n",
    "def string_to_dict(my_string):\n",
    "    # Load the JSON string into a list of tuples\n",
    "    tuples_list = json.JSONDecoder(object_pairs_hook=list).decode(my_string)\n",
    "\n",
    "    # Create a new dictionary to hold the final result\n",
    "    final_dict = {}\n",
    "\n",
    "    # Iterate over the list of tuples\n",
    "    for key, value in tuples_list:\n",
    "        # If the key is already in the final dictionary, append the value\n",
    "        # to the list of values for that key\n",
    "        if key in final_dict:\n",
    "            # Ensure the value is in a list form\n",
    "            if not isinstance(final_dict[key], list):\n",
    "                final_dict[key] = [final_dict[key]]\n",
    "            final_dict[key].append(value)\n",
    "        else:\n",
    "            # If the key is not in the final dictionary, add it with the value\n",
    "            final_dict[key] = value\n",
    "\n",
    "    return final_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53c1858e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dictionaries(dict1, dict2):\n",
    "    union_dict = dict1.copy()\n",
    "\n",
    "    for key, value in dict2.items():\n",
    "        if key in union_dict:\n",
    "            if isinstance(union_dict[key], list):\n",
    "                if value not in union_dict[key]:\n",
    "                    union_dict[key].append(value)\n",
    "            else:\n",
    "                if union_dict[key] != value:\n",
    "                    union_dict[key] = [union_dict[key], value]\n",
    "        else:\n",
    "            union_dict[key] = value\n",
    "\n",
    "    return union_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff36d705",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_from_tags(element):\n",
    "    if isinstance(element, NavigableString):\n",
    "        return element\n",
    "    if element.name == 'mi':\n",
    "        return str(element)\n",
    "    return ''.join(get_text_from_tags(child) for child in element.children)\n",
    "\n",
    "def parse_html(file_path, clean=True):\n",
    "    with open(file_path, 'r', encoding='utf-8') as html_file:\n",
    "        soup = BeautifulSoup(html_file, 'html.parser')\n",
    "\n",
    "    texts = get_text_from_tags(soup)\n",
    "    if clean:\n",
    "        matches = re.findall(r'<mi(.*?)</mi>', texts)\n",
    "        for match in matches:\n",
    "            original_string = f'<mi{match}</mi>'\n",
    "            replaced_string = re.sub(r'<.*?>(.*?)</.*?>', r'<|\\1|>', original_string)\n",
    "            texts = texts.replace(original_string, replaced_string)\n",
    "\n",
    "    return texts\n",
    "\n",
    "# call the function with your HTML file path\n",
    "page = parse_html(f'{file_code}.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb9f95a7-690d-49a1-947e-2d1c107a5f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(prompt):\n",
    "    return f'''SYSTEM: {prompt[0]['content']}\n",
    "USER: {prompt[1]['content']}\n",
    "ASSISTANT: {prompt[2]['content']}\n",
    "USER: {prompt[3]['content']}\n",
    "ASSISTANT: '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d0568d3-d765-4c53-ac27-c67650dad1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_pattern(input_string):\n",
    "    pattern = r\"<\\|[^<\\|>]*\\|>\"\n",
    "    if re.search(pattern, input_string):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f0f85cb-e25a-4a21-81a4-c8c143a47c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_output(output):\n",
    "    pos = output.index('Do not include the angle brackets in the dictionary')\n",
    "    end_of_string = output[pos:]\n",
    "    print(end_of_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abf7ee5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "1080 prompt tokens counted.\n"
     ]
    }
   ],
   "source": [
    "text = page\n",
    "chunks = split_into_chunks(text, max_tokens=512)\n",
    "i = 0\n",
    "\n",
    "while not contains_pattern(chunks[i]):\n",
    "    i += 1\n",
    "print(i)\n",
    "\n",
    "question = chunks[i]\n",
    "\n",
    "actual_total_tokens = 0\n",
    "completion_tokens = 0\n",
    "prompt_tokens = 0\n",
    "\n",
    "prompt = [\n",
    "        {'role': 'system',\n",
    "         'content': 'You are a helpful research assistant tasked with converting long paragraphs into a JSON '\n",
    "                    'dictionary. The goal is to identify and classify each individual mathematical symbol, variable,'\n",
    "                    ' and identifier in the text marked between \"<||>\". The dictionary should store the identifiers as '\n",
    "                    'keys and their corresponding definitions as values in an array format. '},\n",
    "        {'role': 'system', 'name': 'example_user', 'content': '''A relational model is a triple <|M|>‚Ä≤=(<|X|>,<|R|>,\n",
    "        <|v|>), where <|X|> is a set of states, <|R|><|‚äÜ|><|X|><|√ó|><|X|> is a binary relation on <|X|>, \n",
    "        and <|v|>:<|ùñØùóãùóàùóâ|>‚Üí2<|X|> is a valuation. Given a relational model <|M|>‚Ä≤, the satisfaction relation between \n",
    "        points <|x<|‚àà<|X<| and formulas <|œÜ<|‚àà<|‚Ñí<|<|ùñ™ùñ†<| is defined inductively by <|M|>‚Ä≤,<|x|>‚ä®<|ùñ™|><|œÜ|>‚áî for all \n",
    "        <|y|>‚àà<|X|>,<|x|><|R|><|y|> implies <|M|>‚Ä≤,<|y|>‚ä®<|œÜ|><|M|>‚Ä≤,<|x|>‚ä®<|ùñ†|><|œÜ|><||>‚áî for all <|y|>‚àà<|X|>,\n",
    "        <|M|>‚Ä≤,<|y|>‚ä®<|œÜ|>'''},\n",
    "        {'role': 'system', 'name': 'example_assistant', 'content': '''identifiers = {\n",
    "            \"M\": [\"Model\", \"Expertise Model\"],\n",
    "            \"M'\": \"Relational model\",\n",
    "            \"X\": \"Set of states\",\n",
    "            \"R\": \"Binary relation on X\",\n",
    "            \"v\": \"Valuation\",\n",
    "            \"ùñØùóãùóàùóâ\": \"Set of propositions\",\n",
    "            \"M'\": \"Relational model\",\n",
    "            \"x\": \"Point in X\",\n",
    "            \"œÜ\": \"Formula in ùñ™ùñ†\",\n",
    "            \"‚Ñí_{ùñ™ùñ†}\": \"Set of formulas\",\n",
    "            \"ùñ™\": \"Modal operator K\",\n",
    "            \"ùñ†\": \"Modal operator A\",\n",
    "            \"y\": \"Point in X\",\n",
    "            \"‚ä®\": \"Satisfaction relation\",\n",
    "            \"‚áî\": \"If and only if operator\",\n",
    "            \"‚àà\": \"Element of a set\",\n",
    "            \"‚äÜ\": \"Subset of a set\",\n",
    "            \"√ó\": \"Cartesian product operator\",\n",
    "            \"‚Üí\": \"Function or implication operator\",\n",
    "            \"for all\": \"Universal quantifier\"\n",
    "            }'''},\n",
    "        {'role': 'user', 'content': f'Generate a JSON dictionary for the following text\\n```txt\\n{question}```. '\n",
    "                                    'Only consider the mathematical identifiers inside \"<||>\" for the dictionary. '\n",
    "                                    'Do not consider any other identifier other than those marked. Consider all the '\n",
    "                                    'identifiers individually. Do not skip any identifier, mention all the identifiers '\n",
    "                                    'inside \"<||>\" in your dictionary. Do not include the angle brackets in the '\n",
    "                                    'dictionary.'}\n",
    "    ]\n",
    "\n",
    "\n",
    "open_prompt = get_prompt(prompt)\n",
    "\n",
    "prompt_size = num_tokens_from_messages(open_prompt)\n",
    "print(f\"{prompt_size} prompt tokens counted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "612640fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TheBloke/Vicuna-33B-1-3-SuperHOT-8K-GPTQ\n",
      "Do not include the angle brackets in the dictionary.\n",
      "ASSISTANT:  identifiers = {\n",
      "\"LSTM RNN\": [\"LSTM Recurrent Neural Network\", \"List of Steps and Tasks\"],\n",
      "\"predicted data\": [\"Output from LSTM RNN\", \"Product of predicted values\"],\n",
      "\"ANOMALOUS\": [\"Algorithm for anomaly detection\", \"Academic Institution\"],\n",
      "\"multidimensional array\": [\"Data structure for multiple dimensions\", \"Array of arrays\"],\n",
      "\"NCC\": [\"Normalized Cross Correlation\", \"Quantity of correlation\"],\n",
      "\"Corr\": [\"Cross-correlation, short form\", \"Academic Institution\"],\n",
      "\"x\": [\"First signal\", \"Individual item in array\"],\n",
      "\"y\": [\"Second signal\", \"Individual item in array\"],\n",
      "\"N\": [\"Length of signals\", \"Total number of items\"],\n",
      "\"‚àë\": [\"Summation operator\", \"Add up all values\"],\n",
      "\"‚Å¢\": [\"Equals sign\", \"Assignment operator\"],\n",
      "\"‚Å¢\": [\"Equals sign\", \"Assignment operator\"],\n",
      "\"‚Å°\": [\"Equals sign\", \"Assignment operator\"],\n",
      "\"[\": [\"Left bracket\", \"Start of array or list\"],\n",
      "\"]\": [\"Right bracket\", \"End of array or list\"],\n",
      "\"<\": [\"Less than or equal to\", \"Comparison operator\"],\n",
      "\"1\": [\"Single value\", \"Single item\"],\n",
      "\"1\": [\"Lowest value\", \"Minimum bound\"],\n",
      "\"1\": [\"Highest value\", \"Maximum bound\"],\n",
      "\"\": [\"Written in capital letters\", \"Mathematical constant\"]\n",
      "}</s>\n",
      "Time taken: 0:00:22.802466\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using {model_name_or_path}\")\n",
    "start_time = datetime.now()\n",
    "\n",
    "input_ids = tokenizer(open_prompt, return_tensors='pt').input_ids.cuda()\n",
    "output = model.generate(inputs=input_ids, temperature=0.5, max_new_tokens=512, repetition_penalty=1.05)\n",
    "output_string = tokenizer.decode(output[0])\n",
    "print_output(output_string)\n",
    "\n",
    "total_time_taken = datetime.now() - start_time\n",
    "print(f\"Time taken: {total_time_taken}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "507e7993",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1429 349 1080\n"
     ]
    }
   ],
   "source": [
    "actual_total_tokens += num_tokens_from_messages(output_string)\n",
    "completion_tokens += num_tokens_from_messages(output_string) - num_tokens_from_messages(open_prompt)\n",
    "prompt_tokens += num_tokens_from_messages(open_prompt)\n",
    "print(actual_total_tokens, completion_tokens, prompt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ff56fd8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Safely convert the dictionary string to a dictionary using json.loads()\n",
    "try:\n",
    "    ind = output_string.index('Do not include the angle brackets in the dictionary.')\n",
    "    correct_output_string = modify_string(output_string[ind:])\n",
    "    dic_output_string = get_json_of_string(correct_output_string)\n",
    "    dictionary = [string_to_dict(dic_output_string)]\n",
    "except Exception as e:\n",
    "    dictionary = [{}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "acbf76ad-2acb-41dc-be3d-aab9c19c15c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'LSTM RNN': ['LSTM Recurrent Neural Network', 'List of Steps and Tasks'], 'predicted data': ['Output from LSTM RNN', 'Product of predicted values'], 'ANOMALOUS': ['Algorithm for anomaly detection', 'Academic Institution'], 'multidimensional array': ['Data structure for multiple dimensions', 'Array of arrays'], 'NCC': ['Normalized Cross Correlation', 'Quantity of correlation'], 'Corr': ['Cross-correlation, short form', 'Academic Institution'], 'x': ['First signal', 'Individual item in array'], 'y': ['Second signal', 'Individual item in array'], 'N': ['Length of signals', 'Total number of items'], '‚àë': ['Summation operator', 'Add up all values'], '\\u2062': ['Equals sign', 'Assignment operator', ['Equals sign', 'Assignment operator']], '\\u2061': ['Equals sign', 'Assignment operator'], '[': ['Left bracket', 'Start of array or list'], ']': ['Right bracket', 'End of array or list'], '<': ['Less than or equal to', 'Comparison operator'], '1': ['Single value', 'Single item', ['Lowest value', 'Minimum bound'], ['Highest value', 'Maximum bound']], '': ['Written in capital letters', 'Mathematical constant']}]\n"
     ]
    }
   ],
   "source": [
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d18ab90-38c2-4724-9a03-239fc73078d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_loop(prompt):\n",
    "    return f'''SYSTEM: {prompt[0]['content']}\n",
    "USER: {prompt[1]['content']}\n",
    "ASSISTANT: {prompt[2]['content']}\n",
    "SYSTEM: {prompt[3]['content']}\n",
    "USER: {prompt[4]['content']}\n",
    "ASSISTANT: '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5bab7543",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17 of 27\n",
      "Iteration 18 of 27\n",
      "Iteration 19 of 27\n",
      "Iteration 20 of 27\n",
      "Iteration 21 of 27\n",
      "Iteration 22 of 27\n",
      "Iteration 23 of 27\n",
      "Iteration 24 of 27\n",
      "Iteration 25 of 27\n",
      "Iteration 26 of 27\n",
      "Iteration 27 of 27\n",
      "Iteration 28 of 27\n",
      "Iteration 29 of 27\n",
      "Iteration 30 of 27\n",
      "Iteration 31 of 27\n",
      "Iteration 32 of 27\n",
      "Iteration 33 of 27\n",
      "Iteration 34 of 27\n",
      "Iteration 35 of 27\n",
      "\n",
      "\n",
      "\n",
      "1547 prompt tokens counted.\n",
      "\n",
      "Do not include the angle brackets in your dictionary.\n",
      "ASSISTANT: \n",
      "identifiers = {\n",
      "\"x\": \"Vectors of time series data\",\n",
      "\"y\": \"Vectors of time series data\",\n",
      "\"N\": \"Number of days in the series\",\n",
      "\"Corr\": \"Cross-correlation\",\n",
      "\"NCC\": \"Normalized Cross Correlation\",\n",
      "\"‚àë\": \"Summation operator\",\n",
      "\"n\": \"Index for loop\",\n",
      "\"x\": \"First signal\",\n",
      "\"y\": \"Second signal\",\n",
      "\"N\": \"Length of signals\",\n",
      "\"=\": \"Equals sign\",\n",
      "\"xcorr\": \"Built-in feature in Matlab for cross-correlation\",\n",
      "\"coeff\": \"Parameter for normalization\"\n",
      "}</s>\n",
      "502 2627\n",
      "Actual total tokens till now: 3129\n",
      "Iteration 36 of 27\n",
      "\n",
      "\n",
      "\n",
      "1652 prompt tokens counted.\n",
      "\n",
      "Do not include the angle brackets in your dictionary.\n",
      "ASSISTANT:  {\n",
      "\"window\\_size\": [\"Window size\", \"Size of the sliding window\"],\n",
      "\"w\": [\"Number of windows\", \"Number of repetitions\"],\n",
      "\"d\": [\"Shift value\", \"Distance between windows\"],\n",
      "\"NCR\": [\"Normalized Cross Correlation\", \"Quantity of correlation\"],\n",
      "\"day\": [\"Index for loop\", \"Day when NCR is highest\"],\n",
      "\"window\\_based\": [\"Time series based on windows\"],\n",
      "\"day\\_based\": [\"Time series based on days\"],\n",
      "\"fig\": [\"Figure\", \"Illustration or graph\"],\n",
      "\"160\": [\"Resulting day\", \"160th day from the beginning of the entire time series\"],\n",
      "\"2\": [\"Number of windows\", \"Repetition factor\"],\n",
      "\"50\": [\"Window size\", Size of the sliding window\"],\n",
      "\"1\": [\"Incremental value\", \"Day when NCR is highest\"],\n",
      "\"9\": [\"Anomalous time series\", \"Unusual time series\"],\n",
      "\"12\": [\"All figures\", \"All graphs\"],\n",
      "\"9\": [\"Correlation value\", \"Highest correlation achieved\"],\n",
      "\"8\": [\"Correlation value\", \"Similarity between time series\"],\n",
      "\"7\": [\"Time series\", \"Series of data\"],\n",
      "\"1\": [\"Highest correlation value\", \"Maximum correlation\"],\n",
      "\"formulated\": [\"Formulated equation\", \"Calculated relationship\"],\n",
      "\"3\": [\"Equation\", \"Formula used\"],\n",
      "\"p\": [\"Pattern number\", \"Category of time series\"],\n",
      "\"\">\": [\"Greater than or equal to\", \"Comparison operator\"],\n",
      "\"=\": [\"Equal to\", \"Assignment operator\"],\n",
      "\"<\": [\"Less than or equal to\", \"Comparison operator\"]\n",
      "}</s>\n",
      "883 4279\n",
      "Actual total tokens till now: 5162\n",
      "INCORRECT DICTIONARY\n",
      "Iteration 37 of 27\n",
      "Iteration 38 of 27\n",
      "Iteration 39 of 27\n",
      "Iteration 40 of 27\n",
      "Iteration 41 of 27\n",
      "Iteration 42 of 27\n",
      "Iteration 43 of 27\n",
      "Time taken: 0:00:36.495260\n",
      "Total time taken: 0:00:59.297647\n",
      "5162 883 4279\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.now()\n",
    "number_of_dictionaries = 0\n",
    "for chunk in chunks:\n",
    "    print(f\"Iteration {i} of {len(chunks)}\")\n",
    "    i += 1\n",
    "    if chunk == question:\n",
    "        continue\n",
    "    if not contains_pattern(chunk):\n",
    "        continue\n",
    "    question = chunk\n",
    "    \n",
    "    if prompt_size > 1600:\n",
    "        number_of_dictionaries += 1\n",
    "        print(\"\\nNew dictionary\\n\")\n",
    "        dictionary.append({})\n",
    "    \n",
    "    prompt = [\n",
    "            {'role': 'system',\n",
    "             'content': 'You are a helpful research assistant tasked with converting long paragraphs into a JSON '\n",
    "                        'dictionary. '\n",
    "                        'The goal is to identify and classify each individual mathematical symbol, variable, '\n",
    "                        'and identifier in the text marked between \"<||>\"'\n",
    "                        'The dictionary should store the identifiers as keys and their corresponding definitions as '\n",
    "                        'values in an array format. '},\n",
    "            {'role': 'system', 'name': 'example_user', 'content': '''A relational model is a triple <|M|>‚Ä≤=(<|X|>,<|R|>,\n",
    "            <|v|>), where <|X|> is a set of states, <|R|><|‚äÜ|><|X|><|√ó|><|X|> is a binary relation on <|X|>, \n",
    "            and <|v|>:<|ùñØùóãùóàùóâ|>‚Üí2<|X|> is a valuation. Given a relational model <|M|>‚Ä≤, the satisfaction relation \n",
    "            between points <|x<|‚àà<|X<| and formulas <|œÜ<|‚àà<|‚Ñí<|<|ùñ™ùñ†<| is defined inductively by <|M|>‚Ä≤,\n",
    "            <|x|>‚ä®<|ùñ™|><|œÜ|>‚áî for all <|y|>‚àà<|X|>,<|x|><|R|><|y|> implies <|M|>‚Ä≤,<|y|>‚ä®<|œÜ|><|M|>‚Ä≤,\n",
    "            <|x|>‚ä®<|ùñ†|><|œÜ|><||>‚áî for all <|y|>‚àà<|X|>,<|M|>‚Ä≤,<|y|>‚ä®<|œÜ|>'''},\n",
    "            {'role': 'system', 'name': 'example_assistant', 'content': '''identifiers = {\n",
    "            \"M\": [\"Model\", \"Expertise Model\"],\n",
    "            \"M'\": \"Relational model\",\n",
    "            \"X\": \"Set of states\",\n",
    "            \"R\": \"Binary relation on X\",\n",
    "            \"v\": \"Valuation\",\n",
    "            \"ùñØùóãùóàùóâ\": \"Set of propositions\",\n",
    "            \"M'\": \"Relational model\",\n",
    "            \"x\": \"Point in X\",\n",
    "            \"œÜ\": \"Formula in ùñ™ùñ†\",\n",
    "            \"‚Ñí_{ùñ™ùñ†}\": \"Set of formulas\",\n",
    "            \"ùñ™\": \"Modal operator K\",\n",
    "            \"ùñ†\": \"Modal operator A\",\n",
    "            \"y\": \"Point in X\",\n",
    "            \"‚ä®\": \"Satisfaction relation\",\n",
    "            \"‚áî\": \"If and only if operator\",\n",
    "            \"‚àà\": \"Element of a set\",\n",
    "            \"‚äÜ\": \"Subset of a set\",\n",
    "            \"√ó\": \"Cartesian product operator\",\n",
    "            \"‚Üí\": \"Function or implication operator\",\n",
    "            \"for all\": \"Universal quantifier\"\n",
    "            }'''},\n",
    "            {'role': 'system',\n",
    "             'content': f'Given is already a pre existing dictionary. Your job is to extend this dictionary. Do not '\n",
    "                        f'remove any pre existing definitions from this dictionary.'\n",
    "                        f'\\n{dictionary[number_of_dictionaries]}. If there is nothing to mention, reply with an empty '\n",
    "                        f'dictionary'},\n",
    "            {'role': 'user', 'content': f'Generate a JSON dictionary for the following text: {question}. '\n",
    "                                        'Only consider the mathematical identifiers inside \"<||>\" for the dictionary. '\n",
    "                                        'Do not consider any other identifier other than those marked. '\n",
    "                                        'Consider all the identifiers individually. Do not skip any identifier, mention'\n",
    "                                        ' all the identifiers inside \"<||>\" in your dictionary. '\n",
    "                                        'Do not include the angle brackets in your dictionary.'}\n",
    "        ]\n",
    "    \n",
    "    open_prompt = get_prompt_loop(prompt)\n",
    "    \n",
    "    prompt_size = num_tokens_from_messages(open_prompt)\n",
    "    print(f\"\\n\\n\\n{prompt_size} prompt tokens counted.\\n\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            input_ids = tokenizer(open_prompt, return_tensors='pt').input_ids.cuda()\n",
    "            output = model.generate(inputs=input_ids, temperature=0.5, max_new_tokens=512, repetition_penalty=1.05)\n",
    "            output_string = tokenizer.decode(output[0])\n",
    "            \n",
    "            actual_total_tokens += num_tokens_from_messages(output_string)\n",
    "            completion_tokens += num_tokens_from_messages(output_string) - num_tokens_from_messages(open_prompt)\n",
    "            prompt_tokens += num_tokens_from_messages(open_prompt)\n",
    "\n",
    "            ind = output_string.index('Do not include the angle brackets in your dictionary.')\n",
    "            print(output_string[ind:])\n",
    "            \n",
    "            print(completion_tokens, prompt_tokens)\n",
    "    \n",
    "            print(f\"Actual total tokens till now: {actual_total_tokens}\")\n",
    "\n",
    "            try:\n",
    "                correct_output_string = modify_string(output_string[ind:])\n",
    "                dic_output_string = get_json_of_string(correct_output_string)\n",
    "                new_dictionary = string_to_dict(dic_output_string)\n",
    "            except Exception as e:\n",
    "                print(\"INCORRECT DICTIONARY\")\n",
    "            dictionary[number_of_dictionaries] = merge_dictionaries(dictionary[number_of_dictionaries], new_dictionary)\n",
    "            \n",
    "            break\n",
    "        except Exception as e:\n",
    "            number_of_dictionaries += 1\n",
    "            dictionary.append({})\n",
    "            print(f\"Exception occurred: {e}\")\n",
    "            print(\"Retrying...\")\n",
    "total_time_taken += (datetime.now() - start_time)\n",
    "print(f\"Time taken: {datetime.now() - start_time }\")\n",
    "print(f\"Total time taken: {total_time_taken}\")\n",
    "print(actual_total_tokens, completion_tokens, prompt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04278ee2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dct = {}\n",
    "for dic in dictionary:\n",
    "    dct = merge_dictionaries(dct, dic)\n",
    "# pprint.pprint(dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d4212dbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'LSTM RNN': '['LSTM Recurrent Neural Network', 'List of Steps and Tasks']'\n",
      "'predicted data': '['Output from LSTM RNN', 'Product of predicted values']'\n",
      "'ANOMALOUS': '['Algorithm for anomaly detection', 'Academic Institution']'\n",
      "'multidimensional array': '['Data structure for multiple dimensions', 'Array of arrays']'\n",
      "'NCC': '['Normalized Cross Correlation', 'Quantity of correlation']'\n",
      "'Corr': '['Cross-correlation, short form', 'Academic Institution', 'Cross-correlation']'\n",
      "'x': '['First signal', 'Individual item in array', ['Vectors of time series data', 'First signal']]'\n",
      "'y': '['Second signal', 'Individual item in array', ['Vectors of time series data', 'Second signal']]'\n",
      "'N': '['Length of signals', 'Total number of items', ['Number of days in the series', 'Length of signals']]'\n",
      "'‚àë': '['Summation operator', 'Add up all values']'\n",
      "'‚Å¢': '['Equals sign', 'Assignment operator', ['Equals sign', 'Assignment operator']]'\n",
      "'‚Å°': '['Equals sign', 'Assignment operator']'\n",
      "'[': '['Left bracket', 'Start of array or list']'\n",
      "']': '['Right bracket', 'End of array or list']'\n",
      "'<': '['Less than or equal to', 'Comparison operator']'\n",
      "'1': '['Single value', 'Single item', ['Lowest value', 'Minimum bound'], ['Highest value', 'Maximum bound']]'\n",
      "'': '['Written in capital letters', 'Mathematical constant']'\n"
     ]
    }
   ],
   "source": [
    "for key, value in dct.items():\n",
    "    if type(value) == list:\n",
    "        print(f\"'{key}': '{value}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed252bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_list(input_list):\n",
    "    output_list = []\n",
    "    for i in input_list:\n",
    "        if isinstance(i, list):\n",
    "            output_list.extend(flatten_list(i))\n",
    "        else:\n",
    "            output_list.append(i)\n",
    "    return output_list\n",
    "\n",
    "\n",
    "def remove_duplicates(input_list):\n",
    "    output_list = []\n",
    "    for item in input_list:\n",
    "        if item not in output_list:\n",
    "            output_list.append(item)\n",
    "    if len(output_list) == 1:\n",
    "        return output_list[0]\n",
    "    return output_list\n",
    "\n",
    "\n",
    "def process_value(v):\n",
    "    if isinstance(v, str):\n",
    "        new_v = v.replace('$', '')\n",
    "        while '\\\\\\\\' in new_v:\n",
    "            new_v = new_v.replace('\\\\\\\\', '\\\\').replace('\\n', '')\n",
    "    else:  # Assuming it's a list\n",
    "        new_v = flatten_list([process_value(val) for val in v])\n",
    "        \n",
    "    return remove_duplicates(new_v) if isinstance(new_v, list) else new_v\n",
    "\n",
    "\n",
    "def reduce_pairs(dictionary):\n",
    "    new_dict = {}\n",
    "    for k, v in dictionary.items():\n",
    "        # reduce key backslashes\n",
    "        new_k = k.replace('$', '')\n",
    "        while '\\\\\\\\' in new_k:\n",
    "            new_k = new_k.replace('\\\\\\\\', '\\\\')\n",
    "\n",
    "        # process value\n",
    "        new_v = process_value(v)\n",
    "\n",
    "        new_dict[new_k] = new_v\n",
    "\n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "85c8083f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dict_without_backslashes = reduce_pairs(dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ab472ef2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pprint.pprint(dict_without_backslashes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "09b426f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_json = dict_without_backslashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0dec9c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{file_code}_mcdict.json', 'r', encoding='utf-8') as f:\n",
    "    mc_dict_original = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4284b146",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to create a hex code (a binary representation of the key)\n",
    "def get_hex_code(key):\n",
    "    return key.encode().hex()\n",
    "\n",
    "mc_dict_original['_author'] = model_name_or_path\n",
    "\n",
    "# Iterate over your dictionary and fill the new one\n",
    "for key, values in parsed_json.items():\n",
    "    # Determine the base key and the affix\n",
    "    base_key = re.match(r\"^[^*'_^,(\\[]*\", key).group()\n",
    "    affix = key[len(base_key):]\n",
    "\n",
    "    hex_code = get_hex_code(base_key)\n",
    "    values = values if isinstance(values, list) else [values]\n",
    "\n",
    "    if hex_code in mc_dict_original[\"concepts\"]:\n",
    "        k = list(mc_dict_original[\"concepts\"][hex_code][\"identifiers\"].keys())[0]\n",
    "        new_identifier = []\n",
    "        for value in values:\n",
    "            mc_dict_original[\"concepts\"][hex_code][\"identifiers\"][k].append({\n",
    "                \"affixes\": [affix] if affix else [],\n",
    "                \"arity\": 0,\n",
    "                \"description\": value\n",
    "            })\n",
    "    else:\n",
    "        if hex_code not in mc_dict_original[\"concepts\"]:\n",
    "            mc_dict_original[\"concepts\"][hex_code] = {\n",
    "                \"_surface\": {\n",
    "                    \"text\": base_key,\n",
    "                    \"unicode_name\": base_key if len(base_key) != 1 else unicodedata.name(base_key)\n",
    "                },\n",
    "                \"identifiers\": {\n",
    "                    'default': []\n",
    "                }\n",
    "            }\n",
    "\n",
    "        for value in values:\n",
    "            mc_dict_original[\"concepts\"][hex_code][\"identifiers\"][\"default\"].append({\n",
    "                \"affixes\": [affix] if affix else [],\n",
    "                \"arity\": 0,\n",
    "                \"description\": value\n",
    "            })\n",
    "\n",
    "\n",
    "# Convert new dictionary to a sorted dictionary\n",
    "sorted_dict = dict(sorted(mc_dict_original[\"concepts\"].items(), key=lambda x: (len(x[0]), x[0])))\n",
    "mc_dict_original[\"concepts\"] = sorted_dict\n",
    "\n",
    "# Convert new dictionary to JSON\n",
    "json_str = json.dumps(mc_dict_original, indent=4, ensure_ascii=False)\n",
    "\n",
    "#print(json_str)\n",
    "\n",
    "with open(f'{file_code}-{model_name}_mcdict.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(mc_dict_original, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a79ec2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_from_tags(element):\n",
    "    if isinstance(element, NavigableString):\n",
    "        return element\n",
    "    if element.name == 'mi':\n",
    "        return str(element)\n",
    "    return ''.join(get_text_from_tags(child) for child in element.children)\n",
    "\n",
    "def parse_html(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as html_file:\n",
    "        soup = BeautifulSoup(html_file, 'html.parser')\n",
    "\n",
    "    texts = get_text_from_tags(soup)\n",
    "    return texts\n",
    "\n",
    "def find_mi_strings(text):\n",
    "    pattern = r'(<mi.*?</mi>)'\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "    return matches\n",
    "\n",
    "# call the function with your HTML file path\n",
    "page = parse_html(f'{file_code}.html')\n",
    "matches = find_mi_strings(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "64a0cba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_dict = mc_dict_original\n",
    "with open(f'{file_code}_anno.json', encoding='utf-8') as fp:\n",
    "    parsed_annotation = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "163356dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_index_from_char_index(message, key, char_index):\n",
    "    i = 0\n",
    "    index = -1\n",
    "    for word in message:\n",
    "        if key in word:\n",
    "            index = i\n",
    "        i += 1\n",
    "    return index\n",
    "\n",
    "def expand_string_to_tokens(message, index, num_tokens_right=25, num_tokens_left=75):\n",
    "    words = message.split()  # Split the message into words\n",
    "\n",
    "    # Start at the index where the center word is\n",
    "    left_index = right_index = index\n",
    "\n",
    "    tokens_counter_right = num_tokens_from_messages(words[right_index])\n",
    "    tokens_counter_left = num_tokens_from_messages(words[left_index])\n",
    "\n",
    "    # Expand to the left from the center index until you reach num_tokens_left\n",
    "    while tokens_counter_left < num_tokens_left and left_index > 0:\n",
    "        left_index -= 1\n",
    "        tokens_counter_left += num_tokens_from_messages(words[left_index])\n",
    "\n",
    "    # Expand to the right from the center index until you reach num_tokens_right\n",
    "    while tokens_counter_right < num_tokens_right and right_index < len(words) - 1:\n",
    "        right_index += 1\n",
    "        tokens_counter_right += num_tokens_from_messages(words[right_index])\n",
    "\n",
    "    # Combine the words back into a string and return\n",
    "    return ' '.join(words[left_index:right_index + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "01b6b971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_text(text, replacement, exception):\n",
    "    # Find all matches\n",
    "    matches = re.findall(r'<mi(.*?)</mi>', text)\n",
    "    \n",
    "    for match in matches:\n",
    "        original_string = f'<mi{match}</mi>'\n",
    "        \n",
    "        # Skip exception\n",
    "        if original_string == exception:\n",
    "            continue\n",
    "        \n",
    "        # Replace match\n",
    "        replaced_string = re.sub(r'<.*?>(.*?)</.*?>', r'\\1', original_string)\n",
    "        text = text.replace(original_string, replaced_string)\n",
    "\n",
    "    return text\n",
    "\n",
    "def get_context(match):\n",
    "    match_len = len(match)\n",
    "    new_page = replace_text(page, '', match)\n",
    "    char_index = new_page.index(match) + int(match_len/2)\n",
    "    word_index = get_word_index_from_char_index(new_page, char_index)\n",
    "    section = expand_string_to_tokens(new_page, word_index)\n",
    "    section = re.sub(r'<.*?>(.*?)</.*?>', r'<<\\1>>', section)\n",
    "    return match, section\n",
    "\n",
    "# Function to create a hex code (a binary representation of the key)\n",
    "def get_hex_code(key):\n",
    "    return key.encode('utf-8').hex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7706ec86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_trailing_tags(s):\n",
    "    parts = re.split('(<mi)', s)\n",
    "    for i in range(1, len(parts), 2):\n",
    "        if '>' not in parts[i + 1]:\n",
    "            parts[i] = ''\n",
    "            parts[i + 1] = ''\n",
    "    return ''.join(parts)\n",
    "\n",
    "def get_definition_of_id(dict_id, identifier):\n",
    "    \n",
    "    try:\n",
    "        hex_code = get_hex_code(identifier)\n",
    "        index = parsed_annotation['mi_anno'][dict_id]['concept_id']\n",
    "        key = list(parsed_dict['concepts'][hex_code]['identifiers'].keys())[0]\n",
    "        return f\"({parsed_dict['concepts'][hex_code]['identifiers'][key][index]['description']})\"\n",
    "    except Exception as e:\n",
    "        return \"\"\n",
    "\n",
    "def get_context(match):\n",
    "    key_word = page.index(match) + len(match)\n",
    "    last_index = min(len(page), key_word + 500)\n",
    "    first_index = max(0, key_word - 3000)\n",
    "    context_window = page[first_index:last_index]\n",
    "    \n",
    "    reg_matches = re.findall(r'<mi(.*?)</mi>', context_window)\n",
    "    \n",
    "    identifier = None\n",
    "    \n",
    "    for reg_match in reg_matches:\n",
    "        original_string = f'<mi{reg_match}</mi>'\n",
    "        soup = BeautifulSoup(original_string, 'html.parser')\n",
    "        \n",
    "        \n",
    "        tags = soup.find_all('mi')\n",
    "        \n",
    "        if original_string == match:\n",
    "            identifier = tags[0].text\n",
    "            continue\n",
    "\n",
    "        context_window = context_window.replace(original_string,\n",
    "                                                f\"{tags[0].text}{get_definition_of_id(tags[0].get('id'), tags[0].text)}\")\n",
    "    \n",
    "    context_window = re.sub(r'<mi.*?>(.*?)<\\/mi>', r'<<\\1>>', context_window)\n",
    "    \n",
    "    context_window = remove_trailing_tags(context_window)\n",
    "    context_window = re.sub(r'^(?!.*<mi.*).*<\\/mi>', '', context_window, flags=re.DOTALL)\n",
    "        \n",
    "    index = 0\n",
    "    for word in context_window.split():\n",
    "        if f\"<<{identifier}>>\" in word:\n",
    "            word_index = index\n",
    "        index += 1\n",
    "    \n",
    "    if word_index == -1:\n",
    "        return context_window\n",
    "    else:\n",
    "        context_window = expand_string_to_tokens(context_window, word_index)\n",
    "    return context_window\n",
    "\n",
    "#print(get_context('<mi id=\"S1.p2.1.m1.1.1.3\" xref=\"S1.p2.1.m1.1.1.3.cmml\">œÜ</mi>'))\n",
    "#get_context('<mi id=\"S1.p2.1.m1.1.1.2\" xref=\"S1.p2.1.m1.1.1.2.cmml\">ùñ§</mi>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "62ec5036-7177-46d0-9728-571edb9f6bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_anno(prompt):\n",
    "    return f'''SYSTEM: {prompt[0]['content']}\n",
    "USER: {prompt[1]['content']}\n",
    "ASSISTANT:'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f57696ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 of 35: ASSISTANT: 2</s>\n",
      "6 290\n",
      "2\n",
      "Iteration 2 of 35: ASSISTANT: 2</s>\n",
      "12 577\n",
      "2\n",
      "Iteration 3 of 35: ASSISTANT: I am a professional annotater API, and I will select the fitting annotation from the provided dictionary for the mathematical identifier \"y\" based on the given text.\n",
      "\n",
      "Identifier: y\n",
      "Possible annotations:\n",
      "[\n",
      "{\n",
      "\"index\": 0,\n",
      "\"identifier\": \"y\",\n",
      "\"description\": \"Second signal\"\n",
      "},\n",
      "{\n",
      "\"index\": 1,\n",
      "\"identifier\": \"y\",\n",
      "\"description\": \"Individual item in array\"\n",
      "},\n",
      "{\n",
      "\"index\": 2,\n",
      "\"identifier\": \"y\",\n",
      "\"description\": \"Vectors of time series data\"\n",
      "}\n",
      "]\n",
      "\n",
      "Based on the text: (-1 to 1). The formula for the cross-correlation is: Corr(Cross-correlation)x(Vectors of time series data),<<y>>=‚àën=0N-1x‚Å¢[n]‚Å¢y‚Å¢[n]\n",
      "\n",
      "\n",
      "The most fitting description is: \"Vectors of time series data\" (index: 2)\n",
      "\n",
      "However, if we consider the context of the equation and the fact that correlation is usually applied to time series data, it would make more sense to use the \"Individual item in array\" (index: 1) annotation. So, I'll go with the latter:\n",
      "\n",
      "Index: 1</s>\n",
      "304 864\n",
      "0\n",
      "Iteration 4 of 35: 0\n",
      "Iteration 5 of 35: ASSISTANT: Based on the provided text and possible annotations, the most fitting description for the identifier \"N\" is:\n",
      "\n",
      "index: 1, identifier: N, description: Total number of items</s>\n",
      "346 1151\n",
      "1\n",
      "Iteration 6 of 35: ASSISTANT: 2</s>\n",
      "352 1446\n",
      "2\n",
      "Iteration 7 of 35: 0\n",
      "Iteration 8 of 35: ASSISTANT: 2</s>\n",
      "358 1751\n",
      "2\n",
      "Iteration 9 of 35: 0\n",
      "Iteration 10 of 35: None\n",
      "Iteration 11 of 35: ASSISTANT: 2</s>\n",
      "364 2036\n",
      "2\n",
      "Iteration 12 of 35: ASSISTANT: 2</s>\n",
      "370 2322\n",
      "2\n",
      "Iteration 13 of 35: 0\n",
      "Iteration 14 of 35: ASSISTANT: 2</s>\n",
      "376 2610\n",
      "2\n",
      "Iteration 15 of 35: ASSISTANT: 2</s>\n",
      "382 2899\n",
      "2\n",
      "Iteration 16 of 35: 0\n",
      "Iteration 17 of 35: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"y\" is \"Individual item in array\". Therefore, the index is 1.</s>\n",
      "417 3194\n",
      "1\n",
      "Iteration 18 of 35: 0\n",
      "Iteration 19 of 35: 0\n",
      "Iteration 20 of 35: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"N\" is \"Number of days in the series\". Therefore, the index for this annotation is 2.</s>\n",
      "456 3483\n",
      "2\n",
      "Iteration 21 of 35: ASSISTANT: 2</s>\n",
      "462 3771\n",
      "2\n",
      "Iteration 22 of 35: 0\n",
      "Iteration 23 of 35: ASSISTANT: Based on the provided text, the most fitting description for the identifier <<x>> is index 2, which represents \"Vectors of time series data\".</s>\n",
      "497 4066\n",
      "2\n",
      "Iteration 24 of 35: 0\n",
      "Iteration 25 of 35: 0\n",
      "Iteration 26 of 35: ASSISTANT: Based on the provided text and possible annotations, the most fitting description for the identifier N is:\n",
      "\n",
      "index: 2, identifier: N, description: Number of days in the series</s>\n",
      "539 4353\n",
      "2\n",
      "Iteration 27 of 35: ASSISTANT: 2</s>\n",
      "545 4647\n",
      "2\n",
      "Iteration 28 of 35: 0\n",
      "Iteration 29 of 35: ASSISTANT: 2</s>\n",
      "551 4939\n",
      "2\n",
      "Iteration 30 of 35: 0\n",
      "Iteration 31 of 35: None\n",
      "Iteration 32 of 35: None\n",
      "Iteration 33 of 35: None\n",
      "Iteration 34 of 35: None\n",
      "Iteration 35 of 35: None\n",
      "Annotation completed\n",
      "Time taken: 0:00:48.098384\n",
      "Total time taken: 0:00:48.098334\n",
      "5490 551 4939\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.now()\n",
    "actual_total_tokens = 0\n",
    "completion_tokens = 0\n",
    "prompt_tokens = 0\n",
    "no_tags = 0\n",
    "no_keys = 0\n",
    "no_anno = 0\n",
    "i = 1\n",
    "for match in matches:\n",
    "    print(f\"Iteration {i} of {len(matches)}: \", end='')\n",
    "    i += 1\n",
    "    context = get_context(match)\n",
    "    match_variable = re.sub(r'<.*?>(.*?)</.*?>', r'\\1', match)\n",
    "    context_index = context.index(f\"<<{match_variable}>>\") + len(match_variable)\n",
    "    possible_affix = str(context[context_index+4:context_index+5]).replace(\"‚Ä≤\", \"'\")\n",
    "    soup = BeautifulSoup(match, 'html.parser')\n",
    "    mi_tag = soup.find('mi')\n",
    "    if mi_tag is not None and 'id' in mi_tag.attrs:\n",
    "        anno_id = mi_tag['id']\n",
    "    else:\n",
    "        print('TAG NOT FOUND', match)\n",
    "        no_tags += 1\n",
    "        continue\n",
    "    \n",
    "    hex_code = get_hex_code(match_variable)\n",
    "    if hex_code not in parsed_dict['concepts']:\n",
    "        match_variable = f\"{unidecode(match_variable)}\"\n",
    "        hex_code = get_hex_code(match_variable)\n",
    "        if hex_code not in parsed_dict['concepts']:\n",
    "            print(\"Key does not exist in the dictionary of concepts\", match_variable, hex_code)\n",
    "            no_keys += 1\n",
    "            continue\n",
    "    \n",
    "    if anno_id not in parsed_annotation['mi_anno']:\n",
    "        print(\"Annotation ID does not exist in annotation.json\", anno_id)\n",
    "        no_anno += 1\n",
    "        continue\n",
    "\n",
    "    k = list(parsed_dict[\"concepts\"][hex_code][\"identifiers\"].keys())[0]\n",
    "    mcdict = parsed_dict['concepts'][hex_code]['identifiers'][k]\n",
    "    \n",
    "    if len(mcdict) == 1:\n",
    "        parsed_annotation['mi_anno'][anno_id]['concept_id'] = 0\n",
    "        print('0')\n",
    "    elif len(mcdict) > 1:\n",
    "        prompt_mcdict = []\n",
    "\n",
    "        index = 0\n",
    "        for val in mcdict:\n",
    "            prompt_mcdict.append({'index': f\"{index}\", 'identifier': f\"{match_variable}{'' if len(val['affixes']) == 0 else val['affixes'][0]}\", 'description': val['description']})\n",
    "            index += 1\n",
    "            \n",
    "        prompt = [\n",
    "            {'role': 'system', 'content': 'You are a professional annotater API. Your job is to select a fitting annotation from a dictionary for a mathematical identifier.'},\n",
    "            {'role': 'user', 'content': f'''Given the following possible annotations:\\n```json\\n{prompt_mcdict}```.\n",
    "             Select the index for the most fitting description for the identifier <<{match_variable}>> from the following text.\n",
    "             The potential affix of the indentifier could be <<{possible_affix}>>. Take the affixes of the possible annotations into account.\n",
    "             Only return the value of the index and nothing else.\n",
    "             Do not add any explanation otherwise the API breaks.\n",
    "             The identifier has been marked with <<>>.\n",
    "             If you can't come up with an index, write 'None'\n",
    "             ```txt\n",
    "             {context}\n",
    "             ```'''}\n",
    "        ]\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                open_prompt = get_prompt_anno(prompt)\n",
    "                \n",
    "                input_ids = tokenizer(open_prompt, return_tensors='pt').input_ids.cuda()\n",
    "                output = model.generate(inputs=input_ids, temperature=0.5, max_new_tokens=512, repetition_penalty=1.05)\n",
    "                output_string = tokenizer.decode(output[0])\n",
    "                \n",
    "                actual_total_tokens += num_tokens_from_messages(output_string)\n",
    "                completion_tokens += num_tokens_from_messages(output_string) - num_tokens_from_messages(open_prompt)\n",
    "                prompt_tokens += num_tokens_from_messages(open_prompt)\n",
    "\n",
    "                ind = output_string.index('ASSISTANT:')\n",
    "                value = output_string[ind:]\n",
    "                print(value)\n",
    "                \n",
    "                print(completion_tokens, prompt_tokens)\n",
    "\n",
    "                try:\n",
    "                    index = int(int(re.search('\\d+', value).group()))\n",
    "                    print(index)\n",
    "                    parsed_annotation['mi_anno'][anno_id]['concept_id'] = index\n",
    "                except Exception as f:\n",
    "                    print(f)\n",
    "\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Exception occurred\\n{e}\")\n",
    "                print(\"Retrying...\")\n",
    "    else:\n",
    "        print('None')\n",
    "\n",
    "print('Annotation completed')\n",
    "\n",
    "    \n",
    "total_time_taken = (datetime.now() - start_time)\n",
    "print(f\"Time taken: {datetime.now() - start_time }\")\n",
    "print(f\"Total time taken: {total_time_taken}\")\n",
    "print(actual_total_tokens, completion_tokens, prompt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bfeab45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_annotation['_annotator'] = model_name_or_path\n",
    "with open(f'{file_code}-{model_name}_anno.json', 'w') as fp:\n",
    "    json.dump(parsed_annotation, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9180c363",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    }
   ],
   "source": [
    "items = 0\n",
    "for key, value in parsed_annotation['mi_anno'].items():\n",
    "    if value['concept_id'] is not None:\n",
    "        #print(key, value)\n",
    "        items += 1\n",
    "print(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffe4c27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
