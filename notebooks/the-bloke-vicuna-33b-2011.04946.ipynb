{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63e50065-dd79-48b6-9bc3-87eb6522a15f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unidecode\n",
      "  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.9/235.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tiktoken\n",
      "  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting transformers\n",
      "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting einops\n",
      "  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting auto-gptq\n",
      "  Downloading auto_gptq-0.3.2.tar.gz (63 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.5/63.5 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hDiscarding \u001b[4;34mhttps://files.pythonhosted.org/packages/1b/79/5a3a7d877a9b0a72f528e9977ec65cdb9fad800fa4f5110f87f2acaaf6fe/auto_gptq-0.3.2.tar.gz (from https://pypi.org/simple/auto-gptq/) (requires-python:>=3.8.0)\u001b[0m: \u001b[33mRequested auto-gptq from https://files.pythonhosted.org/packages/1b/79/5a3a7d877a9b0a72f528e9977ec65cdb9fad800fa4f5110f87f2acaaf6fe/auto_gptq-0.3.2.tar.gz has inconsistent version: expected '0.3.2', but metadata has '0.3.2+cu118'\u001b[0m\n",
      "  Downloading auto_gptq-0.3.1.tar.gz (63 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.5/63.5 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hDiscarding \u001b[4;34mhttps://files.pythonhosted.org/packages/3f/5c/28d57f833498e014e9d066ea0199a503b5e59d4c8e8e701a460223e143da/auto_gptq-0.3.1.tar.gz (from https://pypi.org/simple/auto-gptq/) (requires-python:>=3.8.0)\u001b[0m: \u001b[33mRequested auto-gptq from https://files.pythonhosted.org/packages/3f/5c/28d57f833498e014e9d066ea0199a503b5e59d4c8e8e701a460223e143da/auto_gptq-0.3.1.tar.gz has inconsistent version: expected '0.3.1', but metadata has '0.3.1+cu1180'\u001b[0m\n",
      "  Downloading auto_gptq-0.3.0.tar.gz (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting regex>=2022.1.18 (from tiktoken)\n",
      "  Downloading regex-2023.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (770 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m770.4/770.4 kB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.28.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.9.0)\n",
      "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
      "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
      "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m126.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tqdm>=4.27 (from transformers)\n",
      "  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting accelerate>=0.19.0 (from auto-gptq)\n",
      "  Downloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting datasets (from auto-gptq)\n",
      "  Downloading datasets-2.14.3-py3-none-any.whl (519 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.1/519.1 kB\u001b[0m \u001b[31m81.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting rouge (from auto-gptq)\n",
      "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (2.0.1+cu118)\n",
      "Collecting peft (from auto-gptq)\n",
      "  Downloading peft-0.4.0-py3-none-any.whl (72 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.19.0->auto-gptq) (5.9.5)\n",
      "Collecting fsspec (from huggingface-hub<1.0,>=0.14.1->transformers)\n",
      "  Downloading fsspec-2023.6.0-py3-none-any.whl (163 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2022.12.7)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (1.11.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (2.0.0)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.13.0->auto-gptq) (3.25.0)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.13.0->auto-gptq) (15.0.7)\n",
      "Collecting pyarrow>=8.0.0 (from datasets->auto-gptq)\n",
      "  Downloading pyarrow-12.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.9/38.9 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting dill<0.3.8,>=0.3.0 (from datasets->auto-gptq)\n",
      "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pandas (from datasets->auto-gptq)\n",
      "  Downloading pandas-2.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m123.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting xxhash (from datasets->auto-gptq)\n",
      "  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multiprocess (from datasets->auto-gptq)\n",
      "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiohttp (from datasets->auto-gptq)\n",
      "  Downloading aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m98.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six in /usr/lib/python3/dist-packages (from rouge->auto-gptq) (1.16.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (23.1.0)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets->auto-gptq)\n",
      "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets->auto-gptq)\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets->auto-gptq)\n",
      "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->datasets->auto-gptq)\n",
      "  Downloading frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (225 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.7/225.7 kB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->datasets->auto-gptq)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->auto-gptq) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->auto-gptq) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets->auto-gptq)\n",
      "  Downloading pytz-2023.3-py2.py3-none-any.whl (502 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.3/502.3 kB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tzdata>=2022.1 (from pandas->datasets->auto-gptq)\n",
      "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->auto-gptq) (1.2.1)\n",
      "Building wheels for collected packages: auto-gptq\n",
      "  Building wheel for auto-gptq (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for auto-gptq: filename=auto_gptq-0.3.0-cp310-cp310-linux_x86_64.whl size=5664438 sha256=4e7e987e449022a25a112145c129d74293fa0b9cbf5edac908016835d4ec60e5\n",
      "  Stored in directory: /root/.cache/pip/wheels/62/ca/da/632464db8c2071b514d3b659652383c789f53cc2fc917737bb\n",
      "Successfully built auto-gptq\n",
      "Installing collected packages: tokenizers, safetensors, pytz, xxhash, unidecode, tzdata, tqdm, rouge, regex, pyarrow, multidict, fsspec, frozenlist, einops, dill, async-timeout, yarl, tiktoken, pandas, multiprocess, huggingface-hub, aiosignal, transformers, aiohttp, datasets, accelerate, peft, auto-gptq\n",
      "Successfully installed accelerate-0.21.0 aiohttp-3.8.5 aiosignal-1.3.1 async-timeout-4.0.2 auto-gptq-0.3.0 datasets-2.14.3 dill-0.3.7 einops-0.6.1 frozenlist-1.4.0 fsspec-2023.6.0 huggingface-hub-0.16.4 multidict-6.0.4 multiprocess-0.70.15 pandas-2.0.3 peft-0.4.0 pyarrow-12.0.1 pytz-2023.3 regex-2023.6.3 rouge-1.0.1 safetensors-0.3.1 tiktoken-0.4.0 tokenizers-0.13.3 tqdm-4.65.0 transformers-4.31.0 tzdata-2023.3 unidecode-1.3.6 xxhash-3.3.0 yarl-1.9.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install unidecode tiktoken transformers einops auto-gptq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b5a187-7c86-4ea4-863a-55817112cfb5",
   "metadata": {},
   "source": [
    "## Time to install packages 4 mins 0 seconds\n",
    "## Time started: 23:59:00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4255ace1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, NavigableString\n",
    "from unidecode import unidecode\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime\n",
    "import unicodedata\n",
    "import itertools\n",
    "import tiktoken\n",
    "import ast\n",
    "import pprint\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af9777c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The safetensors archive passed at /root/.cache/huggingface/hub/models--TheBloke--Vicuna-33B-1-3-SuperHOT-8K-GPTQ/snapshots/6bd46c2caeac69d4329b48f66bbd05f8546998a8/vicuna-33b-1.3-superhot-8k-GPTQ-4bit--1g.act.order.safetensors does not contain metadata. Make sure to save your model with the `save_pretrained` method. Defaulting to 'pt' metadata.\n",
      "skip module injection for FusedLlamaMLPForQuantizedModel not support integrate without triton yet.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, pipeline, logging\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "import argparse\n",
    "\n",
    "model_name_or_path = \"TheBloke/Vicuna-33B-1-3-SuperHOT-8K-GPTQ\"\n",
    "model_basename = \"vicuna-33b-1.3-superhot-8k-GPTQ-4bit--1g.act.order\"\n",
    "model_name = 'vicuna-33b'\n",
    "file_code = '2011.04946'\n",
    "\n",
    "use_triton = False\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "\n",
    "model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n",
    "        model_basename=model_basename,\n",
    "        use_safetensors=True,\n",
    "        trust_remote_code=True,\n",
    "        device_map='auto',\n",
    "        use_triton=use_triton,\n",
    "        quantize_config=None)\n",
    "\n",
    "model.seqlen = 8192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1f3d52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_messages(prompt_template, model=\"gpt-3.5-turbo\"):\n",
    "    return len(tokenizer(prompt_template, return_tensors='pt').input_ids.cuda().tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "604d244c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_messages(text):\n",
    "    # This is a placeholder for your actual implementation\n",
    "    return text.split(\"\\n\")\n",
    "\n",
    "def split_into_chunks(text, max_tokens=2000):\n",
    "    messages = split_into_messages(text)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_tokens = 0\n",
    "    for message in messages:\n",
    "        message_tokens = num_tokens_from_messages(message, model=\"gpt-3.5-turbo\")\n",
    "        if current_tokens + message_tokens > max_tokens:\n",
    "            # If adding this message would exceed the max tokens, start a new chunk\n",
    "            chunks.append('\\n'.join(current_chunk))\n",
    "            current_chunk = [message]\n",
    "            current_tokens = message_tokens\n",
    "        else:\n",
    "            # Otherwise, add the message to the current chunk\n",
    "            current_chunk.append(message)\n",
    "            current_tokens += message_tokens\n",
    "    # Don't forget the last chunk!\n",
    "    if current_chunk:\n",
    "        chunks.append('\\n'.join(current_chunk))\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e285df1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_string(input_str):\n",
    "    if \"</s>\" in input_str:\n",
    "        return input_str\n",
    "    else:\n",
    "        last_comma_index = input_str.rfind(',')\n",
    "        if last_comma_index == -1:\n",
    "            return input_str  # No comma found, return the string as is\n",
    "        else:\n",
    "            return input_str[:last_comma_index] + '}' + input_str[last_comma_index+1:]\n",
    "        \n",
    "def get_json_of_string(incorrect, pattern=r'\\{[^\\}]*\\}'):\n",
    "    match = re.search(r'{(.*)}', incorrect, re.DOTALL)\n",
    "    if match:\n",
    "        return \"{\" + match.group(1).replace('{', '[').replace('}', ']') + \"}\"\n",
    "    else:\n",
    "        return \"{}\"\n",
    "\n",
    "\n",
    "def string_to_dict(my_string):\n",
    "    # Load the JSON string into a list of tuples\n",
    "    tuples_list = json.JSONDecoder(object_pairs_hook=list).decode(my_string)\n",
    "\n",
    "    # Create a new dictionary to hold the final result\n",
    "    final_dict = {}\n",
    "\n",
    "    # Iterate over the list of tuples\n",
    "    for key, value in tuples_list:\n",
    "        # If the key is already in the final dictionary, append the value\n",
    "        # to the list of values for that key\n",
    "        if key in final_dict:\n",
    "            # Ensure the value is in a list form\n",
    "            if not isinstance(final_dict[key], list):\n",
    "                final_dict[key] = [final_dict[key]]\n",
    "            final_dict[key].append(value)\n",
    "        else:\n",
    "            # If the key is not in the final dictionary, add it with the value\n",
    "            final_dict[key] = value\n",
    "\n",
    "    return final_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53c1858e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dictionaries(dict1, dict2):\n",
    "    union_dict = dict1.copy()\n",
    "\n",
    "    for key, value in dict2.items():\n",
    "        if key in union_dict:\n",
    "            if isinstance(union_dict[key], list):\n",
    "                if value not in union_dict[key]:\n",
    "                    union_dict[key].append(value)\n",
    "            else:\n",
    "                if union_dict[key] != value:\n",
    "                    union_dict[key] = [union_dict[key], value]\n",
    "        else:\n",
    "            union_dict[key] = value\n",
    "\n",
    "    return union_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff36d705",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_from_tags(element):\n",
    "    if isinstance(element, NavigableString):\n",
    "        return element\n",
    "    if element.name == 'mi':\n",
    "        return str(element)\n",
    "    return ''.join(get_text_from_tags(child) for child in element.children)\n",
    "\n",
    "def parse_html(file_path, clean=True):\n",
    "    with open(file_path, 'r', encoding='utf-8') as html_file:\n",
    "        soup = BeautifulSoup(html_file, 'html.parser')\n",
    "\n",
    "    texts = get_text_from_tags(soup)\n",
    "    if clean:\n",
    "        matches = re.findall(r'<mi(.*?)</mi>', texts)\n",
    "        for match in matches:\n",
    "            original_string = f'<mi{match}</mi>'\n",
    "            replaced_string = re.sub(r'<.*?>(.*?)</.*?>', r'<|\\1|>', original_string)\n",
    "            texts = texts.replace(original_string, replaced_string)\n",
    "\n",
    "    return texts\n",
    "\n",
    "# call the function with your HTML file path\n",
    "page = parse_html(f'{file_code}.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb9f95a7-690d-49a1-947e-2d1c107a5f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(prompt):\n",
    "    return f'''SYSTEM: {prompt[0]['content']}\n",
    "USER: {prompt[1]['content']}\n",
    "ASSISTANT: {prompt[2]['content']}\n",
    "USER: {prompt[3]['content']}\n",
    "ASSISTANT: '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d0568d3-d765-4c53-ac27-c67650dad1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_pattern(input_string):\n",
    "    pattern = r\"<\\|[^<\\|>]*\\|>\"\n",
    "    if re.search(pattern, input_string):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f0f85cb-e25a-4a21-81a4-c8c143a47c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_output(output):\n",
    "    pos = output.index('Do not include the angle brackets in the dictionary')\n",
    "    end_of_string = output[pos:]\n",
    "    print(end_of_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abf7ee5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1263 prompt tokens counted.\n"
     ]
    }
   ],
   "source": [
    "text = page\n",
    "chunks = split_into_chunks(text, max_tokens=512)\n",
    "i = 0\n",
    "\n",
    "while not contains_pattern(chunks[i]):\n",
    "    i += 1\n",
    "print(i)\n",
    "\n",
    "question = chunks[i]\n",
    "\n",
    "actual_total_tokens = 0\n",
    "completion_tokens = 0\n",
    "prompt_tokens = 0\n",
    "\n",
    "prompt = [\n",
    "        {'role': 'system',\n",
    "         'content': 'You are a helpful research assistant tasked with converting long paragraphs into a JSON '\n",
    "                    'dictionary. The goal is to identify and classify each individual mathematical symbol, variable,'\n",
    "                    ' and identifier in the text marked between \"<||>\". The dictionary should store the identifiers as '\n",
    "                    'keys and their corresponding definitions as values in an array format. '},\n",
    "        {'role': 'system', 'name': 'example_user', 'content': '''A relational model is a triple <|M|>′=(<|X|>,<|R|>,\n",
    "        <|v|>), where <|X|> is a set of states, <|R|><|⊆|><|X|><|×|><|X|> is a binary relation on <|X|>, \n",
    "        and <|v|>:<|𝖯𝗋𝗈𝗉|>→2<|X|> is a valuation. Given a relational model <|M|>′, the satisfaction relation between \n",
    "        points <|x<|∈<|X<| and formulas <|φ<|∈<|ℒ<|<|𝖪𝖠<| is defined inductively by <|M|>′,<|x|>⊨<|𝖪|><|φ|>⇔ for all \n",
    "        <|y|>∈<|X|>,<|x|><|R|><|y|> implies <|M|>′,<|y|>⊨<|φ|><|M|>′,<|x|>⊨<|𝖠|><|φ|><||>⇔ for all <|y|>∈<|X|>,\n",
    "        <|M|>′,<|y|>⊨<|φ|>'''},\n",
    "        {'role': 'system', 'name': 'example_assistant', 'content': '''identifiers = {\n",
    "            \"M\": [\"Model\", \"Expertise Model\"],\n",
    "            \"M'\": \"Relational model\",\n",
    "            \"X\": \"Set of states\",\n",
    "            \"R\": \"Binary relation on X\",\n",
    "            \"v\": \"Valuation\",\n",
    "            \"𝖯𝗋𝗈𝗉\": \"Set of propositions\",\n",
    "            \"M'\": \"Relational model\",\n",
    "            \"x\": \"Point in X\",\n",
    "            \"φ\": \"Formula in 𝖪𝖠\",\n",
    "            \"ℒ_{𝖪𝖠}\": \"Set of formulas\",\n",
    "            \"𝖪\": \"Modal operator K\",\n",
    "            \"𝖠\": \"Modal operator A\",\n",
    "            \"y\": \"Point in X\",\n",
    "            \"⊨\": \"Satisfaction relation\",\n",
    "            \"⇔\": \"If and only if operator\",\n",
    "            \"∈\": \"Element of a set\",\n",
    "            \"⊆\": \"Subset of a set\",\n",
    "            \"×\": \"Cartesian product operator\",\n",
    "            \"→\": \"Function or implication operator\",\n",
    "            \"for all\": \"Universal quantifier\"\n",
    "            }'''},\n",
    "        {'role': 'user', 'content': f'Generate a JSON dictionary for the following text\\n```txt\\n{question}```. '\n",
    "                                    'Only consider the mathematical identifiers inside \"<||>\" for the dictionary. '\n",
    "                                    'Do not consider any other identifier other than those marked. Consider all the '\n",
    "                                    'identifiers individually. Do not skip any identifier, mention all the identifiers '\n",
    "                                    'inside \"<||>\" in your dictionary. Do not include the angle brackets in the '\n",
    "                                    'dictionary.'}\n",
    "    ]\n",
    "\n",
    "\n",
    "open_prompt = get_prompt(prompt)\n",
    "\n",
    "prompt_size = num_tokens_from_messages(open_prompt)\n",
    "print(f\"{prompt_size} prompt tokens counted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "612640fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TheBloke/Vicuna-33B-1-3-SuperHOT-8K-GPTQ\n",
      "Do not include the angle brackets in the dictionary.\n",
      "ASSISTANT: \n",
      "identifiers = {\n",
      "\"Yian\": [\"Yian Zhang\", \"Author\"],\n",
      "\"Alex\": [\"Alex Warstadt\", \"Author\"],\n",
      "\"Haau-Sing\": [\"Haau-Sing Li\", \"Author\"],\n",
      "\"Samuel\": [\"Samuel R. Bowman\", \"Author\"],\n",
      "\"Dept. of Computer Science\": [\"Department of Computer Science\"],\n",
      "\"Dept. of Linguistics\": [\"Department of Linguistics\"],\n",
      "\"Center for Data Science\": [\"Center for Data Science\"],\n",
      "\"New York University\": [\"New York University\"],\n",
      "\"yian.zhang\": [\"Yian Zhang's email address\"],\n",
      "\"warstadt\": [\"Alex Warstadt's email address\"],\n",
      "\"xl3119\": [\"Haau-Sing Li's email address\"],\n",
      "\"bowman\": [\"Samuel R. Bowman's email address\"],\n",
      "\"RoBERTa\": [\"Robust Berkeley Transformers\"],\n",
      "\"MiniBERTa\": [\"Miniature BERTas\"],\n",
      "\"1M\": [\"1 million words\"],\n",
      "\"10M\": [\"10 million words\"],\n",
      "\"100M\": [\"1 hundred million words\"],\n",
      "\"1B\": [\"1 billion words\"],\n",
      "\"UltraBERTa\": [\"Ultra Large Model\"],\n",
      "\"probing\": [\"Probing Methods\"],\n",
      "\"classifier\": [\"Classifier Probing\"],\n",
      "\"information-theoretic\": [\"Information-Theoretic Probing\"],\n",
      "\"acceptability\": [\"Unsupervised Relative Acceptability Judgment\"],\n",
      "\"fine-tuning\": [\"Fine-tuning on NLU Tasks\"],\n",
      "\"representations\": [\"Learned Representations\"],\n",
      "\"syntactic\": [\"Syntactic Features\"],\n",
      "\"semantic\": [\"Semantic Features\"],\n",
      "\"commonsense\": [\"Commonsense Knowledge\"],\n",
      "\"downstream\": [\"Downstream NLU Tasks\"],\n",
      "\"encoding\": [\"Encoding Linguistic Features\"],\n",
      "\"improvement\": [\"Improvements in Language Understanding\"],\n",
      "\"necessary\": [\"Necessary for Language Understanding\"],\n",
      "\"driver\": [\"Major Driver of Improvements\"],\n",
      "\"curve\": [\"Learning Curves\"],\n",
      "\"growth\": [\"Growth with Respect to Pretraining Data\n",
      "Time taken: 0:00:31.548315\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using {model_name_or_path}\")\n",
    "start_time = datetime.now()\n",
    "\n",
    "input_ids = tokenizer(open_prompt, return_tensors='pt').input_ids.cuda()\n",
    "output = model.generate(inputs=input_ids, temperature=0.5, max_new_tokens=512, repetition_penalty=1.05)\n",
    "output_string = tokenizer.decode(output[0])\n",
    "print_output(output_string)\n",
    "\n",
    "total_time_taken = datetime.now() - start_time\n",
    "print(f\"Time taken: {total_time_taken}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "507e7993",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1776 513 1263\n"
     ]
    }
   ],
   "source": [
    "actual_total_tokens += num_tokens_from_messages(output_string)\n",
    "completion_tokens += num_tokens_from_messages(output_string) - num_tokens_from_messages(open_prompt)\n",
    "prompt_tokens += num_tokens_from_messages(open_prompt)\n",
    "print(actual_total_tokens, completion_tokens, prompt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ff56fd8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Safely convert the dictionary string to a dictionary using json.loads()\n",
    "try:\n",
    "    ind = output_string.index('Do not include the angle brackets in the dictionary.')\n",
    "    correct_output_string = modify_string(output_string[ind:])\n",
    "    dic_output_string = get_json_of_string(correct_output_string)\n",
    "    dictionary = [string_to_dict(dic_output_string)]\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    dictionary = [{}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "acbf76ad-2acb-41dc-be3d-aab9c19c15c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Yian': ['Yian Zhang', 'Author'], 'Alex': ['Alex Warstadt', 'Author'], 'Haau-Sing': ['Haau-Sing Li', 'Author'], 'Samuel': ['Samuel R. Bowman', 'Author'], 'Dept. of Computer Science': ['Department of Computer Science'], 'Dept. of Linguistics': ['Department of Linguistics'], 'Center for Data Science': ['Center for Data Science'], 'New York University': ['New York University'], 'yian.zhang': [\"Yian Zhang's email address\"], 'warstadt': [\"Alex Warstadt's email address\"], 'xl3119': [\"Haau-Sing Li's email address\"], 'bowman': [\"Samuel R. Bowman's email address\"], 'RoBERTa': ['Robust Berkeley Transformers'], 'MiniBERTa': ['Miniature BERTas'], '1M': ['1 million words'], '10M': ['10 million words'], '100M': ['1 hundred million words'], '1B': ['1 billion words'], 'UltraBERTa': ['Ultra Large Model'], 'probing': ['Probing Methods'], 'classifier': ['Classifier Probing'], 'information-theoretic': ['Information-Theoretic Probing'], 'acceptability': ['Unsupervised Relative Acceptability Judgment'], 'fine-tuning': ['Fine-tuning on NLU Tasks'], 'representations': ['Learned Representations'], 'syntactic': ['Syntactic Features'], 'semantic': ['Semantic Features'], 'commonsense': ['Commonsense Knowledge'], 'downstream': ['Downstream NLU Tasks'], 'encoding': ['Encoding Linguistic Features'], 'improvement': ['Improvements in Language Understanding'], 'necessary': ['Necessary for Language Understanding'], 'driver': ['Major Driver of Improvements'], 'curve': ['Learning Curves']}]\n"
     ]
    }
   ],
   "source": [
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d18ab90-38c2-4724-9a03-239fc73078d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_loop(prompt):\n",
    "    return f'''SYSTEM: {prompt[0]['content']}\n",
    "USER: {prompt[1]['content']}\n",
    "ASSISTANT: {prompt[2]['content']}\n",
    "SYSTEM: {prompt[3]['content']}\n",
    "USER: {prompt[4]['content']}\n",
    "ASSISTANT: '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5bab7543",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 of 34\n",
      "Iteration 1 of 34\n",
      "\n",
      "\n",
      "\n",
      "1549 prompt tokens counted.\n",
      "\n",
      "Do not include the angle brackets in your dictionary.\n",
      "ASSISTANT:  mathematicians = {\n",
      "\"Yian Zhang\": [\"Yian Zhang\", \"Author\"],\n",
      "\"Alex Warstadt\": [\"Alex Warstadt\", \"Author\"],\n",
      "\"Haau-Sing Li\": [\"Haau-Sing Li\", \"Author\"],\n",
      "\"Samuel R. Bowman\": [\"Samuel R. Bowman\", \"Author\"],\n",
      "\"Dept. of Computer Science\": [\"Department of Computer Science\"],\n",
      "\"Dept. of Linguistics\": [\"Department of Linguistics\"],\n",
      "\"Center for Data Science\": [\"Center for Data Science\"],\n",
      "\"New York University\": [\"New York University\"],\n",
      "\"yian.zhang\": [\"Yian Zhang's email address\"],\n",
      "\"warstadt\": [\"Alex Warstadt's email address\"],\n",
      "\"xl3119\": [\"Haau-Sing Li's email address\"],\n",
      "\"bowman\": [\"Samuel R. Bowman's email address\"]\n",
      "}</s>\n",
      "718 2812\n",
      "Actual total tokens till now: 3530\n",
      "Iteration 2 of 34\n",
      "\n",
      "\n",
      "\n",
      "1713 prompt tokens counted.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2226 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do not include the angle brackets in your dictionary.\n",
      "ASSISTANT:  mathematicians = {\n",
      "\"Belinkov\": [\"Belinkov, S.A. & Glass, J.M.\", \"Authors\"],\n",
      "\"Glass\": [\"Glass, J.M.\", \"Author\"],\n",
      "\"Tenney\": [\"Tenney, A.R. et al.\", \"Authors\"],\n",
      "\"Rogers\": [\"Rogers, J. et al.\", \"Authors\"],\n",
      "\"Ettinger\": [\"Ettinger, M.J.\", \"Author\"],\n",
      "\"Hart\": [\"Hart, B. & Risley, T.R.\", \"Authors\"],\n",
      "\"1M\": [\"1 million words\"],\n",
      "\"10M\": [\"10 million words\"],\n",
      "\"100M\": [\"1 hundred million words\"],\n",
      "\"1B\": [\"1 billion words\"],\n",
      "\"Liu\": [\"Liu, Y. et al.\", \"Authors\"],\n",
      "\"MiniBERTas\": [\"MiniBERTas\", \"Models\"],\n",
      "\"RoBERTa\": [\"RoBERTa\", \"Model\"],\n",
      "\"RoBERTa<||>BASE\": [\"RoBERTa<||>BASE\", \"Model\"],\n",
      "\"probing\": [\"Probing Methods\"],\n",
      "\"grammatical\": [\"Grammatical Features\"],\n",
      "\"linguistic\": [\"Linguistic Phenomena\"],\n",
      "\"pretraining\": [\"Pretraining Data Volume\"],\n",
      "\"improvement\": [\"Improvements in Language Understanding\"],\n",
      "\"necessary\": [\"Necessary for Language Understanding\"],\n",
      "\"data-rich\": [\"Data-Rich Models\"],\n",
      "\"less\": [\"Less Pretraining Data\"],\n",
      "\"unanswered\": [\"Unanswered Questions\"],\n",
      "\"30B\": [\"30 billion words\"],\n",
      "\"10M\": [\"10 million words\"],\n",
      "\"100M\": [\"1 hundred million words\"],\n",
      "\"1B\": [\"1 billion words\"],\n",
      "\"learned\": [\"Learned Grammatical Features\"],\n",
      "\"learned\": [\"Learned Linguistic Phenomena\"],\n",
      "\"input\": [\"Input to Human Learners\"],\n",
      "\"improvement\": [\"Improvement in Language Understanding\"],\n",
      "\"expect\": [\"Expectation of Improvement in Language Understanding\"],\n",
      "\"MiniBERTas\": [\"MiniBERTas\", \"\n",
      "1231 4525\n",
      "Actual total tokens till now: 5756\n",
      "INCORRECT DICTIONARY Expecting ',' delimiter: line 34 column 28 (char 1339)\n",
      "Iteration 3 of 34\n",
      "\n",
      "New dictionary\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1269 prompt tokens counted.\n",
      "\n",
      "Do not include the angle brackets in your dictionary.\n",
      "ASSISTANT: 📌 Mathematical identifiers inside \"<||>\" are:\n",
      "```json\n",
      "{\"+\": \"Plus operator\",\n",
      "\"-\": \"Minus operator\",\n",
      "\"*\": \"Multiply operator\",\n",
      "\"/\": \"Divide operator\",\n",
      "\"=\": \"Equal to operator\",\n",
      "\">\": \"Greater than operator\",\n",
      "<\": \"Less than operator\",\n",
      "\"10\": \"Number\",\n",
      "\"27\": \"Number\",\n",
      "\"36\": \"Number\",\n",
      "\"54\": \"Number\",\n",
      "\"71\": \"Number\",\n",
      "\"84\": \"Number\",\n",
      "\"124\": \"Number\",\n",
      "\"165\": \"Number\",\n",
      "\"216\": \"Number\",\n",
      "\"334\": \"Number\",\n",
      "\"466\": \"Number\",\n",
      "\"663\": \"Number\",\n",
      "\"996\": \"Number\",\n",
      "\"118\": \"Number\",\n",
      "\"196\": \"Number\",\n",
      "\"384\": \"Number\",\n",
      "\"626\": \"Number\",\n",
      "\"1136\": \"Number\",\n",
      "\"2197\": \"Number\",\n",
      "\"3558\": \"Number\",\n",
      "\"5712\": \"Number\",\n",
      "\"9268\": \"Number\",\n",
      "\"14754\": \"Number\",\n",
      "\"24431\": \"Number\",\n",
      "\"56621\": \"Number\",\n",
      "\"12666\": \"Number\",\n",
      "\"22686\": \"Number\",\n",
      "\"54683\": \"Number\",\n",
      "\"16617\": \"Number\",\n",
      "\"26666\": \"Number\",\n",
      "\"56666\": \"Number\",\n",
      "\"19617\": \"Number\",\n",
      "\"36666\": \"Number\",\n",
      "\"57826\": \"Number\",\n",
      "\"12666\": \"Number\",\n",
      "\"31688\": \"Number\",\n",
      "\"54683\": \"Number\",\n",
      "\"16617\": \"Number\",\n",
      "\"26666\": \"Number\",\n",
      "\"56666\": \"Number\",\n",
      "\"19617\": \"Number\",\n",
      "\"36666\": \"Number\",\n",
      "\"57826\": \"Number\",\n",
      "\"126\n",
      "1744 5794\n",
      "Actual total tokens till now: 7538\n",
      "INCORRECT DICTIONARY Expecting property name enclosed in double quotes: line 7 column 1 (char 152)\n",
      "Iteration 4 of 34\n",
      "\n",
      "\n",
      "\n",
      "1180 prompt tokens counted.\n",
      "\n",
      "Do not include the angle brackets in your dictionary.\n",
      "ASSISTANT: Mathematical identifiers from Figure 3:\n",
      "```json\n",
      "{\n",
      "    \"Part-of-Speech\": \"Part-of-Speech\",\n",
      "    \"Dependencies\": \"Dependencies\",\n",
      "    \"Constituents\": \"Constituents\",\n",
      "    \"Winograd\": \"Winograd\",\n",
      "    \"MiniBERTas\": \"MiniBERTas\",\n",
      "    \"RoBERTa\": \"RoBERTa\",\n",
      "    \"MiniBERTa<BASE>\": \"MiniBERTa<BASE>\",\n",
      "    \"RoBERTa<BASE>\": \"RoBERTa<BASE>\",\n",
      "    \"1M\": \"1M\",\n",
      "    \"10M\": \"10M\",\n",
      "    \"100M\": \"100M\",\n",
      "    \"1B\": \"1B\",\n",
      "    \"12\": \"12\",\n",
      "    \"25\": \"25\",\n",
      "    \"3\": \"3\",\n",
      "    \"333\": \"333\",\n",
      "    \"news\": \"News\",\n",
      "    \"web\": \"Web\",\n",
      "    \"Devlin\": \"Devlin\",\n",
      "    \"Warstadt\": \"Warstadt\",\n",
      "    \"Liu\": \"Liu\"\n",
      "}\n",
      "```</s>\n",
      "2021 6974\n",
      "Actual total tokens till now: 8995\n",
      "Iteration 5 of 34\n",
      "\n",
      "\n",
      "\n",
      "1384 prompt tokens counted.\n",
      "\n",
      "Do not include the angle brackets in your dictionary.\n",
      "ASSISTANT: Mathematical symbols and variables inside \"<||>\" are:\n",
      "```json\n",
      "{\"y\": \"Probing suite\", \"x\": \"Amount of pretraining data\", \"16\": \"16 models\", \"min\": \"Minimum\", \"descr\": \"Minimum description length\", \"BLiMP\": \"BLiMP\", \"unsup\": \"Unsupervised acceptability judgments\", \"SuperGLUE\": \"SuperGLUE\", \"normalize\": \"Normalize\", \"log\": \"Log\", \"logistic\": \"Logistic function\", \"exp\": \"Exponential learning curve\", \"Heathcote\": \"Heathcote et al., 2000\"}.\n",
      "```</s>\n",
      "2174 8358\n",
      "Actual total tokens till now: 10532\n",
      "Iteration 6 of 34\n",
      "Iteration 7 of 34\n",
      "Iteration 8 of 34\n",
      "\n",
      "\n",
      "\n",
      "1566 prompt tokens counted.\n",
      "\n",
      "Do not include the angle brackets in your dictionary.\n",
      "ASSISTANT: 📌 Here is a JSON dictionary for the mathematical identifiers inside \"<||>\":\n",
      "```json\n",
      "{\n",
      "    f: {\n",
      "        meaning: \"function\",\n",
      "        definition: \"(T => { pair of attention pooling functions }) for each task T\"\n",
      "    },\n",
      "    T: {\n",
      "        meaning: \"task\",\n",
      "        definition: \"pairwise tasks, single span tasks\"\n",
      "    },\n",
      "    S: {\n",
      "        meaning: \"span of tokens\",\n",
      "        definition: \"input for each task\"\n",
      "    },\n",
      "    i: {\n",
      "        meaning: \"index\",\n",
      "        definition: \"first and second tokens in a span\"\n",
      "    },\n",
      "    r: {\n",
      "        meaning: \"representation\",\n",
      "        definition: \"pair of representations for each span\"\n",
      "    },\n",
      "    f: {\n",
      "        meaning: \"function\",\n",
      "        definition: \"attention pooling function\"\n",
      "    },\n",
      "    j: {\n",
      "        meaning: \"label\",\n",
      "        definition: \"prediction for each label\"\n",
      "    },\n",
      "  p: {\n",
      "        meaning: \"pair of representations\",\n",
      "        definition: \"(representations of first and second tokens) for each span\"\n",
      "    },\n",
      " t: {\n",
      "        meaning: \"token\",\n",
      "        definition: \"token in a span\"\n",
      "    },\n",
      " k: {\n",
      "        meaning: \"index\",\n",
      "        definition: \"two indices for each token\"\n",
      "},\n",
      "  a: {\n",
      "        meaning: \"activations\",\n",
      "        definition: \"RoBERTa's layer activations\"\n",
      "},\n",
      " d: {\n",
      "        meaning: \"dimension\",\n",
      "        definition: \"256-dimensional space\"\n",
      "}\n",
      "}\n",
      "```\n",
      "I have provided a JSON dictionary for the mathematical identifiers inside \"<||>\" based on their meanings and definitions in the text.</s>\n",
      "2569 9924\n",
      "Actual total tokens till now: 12493\n",
      "INCORRECT DICTIONARY Expecting property name enclosed in double quotes: line 2 column 5 (char 6)\n",
      "Iteration 9 of 34\n",
      "Iteration 10 of 34\n",
      "Iteration 11 of 34\n",
      "\n",
      "\n",
      "\n",
      "1631 prompt tokens counted.\n",
      "\n",
      "Do not include the angle brackets in your dictionary.\n",
      "ASSISTANT: \n",
      "identifiers = {\n",
      "\"MDL\": \"Minimum Description Length\",\n",
      "\"dataset\": \"Dataset for encoder model\",\n",
      "\"decoder\": \"Model\",\n",
      "\"encoder\": \"Model\",\n",
      "\"labels\": \"Data\",\n",
      "\"codelength\": \"Number of bits for transmission\",\n",
      "\"decoder parameters\": \"Model parameters\",\n",
      "\"tradeoff\": \"Balance between\",\n",
      "\"Voita\": \"Voita and Titov\",\n",
      "\"online code estimation\": \"Method of calculating MDL\",\n",
      "\"MLP\": \"Multilayer Perceptron\",\n",
      "\"loss\": \"Loss function\",\n",
      "\"portion\": \"Fraction of training data\",\n",
      "\"uniform prior\": \"Probability distribution\",\n",
      "\"sum\": \"Calculation of online codelength\",\n",
      "\"log\": \"Logarithm base 2\",\n",
      "\"K\": \"Number of examples\"\n",
      "}</s>\n",
      "2766 11555\n",
      "Actual total tokens till now: 14321\n",
      "Iteration 12 of 34\n",
      "\n",
      "New dictionary\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1156 prompt tokens counted.\n",
      "\n",
      "Do not include the angle brackets in your dictionary.\n",
      "ASSISTANT: math_identifiers = {\n",
      "\"∑\": \"Summation\",\n",
      "\"≤\": \"Less than or equal to\",\n",
      "\"110\": \"110\",\n",
      "\"log2\": \"Logarithm base 2\",\n",
      "\"⁡\": \"Argument of the log function\",\n",
      "\"p\": \"Precision\",\n",
      "\"θ\": \"Threshold\",\n",
      "\"i\": \"Index\",\n",
      "\"∈\": \"Element of a set\",\n",
      "\"t\": \"Time\",\n",
      "\"y\": \"Labels\",\n",
      "\"⁢\": \"Log function\",\n",
      "\"≼\": \"Approximately equal to\",\n",
      "\"≥\": \"Greater than or equal to\",\n",
      "\"−\": \"Negation\",\n",
      "\"±\": \"Plus or minus\",\n",
      "\"=\": \"Equal to\",\n",
      "\"≥\": \"Greater than or equal to\",\n",
      "\"1\": \"One\",\n",
      "\"2\": \"Two\",\n",
      "\"*\": \"Multiplication\",\n",
      "\"/\" : \"Division\",\n",
      "\"1\": \"One\",\n",
      "\"2\": \"Two\",\n",
      "\"1\": \"One\",\n",
      "\"2\": \"Two\",\n",
      "\"1\": \"One\",\n",
      "\"2\": \"Two\",\n",
      "\"1\": \"One\",\n",
      "\"2\": \"Two\",\n",
      "\"1\": \"One\",\n",
      "\"2\": \"Two\",\n",
      "\"1\": \"One\",\n",
      "\"2\": \"Two\",\n",
      "\"1\": \"One\",\n",
      "\"2\": \"Two\",\n",
      "\"1\": \"One\",\n",
      "\"2\": \"Two\",\n",
      "\"1\": \"One\",\n",
      "\"2\": \"Two\",\n",
      "\"1\": \"One\",\n",
      "\"2\": \"Two\",\n",
      "\"1\": \"One\",\n",
      "\"2\": \"Two\",\n",
      "\"1\": \"One\",\n",
      "\"2\": \"Two\",\n",
      "\"1\": \"One\",\n",
      "\"2\": \"Two\",\n",
      "\"1\": \"One\",\n",
      "\"2\": \"Two\",\n",
      "\"1\": \"One\",\n",
      "\"2\": \"Two\",\n",
      "\"1\": \"One\",\n",
      "\"2\": \"Two\",\n",
      "\"1\": \"One\",\n",
      "\"2\": \"Two\",\n",
      "\"1\": \"One\",\n",
      "\"2\": \"Two\",\n",
      "\"1\": \"One\",\n",
      "\"2\": \"Two\",\n",
      "\"1\": \"One\",\n",
      "\"2\": \"Two\",\n",
      "\"1\": \"One\",\n",
      "\"2\": \"Two\",\n",
      "3278 12711\n",
      "Actual total tokens till now: 15989\n",
      "Iteration 13 of 34\n",
      "\n",
      "\n",
      "\n",
      "1510 prompt tokens counted.\n",
      "\n",
      "Do not include the angle brackets in your dictionary.\n",
      "ASSISTANT: 📌 You are providing a very specific and detailed prompt. I will create a JSON dictionary only containing the mathematical symbols and identifiers within the \"<||>\" delimiters, focusing on each individual symbol or identifier separately, without including angle brackets or skipping any of them. Here is the JSON dictionary:\n",
      "```json\n",
      "symbols_and_identifiers = {\n",
      "    \"+\" : \"Addition\",\n",
      "    \"-\" : \"Subtraction\",\n",
      "\"/\": \"Division\",\n",
      "\"\\*\" : \"Multiplication\",\n",
      "\"=\" : \"Equal to\",\n",
      "\"<\" : \"Less than\",\n",
      "\">\" : \"Greater than\",\n",
      "\"!\" : \"Exclamation mark\",\n",
      "\"2\" : \"Two\",\n",
      "\"42\" : \"42\",\n",
      "\"6\" : \"Six\",\n",
      "\"8\" : \"Eight\",\n",
      "\"11\" : \"Eleven\",\n",
      "\"12\" : \"Twelve\",\n",
      "\"16\" : \"Sixteen\",\n",
      "\"17\" : \"Seventeen\",\n",
      "\"2\" : \"Two\",\n",
      "\"3\" : \"Three\",\n",
      "\"5\" : \"Five\",\n",
      "\"7\" : \"Seven\",\n",
      "\"9\" : \"Nine\",\n",
      "\"1\" : \"One\",\n",
      "\"13\" : \"Thirteen\",\n",
      "\"14\" : \"Fourteen\",\n",
      "\"15\" : \"Fifteen\",\n",
      "\"18\" : \"Eighteen\",\n",
      "\"19\" : \"Nineteen\",\n",
      "\"21\" : \"Twenty-one\",\n",
      "\"22\" : \"Twenty-two\",\n",
      "\"23\" : \"Twenty-three\",\n",
      "\"24\" : \"Twenty-four\",\n",
      "\"25\" : \"Twenty-five\",\n",
      "\"26\" : \"Twenty-six\",\n",
      "\"27\" : \"Twenty-seven\",\n",
      "\"28\" : \"Twenty-eight\",\n",
      "\"29\" : \"Twenty-nine\",\n",
      "\"31\" : \"Thirty-one\",\n",
      "\"32\" : \"Thirty-two\",\n",
      "\"33\" : \"Thirty-three\",\n",
      "\"34\" : \"Thirty-four\",\n",
      "\"35\" : \"Thirty-five\",\n",
      "\"36\" :\n",
      "3791 14221\n",
      "Actual total tokens till now: 18012\n",
      "INCORRECT DICTIONARY Invalid \\escape: line 5 column 2 (char 67)\n",
      "Iteration 14 of 34\n",
      "Iteration 15 of 34\n",
      "\n",
      "\n",
      "\n",
      "1538 prompt tokens counted.\n",
      "\n",
      "Do not include the angle brackets in your dictionary.\n",
      "ASSISTANT: JSON dictionary for the given text:\n",
      "```json\n",
      "{\n",
      "\"agreement_phenomena\": \"Agreement phenomena\",\n",
      "\"wh-dependencies\": \"Wh-dependencies\",\n",
      "\"learning_curves\": \"Learning curves\",\n",
      "\"1M\": \"1 million words\",\n",
      "\"10M\": \"10 million words\",\n",
      "\"90%\": \"90 percent\",\n",
      "\"phenomena\": \"Phenomena\",\n",
      "\"involving\": \"Involving\",\n",
      "\"local_dependencies\": \"Local dependencies\",\n",
      "\"long-distance\": \"Long-distance\",\n",
      "\"frequent\": \"Frequent\",\n",
      "\"multiple_instances\": \"Multiple instances\",\n",
      "\"superGLUE\": \"SuperGLUE\",\n",
      "\"BoolQ\": \"Boolean Questions\",\n",
      "\"COPA\": \"Coordination of noun phrases\",\n",
      "\"WiC\": \"Who is the Cause\",\n",
      "\"RTE\": \"Recognize Textual Entailment\",\n",
      "\"accuracy\": \"Accuracy\",\n",
      "\"CB\": \"Conjunctive Behavior\",\n",
      "\"Nobody_ate\": \"Nobody ate\",\n",
      "\"more_than\": \"More than\",\n",
      "\"*at_least\": \"At least\",\n",
      "\"two_cookies\": \"Two cookies\",\n",
      "\"subtle_semantic_contrasts\": \"Subtle semantic contrasts\",\n",
      "\"MiniBERTa\": \"MiniBERTa\",\n",
      "\"classification-based_tasks\": \"Classification-based tasks\",\n",
      "\"significant_variation\": \"Significant variation\",\n",
      "\"expect_to_see\": \"Expect to see\",\n",
      "\"111111Task_Data_source\": \"11111Task Data source\"\n",
      "}\n",
      "```</s>\n",
      "4184 15759\n",
      "Actual total tokens till now: 19943\n",
      "Iteration 16 of 34\n",
      "Iteration 17 of 34\n",
      "Iteration 18 of 34\n",
      "Iteration 19 of 34\n",
      "Iteration 20 of 34\n",
      "Iteration 21 of 34\n",
      "Iteration 22 of 34\n",
      "Iteration 23 of 34\n",
      "Iteration 24 of 34\n",
      "Iteration 25 of 34\n",
      "Iteration 26 of 34\n",
      "Iteration 27 of 34\n",
      "Iteration 28 of 34\n",
      "Iteration 29 of 34\n",
      "Iteration 30 of 34\n",
      "Iteration 31 of 34\n",
      "Iteration 32 of 34\n",
      "Iteration 33 of 34\n",
      "\n",
      "\n",
      "\n",
      "1731 prompt tokens counted.\n",
      "\n",
      "Do not include the angle brackets in your dictionary.\n",
      "ASSISTANT: 📚 arXiv preprint arXiv:2022.7368\n",
      "\n",
      "📌 Appendix A\n",
      "📌 Table 1: Hyperparameter search ranges for the SuperGLUE tasks. Our search ranges are largely dependent on those used in Pruksachatkun et al. (2022).\n",
      "📌 Table 2: BLiMP results. 5-gram, LSTM, TXL, GPT-2 scores come from Warstadt et al. (2022a). BERT®BASE scores come from Salazar et al. (2022).\n",
      "📌 Figure 1: Our absolute edge probing dev set results (not normalized) compared to BERT®LARGE test set results from Tenney et al. (2021b).\n",
      "📌 Figure 2: Our absolute SuperGLUE results (not normalized) compared to RoBERTa®LARGE results from Liu et al. (2021).\n",
      "\n",
      "🔎 Generated on Tue Dec 8 23:10:57 2022 by LaTeXML\n",
      "\n",
      "Mathematical identifiers inside \"<||>\" are:\n",
      "arXiv\\ preprint\\ arXiv:227368\n",
      "Appendix\\ A\n",
      "Table\\ 1: Hyperparameter\\ search\\ ranges\\ for\\ the\\ SuperGLUE\\ tasks.\\ Our\\ search\\ ranges\\ are\\ largely\\ dependent\\ on\\ those\\ used\\ in\\ Pruksachatkun\\ et\\ al.\\ (2222).\n",
      "Table\\ 2: BLiMP\\ results.\\ 5-gram, LSTM, TXL, GPT-2\\ scores\\ come\\ from\\ Warstadt\\ et\\ al.\\ (2222a).\\ BERT\\®BASE\\ scores\\ come\\ from\\ Salazar\\ et\\ al.\\ (2222).\n",
      "Figure\\ 1: Our\\ absolute\\ edge\\ probing\\ dev\\ set\\ results\\ (not\\ normalized)\\ compared\\ to\\ BERT\\®LARGE\\ test\\ set\\ results\\ from\\ Tenney\\ et\\ al.\\ (2221b).\n",
      "Figure\\ 2: Our\\ absolute\\ SuperGLUE\\\n",
      "4697 17490\n",
      "Actual total tokens till now: 22187\n",
      "Time taken: 0:04:25.959682\n",
      "Total time taken: 0:04:57.507942\n",
      "22187 4697 17490\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.now()\n",
    "number_of_dictionaries = 0\n",
    "for chunk in chunks:\n",
    "    print(f\"Iteration {i} of {len(chunks)}\")\n",
    "    i += 1\n",
    "    if chunk == question:\n",
    "        continue\n",
    "    if not contains_pattern(chunk):\n",
    "        continue\n",
    "    question = chunk\n",
    "    \n",
    "    if prompt_size > 1600:\n",
    "        number_of_dictionaries += 1\n",
    "        print(\"\\nNew dictionary\\n\")\n",
    "        dictionary.append({})\n",
    "    \n",
    "    prompt = [\n",
    "            {'role': 'system',\n",
    "             'content': 'You are a helpful research assistant tasked with converting long paragraphs into a JSON '\n",
    "                        'dictionary. '\n",
    "                        'The goal is to identify and classify each individual mathematical symbol, variable, '\n",
    "                        'and identifier in the text marked between \"<||>\"'\n",
    "                        'The dictionary should store the identifiers as keys and their corresponding definitions as '\n",
    "                        'values in an array format. '},\n",
    "            {'role': 'system', 'name': 'example_user', 'content': '''A relational model is a triple <|M|>′=(<|X|>,<|R|>,\n",
    "            <|v|>), where <|X|> is a set of states, <|R|><|⊆|><|X|><|×|><|X|> is a binary relation on <|X|>, \n",
    "            and <|v|>:<|𝖯𝗋𝗈𝗉|>→2<|X|> is a valuation. Given a relational model <|M|>′, the satisfaction relation \n",
    "            between points <|x<|∈<|X<| and formulas <|φ<|∈<|ℒ<|<|𝖪𝖠<| is defined inductively by <|M|>′,\n",
    "            <|x|>⊨<|𝖪|><|φ|>⇔ for all <|y|>∈<|X|>,<|x|><|R|><|y|> implies <|M|>′,<|y|>⊨<|φ|><|M|>′,\n",
    "            <|x|>⊨<|𝖠|><|φ|><||>⇔ for all <|y|>∈<|X|>,<|M|>′,<|y|>⊨<|φ|>'''},\n",
    "            {'role': 'system', 'name': 'example_assistant', 'content': '''identifiers = {\n",
    "            \"M\": [\"Model\", \"Expertise Model\"],\n",
    "            \"M'\": \"Relational model\",\n",
    "            \"X\": \"Set of states\",\n",
    "            \"R\": \"Binary relation on X\",\n",
    "            \"v\": \"Valuation\",\n",
    "            \"𝖯𝗋𝗈𝗉\": \"Set of propositions\",\n",
    "            \"M'\": \"Relational model\",\n",
    "            \"x\": \"Point in X\",\n",
    "            \"φ\": \"Formula in 𝖪𝖠\",\n",
    "            \"ℒ_{𝖪𝖠}\": \"Set of formulas\",\n",
    "            \"𝖪\": \"Modal operator K\",\n",
    "            \"𝖠\": \"Modal operator A\",\n",
    "            \"y\": \"Point in X\",\n",
    "            \"⊨\": \"Satisfaction relation\",\n",
    "            \"⇔\": \"If and only if operator\",\n",
    "            \"∈\": \"Element of a set\",\n",
    "            \"⊆\": \"Subset of a set\",\n",
    "            \"×\": \"Cartesian product operator\",\n",
    "            \"→\": \"Function or implication operator\",\n",
    "            \"for all\": \"Universal quantifier\"\n",
    "            }'''},\n",
    "            {'role': 'system',\n",
    "             'content': f'Given is already a pre existing dictionary. Your job is to extend this dictionary. Do not '\n",
    "                        f'remove any pre existing definitions from this dictionary.'\n",
    "                        f'\\n{dictionary[number_of_dictionaries]}. If there is nothing to mention, reply with an empty '\n",
    "                        f'dictionary'},\n",
    "            {'role': 'user', 'content': f'Generate a JSON dictionary for the following text: {question}. '\n",
    "                                        'Only consider the mathematical identifiers inside \"<||>\" for the dictionary. '\n",
    "                                        'Do not consider any other identifier other than those marked. '\n",
    "                                        'Consider all the identifiers individually. Do not skip any identifier, mention'\n",
    "                                        ' all the identifiers inside \"<||>\" in your dictionary. '\n",
    "                                        'Do not include the angle brackets in your dictionary.'}\n",
    "        ]\n",
    "    \n",
    "    open_prompt = get_prompt_loop(prompt)\n",
    "    \n",
    "    prompt_size = num_tokens_from_messages(open_prompt)\n",
    "    print(f\"\\n\\n\\n{prompt_size} prompt tokens counted.\\n\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            input_ids = tokenizer(open_prompt, return_tensors='pt').input_ids.cuda()\n",
    "            output = model.generate(inputs=input_ids, temperature=0.5, max_new_tokens=512, repetition_penalty=1.05)\n",
    "            output_string = tokenizer.decode(output[0])\n",
    "            \n",
    "            actual_total_tokens += num_tokens_from_messages(output_string)\n",
    "            completion_tokens += num_tokens_from_messages(output_string) - num_tokens_from_messages(open_prompt)\n",
    "            prompt_tokens += num_tokens_from_messages(open_prompt)\n",
    "\n",
    "            ind = output_string.index('Do not include the angle brackets in your dictionary.')\n",
    "            print(output_string[ind:])\n",
    "            \n",
    "            print(completion_tokens, prompt_tokens)\n",
    "    \n",
    "            print(f\"Actual total tokens till now: {actual_total_tokens}\")\n",
    "\n",
    "            new_dictionary = {}\n",
    "\n",
    "            try:\n",
    "                correct_output_string = modify_string(output_string[ind:])\n",
    "                dic_output_string = get_json_of_string(correct_output_string)\n",
    "                new_dictionary = string_to_dict(dic_output_string)\n",
    "            except Exception as e:\n",
    "                print(\"INCORRECT DICTIONARY\", e)\n",
    "            dictionary[number_of_dictionaries] = merge_dictionaries(dictionary[number_of_dictionaries], new_dictionary)\n",
    "            \n",
    "            break\n",
    "        except Exception as e:\n",
    "            number_of_dictionaries += 1\n",
    "            dictionary.append({})\n",
    "            print(f\"Exception occurred: {e}\")\n",
    "            print(\"Retrying...\")\n",
    "total_time_taken += (datetime.now() - start_time)\n",
    "print(f\"Time taken: {datetime.now() - start_time }\")\n",
    "print(f\"Total time taken: {total_time_taken}\")\n",
    "print(actual_total_tokens, completion_tokens, prompt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04278ee2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dct = {}\n",
    "for dic in dictionary:\n",
    "    dct = merge_dictionaries(dct, dic)\n",
    "# pprint.pprint(dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d4212dbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Yian': '['Yian Zhang', 'Author']'\n",
      "'Alex': '['Alex Warstadt', 'Author']'\n",
      "'Haau-Sing': '['Haau-Sing Li', 'Author']'\n",
      "'Samuel': '['Samuel R. Bowman', 'Author']'\n",
      "'Dept. of Computer Science': '['Department of Computer Science', ['Department of Computer Science']]'\n",
      "'Dept. of Linguistics': '['Department of Linguistics', ['Department of Linguistics']]'\n",
      "'Center for Data Science': '['Center for Data Science', ['Center for Data Science']]'\n",
      "'New York University': '['New York University', ['New York University']]'\n",
      "'yian.zhang': '[\"Yian Zhang's email address\", [\"Yian Zhang's email address\"]]'\n",
      "'warstadt': '[\"Alex Warstadt's email address\", [\"Alex Warstadt's email address\"]]'\n",
      "'xl3119': '[\"Haau-Sing Li's email address\", [\"Haau-Sing Li's email address\"]]'\n",
      "'bowman': '[\"Samuel R. Bowman's email address\", [\"Samuel R. Bowman's email address\"]]'\n",
      "'RoBERTa': '['Robust Berkeley Transformers', 'RoBERTa']'\n",
      "'MiniBERTa': '['Miniature BERTas', 'MiniBERTa']'\n",
      "'1M': '['1 million words', '1M']'\n",
      "'10M': '['10 million words', '10M']'\n",
      "'100M': '['1 hundred million words', '100M']'\n",
      "'1B': '['1 billion words', '1B']'\n",
      "'UltraBERTa': '['Ultra Large Model']'\n",
      "'probing': '['Probing Methods']'\n",
      "'classifier': '['Classifier Probing']'\n",
      "'information-theoretic': '['Information-Theoretic Probing']'\n",
      "'acceptability': '['Unsupervised Relative Acceptability Judgment']'\n",
      "'fine-tuning': '['Fine-tuning on NLU Tasks']'\n",
      "'representations': '['Learned Representations']'\n",
      "'syntactic': '['Syntactic Features']'\n",
      "'semantic': '['Semantic Features']'\n",
      "'commonsense': '['Commonsense Knowledge']'\n",
      "'downstream': '['Downstream NLU Tasks']'\n",
      "'encoding': '['Encoding Linguistic Features']'\n",
      "'improvement': '['Improvements in Language Understanding']'\n",
      "'necessary': '['Necessary for Language Understanding']'\n",
      "'driver': '['Major Driver of Improvements']'\n",
      "'curve': '['Learning Curves']'\n",
      "'Yian Zhang': '['Yian Zhang', 'Author']'\n",
      "'Alex Warstadt': '['Alex Warstadt', 'Author']'\n",
      "'Haau-Sing Li': '['Haau-Sing Li', 'Author']'\n",
      "'Samuel R. Bowman': '['Samuel R. Bowman', 'Author']'\n",
      "'y': '['Probing suite', 'Labels']'\n",
      "'log': '['Log', 'Logarithm base 2']'\n",
      "'≥': '['Greater than or equal to', 'Greater than or equal to']'\n",
      "'1': '['One', 'One', 'One', 'One', 'One', 'One', 'One', 'One', 'One', 'One', 'One', 'One', 'One', 'One', 'One', 'One', 'One', 'One', 'One', 'One', 'One', 'One']'\n",
      "'2': '['Two', 'Two', 'Two', 'Two', 'Two', 'Two', 'Two', 'Two', 'Two', 'Two', 'Two', 'Two', 'Two', 'Two', 'Two', 'Two', 'Two', 'Two', 'Two', 'Two', 'Two', 'Two']'\n"
     ]
    }
   ],
   "source": [
    "for key, value in dct.items():\n",
    "    if type(value) == list:\n",
    "        print(f\"'{key}': '{value}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed252bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_list(input_list):\n",
    "    output_list = []\n",
    "    for i in input_list:\n",
    "        if isinstance(i, list):\n",
    "            output_list.extend(flatten_list(i))\n",
    "        else:\n",
    "            output_list.append(i)\n",
    "    return output_list\n",
    "\n",
    "\n",
    "def remove_duplicates(input_list):\n",
    "    output_list = []\n",
    "    for item in input_list:\n",
    "        if item not in output_list:\n",
    "            output_list.append(item)\n",
    "    if len(output_list) == 1:\n",
    "        return output_list[0]\n",
    "    return output_list\n",
    "\n",
    "\n",
    "def process_value(v):\n",
    "    if isinstance(v, str):\n",
    "        new_v = v.replace('$', '')\n",
    "        while '\\\\\\\\' in new_v:\n",
    "            new_v = new_v.replace('\\\\\\\\', '\\\\').replace('\\n', '')\n",
    "    else:  # Assuming it's a list\n",
    "        new_v = flatten_list([process_value(val) for val in v])\n",
    "        \n",
    "    return remove_duplicates(new_v) if isinstance(new_v, list) else new_v\n",
    "\n",
    "\n",
    "def reduce_pairs(dictionary):\n",
    "    new_dict = {}\n",
    "    for k, v in dictionary.items():\n",
    "        # reduce key backslashes\n",
    "        new_k = k.replace('$', '')\n",
    "        while '\\\\\\\\' in new_k:\n",
    "            new_k = new_k.replace('\\\\\\\\', '\\\\')\n",
    "\n",
    "        # process value\n",
    "        new_v = process_value(v)\n",
    "\n",
    "        new_dict[new_k] = new_v\n",
    "\n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "85c8083f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dict_without_backslashes = reduce_pairs(dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ab472ef2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pprint.pprint(dict_without_backslashes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "09b426f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_json = dict_without_backslashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0dec9c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{file_code}_mcdict.json', 'r', encoding='utf-8') as f:\n",
    "    mc_dict_original = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4284b146",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to create a hex code (a binary representation of the key)\n",
    "def get_hex_code(key):\n",
    "    return key.encode().hex()\n",
    "\n",
    "mc_dict_original['_author'] = model_name_or_path\n",
    "\n",
    "# Iterate over your dictionary and fill the new one\n",
    "for key, values in parsed_json.items():\n",
    "    # Determine the base key and the affix\n",
    "    base_key = re.match(r\"^[^*'_^,(\\[]*\", key).group()\n",
    "    affix = key[len(base_key):]\n",
    "\n",
    "    hex_code = get_hex_code(base_key)\n",
    "    values = values if isinstance(values, list) else [values]\n",
    "\n",
    "    if hex_code in mc_dict_original[\"concepts\"]:\n",
    "        k = list(mc_dict_original[\"concepts\"][hex_code][\"identifiers\"].keys())[0]\n",
    "        new_identifier = []\n",
    "        for value in values:\n",
    "            mc_dict_original[\"concepts\"][hex_code][\"identifiers\"][k].append({\n",
    "                \"affixes\": [affix] if affix else [],\n",
    "                \"arity\": 0,\n",
    "                \"description\": value\n",
    "            })\n",
    "    else:\n",
    "        if hex_code not in mc_dict_original[\"concepts\"]:\n",
    "            mc_dict_original[\"concepts\"][hex_code] = {\n",
    "                \"_surface\": {\n",
    "                    \"text\": base_key,\n",
    "                    \"unicode_name\": base_key if len(base_key) != 1 else unicodedata.name(base_key)\n",
    "                },\n",
    "                \"identifiers\": {\n",
    "                    'default': []\n",
    "                }\n",
    "            }\n",
    "\n",
    "        for value in values:\n",
    "            mc_dict_original[\"concepts\"][hex_code][\"identifiers\"][\"default\"].append({\n",
    "                \"affixes\": [affix] if affix else [],\n",
    "                \"arity\": 0,\n",
    "                \"description\": value\n",
    "            })\n",
    "\n",
    "\n",
    "# Convert new dictionary to a sorted dictionary\n",
    "sorted_dict = dict(sorted(mc_dict_original[\"concepts\"].items(), key=lambda x: (len(x[0]), x[0])))\n",
    "mc_dict_original[\"concepts\"] = sorted_dict\n",
    "\n",
    "# Convert new dictionary to JSON\n",
    "json_str = json.dumps(mc_dict_original, indent=4, ensure_ascii=False)\n",
    "\n",
    "#print(json_str)\n",
    "\n",
    "with open(f'{file_code}-{model_name}_mcdict.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(mc_dict_original, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a79ec2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_from_tags(element):\n",
    "    if isinstance(element, NavigableString):\n",
    "        return element\n",
    "    if element.name == 'mi':\n",
    "        return str(element)\n",
    "    return ''.join(get_text_from_tags(child) for child in element.children)\n",
    "\n",
    "def parse_html(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as html_file:\n",
    "        soup = BeautifulSoup(html_file, 'html.parser')\n",
    "\n",
    "    texts = get_text_from_tags(soup)\n",
    "    return texts\n",
    "\n",
    "def find_mi_strings(text):\n",
    "    pattern = r'(<mi.*?</mi>)'\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "    return matches\n",
    "\n",
    "# call the function with your HTML file path\n",
    "page = parse_html(f'{file_code}.html')\n",
    "matches = find_mi_strings(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "64a0cba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_dict = mc_dict_original\n",
    "with open(f'{file_code}_anno.json', encoding='utf-8') as fp:\n",
    "    parsed_annotation = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "163356dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_index_from_char_index(message, key, char_index):\n",
    "    i = 0\n",
    "    index = -1\n",
    "    for word in message:\n",
    "        if key in word:\n",
    "            index = i\n",
    "        i += 1\n",
    "    return index\n",
    "\n",
    "def expand_string_to_tokens(message, index, num_tokens_right=25, num_tokens_left=75):\n",
    "    words = message.split()  # Split the message into words\n",
    "\n",
    "    # Start at the index where the center word is\n",
    "    left_index = right_index = index\n",
    "\n",
    "    tokens_counter_right = num_tokens_from_messages(words[right_index])\n",
    "    tokens_counter_left = num_tokens_from_messages(words[left_index])\n",
    "\n",
    "    # Expand to the left from the center index until you reach num_tokens_left\n",
    "    while tokens_counter_left < num_tokens_left and left_index > 0:\n",
    "        left_index -= 1\n",
    "        tokens_counter_left += num_tokens_from_messages(words[left_index])\n",
    "\n",
    "    # Expand to the right from the center index until you reach num_tokens_right\n",
    "    while tokens_counter_right < num_tokens_right and right_index < len(words) - 1:\n",
    "        right_index += 1\n",
    "        tokens_counter_right += num_tokens_from_messages(words[right_index])\n",
    "\n",
    "    # Combine the words back into a string and return\n",
    "    return ' '.join(words[left_index:right_index + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "01b6b971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_text(text, replacement, exception):\n",
    "    # Find all matches\n",
    "    matches = re.findall(r'<mi(.*?)</mi>', text)\n",
    "    \n",
    "    for match in matches:\n",
    "        original_string = f'<mi{match}</mi>'\n",
    "        \n",
    "        # Skip exception\n",
    "        if original_string == exception:\n",
    "            continue\n",
    "        \n",
    "        # Replace match\n",
    "        replaced_string = re.sub(r'<.*?>(.*?)</.*?>', r'\\1', original_string)\n",
    "        text = text.replace(original_string, replaced_string)\n",
    "\n",
    "    return text\n",
    "\n",
    "def get_context(match):\n",
    "    match_len = len(match)\n",
    "    new_page = replace_text(page, '', match)\n",
    "    char_index = new_page.index(match) + int(match_len/2)\n",
    "    word_index = get_word_index_from_char_index(new_page, char_index)\n",
    "    section = expand_string_to_tokens(new_page, word_index)\n",
    "    section = re.sub(r'<.*?>(.*?)</.*?>', r'<<\\1>>', section)\n",
    "    return match, section\n",
    "\n",
    "# Function to create a hex code (a binary representation of the key)\n",
    "def get_hex_code(key):\n",
    "    return key.encode('utf-8').hex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7706ec86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_trailing_tags(s):\n",
    "    parts = re.split('(<mi)', s)\n",
    "    for i in range(1, len(parts), 2):\n",
    "        if '>' not in parts[i + 1]:\n",
    "            parts[i] = ''\n",
    "            parts[i + 1] = ''\n",
    "    return ''.join(parts)\n",
    "\n",
    "def get_definition_of_id(dict_id, identifier):\n",
    "    \n",
    "    try:\n",
    "        hex_code = get_hex_code(identifier)\n",
    "        index = parsed_annotation['mi_anno'][dict_id]['concept_id']\n",
    "        key = list(parsed_dict['concepts'][hex_code]['identifiers'].keys())[0]\n",
    "        return f\"({parsed_dict['concepts'][hex_code]['identifiers'][key][index]['description']})\"\n",
    "    except Exception as e:\n",
    "        return \"\"\n",
    "\n",
    "def get_context(match):\n",
    "    key_word = page.index(match) + len(match)\n",
    "    last_index = min(len(page), key_word + 500)\n",
    "    first_index = max(0, key_word - 3000)\n",
    "    context_window = page[first_index:last_index]\n",
    "    \n",
    "    reg_matches = re.findall(r'<mi(.*?)</mi>', context_window)\n",
    "    \n",
    "    identifier = None\n",
    "    \n",
    "    for reg_match in reg_matches:\n",
    "        original_string = f'<mi{reg_match}</mi>'\n",
    "        soup = BeautifulSoup(original_string, 'html.parser')\n",
    "        \n",
    "        \n",
    "        tags = soup.find_all('mi')\n",
    "        \n",
    "        if original_string == match:\n",
    "            identifier = tags[0].text\n",
    "            continue\n",
    "\n",
    "        context_window = context_window.replace(original_string,\n",
    "                                                f\"{tags[0].text}{get_definition_of_id(tags[0].get('id'), tags[0].text)}\")\n",
    "    \n",
    "    context_window = re.sub(r'<mi.*?>(.*?)<\\/mi>', r'<<\\1>>', context_window)\n",
    "    \n",
    "    context_window = remove_trailing_tags(context_window)\n",
    "    context_window = re.sub(r'^(?!.*<mi.*).*<\\/mi>', '', context_window, flags=re.DOTALL)\n",
    "        \n",
    "    index = 0\n",
    "    for word in context_window.split():\n",
    "        if f\"<<{identifier}>>\" in word:\n",
    "            word_index = index\n",
    "        index += 1\n",
    "    \n",
    "    if word_index == -1:\n",
    "        return context_window\n",
    "    else:\n",
    "        context_window = expand_string_to_tokens(context_window, word_index)\n",
    "    return context_window\n",
    "\n",
    "#print(get_context('<mi id=\"S1.p2.1.m1.1.1.3\" xref=\"S1.p2.1.m1.1.1.3.cmml\">φ</mi>'))\n",
    "#get_context('<mi id=\"S1.p2.1.m1.1.1.2\" xref=\"S1.p2.1.m1.1.1.2.cmml\">𝖤</mi>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "62ec5036-7177-46d0-9728-571edb9f6bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_anno(prompt):\n",
    "    return f'''SYSTEM: {prompt[0]['content']}\n",
    "USER: {prompt[1]['content']}\n",
    "ASSISTANT:'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f57696ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 of 123: Annotation ID does not exist in annotation.json id1.1.m1.2.2a\n",
      "Iteration 2 of 123: Annotation ID does not exist in annotation.json id2.2.m2.2.2a\n",
      "Iteration 3 of 123: Annotation ID does not exist in annotation.json id3.3.m3.1.1a\n",
      "Iteration 4 of 123: Annotation ID does not exist in annotation.json id4.4.m4.3.3a\n",
      "Iteration 5 of 123: Annotation ID does not exist in annotation.json id5.5.m5.1.1a\n",
      "Iteration 6 of 123: Annotation ID does not exist in annotation.json id6.6.m6.1.1a\n",
      "Iteration 7 of 123: Annotation ID does not exist in annotation.json id7.7.m7.1.1a\n",
      "Iteration 8 of 123: Annotation ID does not exist in annotation.json p2.1.1.1.1.1.1.1.m1.2.2a\n",
      "Iteration 9 of 123: Annotation ID does not exist in annotation.json p2.2.2.2.2.2.2.2.m2.2.2a\n",
      "Iteration 10 of 123: Annotation ID does not exist in annotation.json p2.3.3.3.3.3.3.3.m3.1.1a\n",
      "Iteration 11 of 123: Annotation ID does not exist in annotation.json p2.4.4.4.4.4.4.4.m4.3.3a\n",
      "Iteration 12 of 123: Annotation ID does not exist in annotation.json p2.5.5.5.5.5.1.m1.1.1a\n",
      "Iteration 13 of 123: Annotation ID does not exist in annotation.json p2.6.6.6.6.6.2.m2.1.1a\n",
      "Iteration 14 of 123: Annotation ID does not exist in annotation.json p2.7.7.7.7.7.3.m3.1.1a\n",
      "Iteration 15 of 123: Annotation ID does not exist in annotation.json S1.p3.1.m1.1.1a\n",
      "Iteration 16 of 123: Annotation ID does not exist in annotation.json S1.p4.1.m1.1.1a\n",
      "Iteration 17 of 123: Annotation ID does not exist in annotation.json S2.F2.2.m1.1.1b\n",
      "Iteration 18 of 123: Annotation ID does not exist in annotation.json S2.p2.1.m1.1.1a\n",
      "Iteration 19 of 123: Annotation ID does not exist in annotation.json footnote3.m1.1.1b\n",
      "Iteration 20 of 123: Annotation ID does not exist in annotation.json S2.p2.2.m2.1.1a\n",
      "Iteration 21 of 123: Annotation ID does not exist in annotation.json S2.p3.1.m1.1.1a\n",
      "Iteration 22 of 123: ASSISTANT: Based on the provided text and possible annotations, the most fitting description for the identifier \"y\" is index 1, which corresponds to the annotation:\n",
      "```json\n",
      "{'index': '1', 'identifier': 'y', 'description': 'Labels'}\n",
      "```\n",
      "So, the selected index is 1.</s>\n",
      "68 260\n",
      "1\n",
      "Iteration 23 of 123: 0\n",
      "Iteration 24 of 123: 0\n",
      "Iteration 25 of 123: 0\n",
      "Iteration 26 of 123: None\n",
      "Iteration 27 of 123: None\n",
      "Iteration 28 of 123: None\n",
      "Iteration 29 of 123: None\n",
      "Iteration 30 of 123: None\n",
      "Iteration 31 of 123: None\n",
      "Iteration 32 of 123: None\n",
      "Iteration 33 of 123: 0\n",
      "Iteration 34 of 123: None\n",
      "Iteration 35 of 123: 0\n",
      "Iteration 36 of 123: None\n",
      "Iteration 37 of 123: 0\n",
      "Iteration 38 of 123: None\n",
      "Iteration 39 of 123: 0\n",
      "Iteration 40 of 123: None\n",
      "Iteration 41 of 123: None\n",
      "Iteration 42 of 123: None\n",
      "Iteration 43 of 123: 0\n",
      "Iteration 44 of 123: None\n",
      "Iteration 45 of 123: None\n",
      "Iteration 46 of 123: None\n",
      "Iteration 47 of 123: 0\n",
      "Iteration 48 of 123: None\n",
      "Iteration 49 of 123: None\n",
      "Iteration 50 of 123: None\n",
      "Iteration 51 of 123: None\n",
      "Iteration 52 of 123: 0\n",
      "Iteration 53 of 123: None\n",
      "Iteration 54 of 123: 0\n",
      "Iteration 55 of 123: None\n",
      "Iteration 56 of 123: None\n",
      "Iteration 57 of 123: None\n",
      "Iteration 58 of 123: 0\n",
      "Iteration 59 of 123: None\n",
      "Iteration 60 of 123: None\n",
      "Iteration 61 of 123: 0\n",
      "Iteration 62 of 123: 0\n",
      "Iteration 63 of 123: None\n",
      "Iteration 64 of 123: 0\n",
      "Iteration 65 of 123: None\n",
      "Iteration 66 of 123: 0\n",
      "Iteration 67 of 123: None\n",
      "Iteration 68 of 123: 0\n",
      "Iteration 69 of 123: 0\n",
      "Iteration 70 of 123: None\n",
      "Iteration 71 of 123: 0\n",
      "Iteration 72 of 123: 0\n",
      "Iteration 73 of 123: None\n",
      "Iteration 74 of 123: Key does not exist in the dictionary of concepts ... 2e2e2e\n",
      "Iteration 75 of 123: 0\n",
      "Iteration 76 of 123: None\n",
      "Iteration 77 of 123: ASSISTANT: Based on the provided text and possible annotations, the most fitting description for the identifier \"y\" is \"Labels\". Therefore, I would select the index '1' as the appropriate annotation for the identifier \"y\" in this context.</s>\n",
      "119 519\n",
      "1\n",
      "Iteration 78 of 123: None\n",
      "Iteration 79 of 123: None\n",
      "Iteration 80 of 123: 0\n",
      "Iteration 81 of 123: 0\n",
      "Iteration 82 of 123: 0\n",
      "Iteration 83 of 123: 0\n",
      "Iteration 84 of 123: 0\n",
      "Iteration 85 of 123: 0\n",
      "Iteration 86 of 123: Key does not exist in the dictionary of concepts ... 2e2e2e\n",
      "Iteration 87 of 123: 0\n",
      "Iteration 88 of 123: 0\n",
      "Iteration 89 of 123: 0\n",
      "Iteration 90 of 123: 0\n",
      "Iteration 91 of 123: 0\n",
      "Iteration 92 of 123: 0\n",
      "Iteration 93 of 123: None\n",
      "Iteration 94 of 123: None\n",
      "Iteration 95 of 123: ASSISTANT: Based on the provided text and possible annotations, the most fitting description for the identifier <<y>> is <<1>>, which corresponds to the index <<1>>.</s>\n",
      "155 776\n",
      "1\n",
      "Iteration 96 of 123: None\n",
      "Iteration 97 of 123: 0\n",
      "Iteration 98 of 123: None\n",
      "Iteration 99 of 123: 0\n",
      "Iteration 100 of 123: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"log\" is the second annotation (index 1, logarithm base 2).</s>\n",
      "191 1054\n",
      "1\n",
      "Iteration 101 of 123: 0\n",
      "Iteration 102 of 123: 0\n",
      "Iteration 103 of 123: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"log\" is the second annotation (index 1, logarithm base 2).</s>\n",
      "227 1335\n",
      "1\n",
      "Iteration 104 of 123: 0\n",
      "Iteration 105 of 123: 0\n",
      "Iteration 106 of 123: 0\n",
      "Iteration 107 of 123: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"y\" is <<Labels>>. Therefore, the index I would select is <<1>>.</s>\n",
      "264 1622\n",
      "1\n",
      "Iteration 108 of 123: 0\n",
      "Iteration 109 of 123: 0\n",
      "Iteration 110 of 123: 0\n",
      "Iteration 111 of 123: 0\n",
      "Iteration 112 of 123: 0\n",
      "Iteration 113 of 123: 0\n",
      "Iteration 114 of 123: 0\n",
      "Iteration 115 of 123: 0\n",
      "Iteration 116 of 123: 0\n",
      "Iteration 117 of 123: 0\n",
      "Iteration 118 of 123: Annotation ID does not exist in annotation.json S5.F5.2.m1.1.1b\n",
      "Iteration 119 of 123: Annotation ID does not exist in annotation.json S5.SS0.SSS0.Px1.p3.1.m1.1.1a\n",
      "Iteration 120 of 123: Annotation ID does not exist in annotation.json S6.F6.2.m1.1.1b\n",
      "Iteration 121 of 123: Annotation ID does not exist in annotation.json A1.T2.4.m1.1.1b\n",
      "Iteration 122 of 123: Annotation ID does not exist in annotation.json A1.F7.2.m1.1.1b\n",
      "Iteration 123 of 123: Annotation ID does not exist in annotation.json A1.F8.2.m1.1.1b\n",
      "Annotation completed\n",
      "Time taken: 0:00:20.625604\n",
      "Total time taken: 0:00:20.625563\n",
      "1886 264 1622\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.now()\n",
    "actual_total_tokens = 0\n",
    "completion_tokens = 0\n",
    "prompt_tokens = 0\n",
    "no_tags = 0\n",
    "no_keys = 0\n",
    "no_anno = 0\n",
    "i = 1\n",
    "for match in matches:\n",
    "    print(f\"Iteration {i} of {len(matches)}: \", end='')\n",
    "    i += 1\n",
    "    context = get_context(match)\n",
    "    match_variable = re.sub(r'<.*?>(.*?)</.*?>', r'\\1', match)\n",
    "    context_index = context.index(f\"<<{match_variable}>>\") + len(match_variable)\n",
    "    possible_affix = str(context[context_index+4:context_index+5]).replace(\"′\", \"'\")\n",
    "    soup = BeautifulSoup(match, 'html.parser')\n",
    "    mi_tag = soup.find('mi')\n",
    "    if mi_tag is not None and 'id' in mi_tag.attrs:\n",
    "        anno_id = mi_tag['id']\n",
    "    else:\n",
    "        print('TAG NOT FOUND', match)\n",
    "        no_tags += 1\n",
    "        continue\n",
    "    \n",
    "    hex_code = get_hex_code(match_variable)\n",
    "    if hex_code not in parsed_dict['concepts']:\n",
    "        match_variable = f\"{unidecode(match_variable)}\"\n",
    "        hex_code = get_hex_code(match_variable)\n",
    "        if hex_code not in parsed_dict['concepts']:\n",
    "            print(\"Key does not exist in the dictionary of concepts\", match_variable, hex_code)\n",
    "            no_keys += 1\n",
    "            continue\n",
    "    \n",
    "    if anno_id not in parsed_annotation['mi_anno']:\n",
    "        print(\"Annotation ID does not exist in annotation.json\", anno_id)\n",
    "        no_anno += 1\n",
    "        continue\n",
    "\n",
    "    k = list(parsed_dict[\"concepts\"][hex_code][\"identifiers\"].keys())[0]\n",
    "    mcdict = parsed_dict['concepts'][hex_code]['identifiers'][k]\n",
    "    \n",
    "    if len(mcdict) == 1:\n",
    "        parsed_annotation['mi_anno'][anno_id]['concept_id'] = 0\n",
    "        print('0')\n",
    "    elif len(mcdict) > 1:\n",
    "        prompt_mcdict = []\n",
    "\n",
    "        index = 0\n",
    "        for val in mcdict:\n",
    "            prompt_mcdict.append({'index': f\"{index}\", 'identifier': f\"{match_variable}{'' if len(val['affixes']) == 0 else val['affixes'][0]}\", 'description': val['description']})\n",
    "            index += 1\n",
    "            \n",
    "        prompt = [\n",
    "            {'role': 'system', 'content': 'You are a professional annotater API. Your job is to select a fitting annotation from a dictionary for a mathematical identifier.'},\n",
    "            {'role': 'user', 'content': f'''Given the following possible annotations:\\n```json\\n{prompt_mcdict}```.\n",
    "             Select the index for the most fitting description for the identifier <<{match_variable}>> from the following text.\n",
    "             The potential affix of the indentifier could be <<{possible_affix}>>. Take the affixes of the possible annotations into account.\n",
    "             Only return the value of the index and nothing else.\n",
    "             Do not add any explanation otherwise the API breaks.\n",
    "             The identifier has been marked with <<>>.\n",
    "             If you can't come up with an index, write 'None'\n",
    "             ```txt\n",
    "             {context}\n",
    "             ```'''}\n",
    "        ]\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                open_prompt = get_prompt_anno(prompt)\n",
    "                \n",
    "                input_ids = tokenizer(open_prompt, return_tensors='pt').input_ids.cuda()\n",
    "                output = model.generate(inputs=input_ids, temperature=0.5, max_new_tokens=512, repetition_penalty=1.05)\n",
    "                output_string = tokenizer.decode(output[0])\n",
    "                \n",
    "                actual_total_tokens += num_tokens_from_messages(output_string)\n",
    "                completion_tokens += num_tokens_from_messages(output_string) - num_tokens_from_messages(open_prompt)\n",
    "                prompt_tokens += num_tokens_from_messages(open_prompt)\n",
    "\n",
    "                ind = output_string.index('ASSISTANT:')\n",
    "                value = output_string[ind:]\n",
    "                print(value)\n",
    "                \n",
    "                print(completion_tokens, prompt_tokens)\n",
    "\n",
    "                try:\n",
    "                    index = int(int(re.search('\\d+', value).group()))\n",
    "                    print(index)\n",
    "                    parsed_annotation['mi_anno'][anno_id]['concept_id'] = index\n",
    "                except Exception as f:\n",
    "                    print(f)\n",
    "\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Exception occurred\\n{e}\")\n",
    "                print(\"Retrying...\")\n",
    "    else:\n",
    "        print('None')\n",
    "\n",
    "print('Annotation completed')\n",
    "\n",
    "    \n",
    "total_time_taken = (datetime.now() - start_time)\n",
    "print(f\"Time taken: {datetime.now() - start_time }\")\n",
    "print(f\"Total time taken: {total_time_taken}\")\n",
    "print(actual_total_tokens, completion_tokens, prompt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bfeab45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_annotation['_annotator'] = model_name_or_path\n",
    "with open(f'{file_code}-{model_name}_anno.json', 'w') as fp:\n",
    "    json.dump(parsed_annotation, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9180c363",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n"
     ]
    }
   ],
   "source": [
    "items = 0\n",
    "for key, value in parsed_annotation['mi_anno'].items():\n",
    "    if value['concept_id'] is not None:\n",
    "        #print(key, value)\n",
    "        items += 1\n",
    "print(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffe4c27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
