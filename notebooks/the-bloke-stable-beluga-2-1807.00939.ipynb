{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63e50065-dd79-48b6-9bc3-87eb6522a15f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unidecode in /usr/local/lib/python3.10/dist-packages (1.3.6)\n",
      "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.4.0)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.31.0)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.6.1)\n",
      "Requirement already satisfied: auto-gptq in /usr/local/lib/python3.10/dist-packages (0.3.0)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.8.8)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.28.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.2)\n",
      "Requirement already satisfied: accelerate>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (0.21.0)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (2.14.4)\n",
      "Requirement already satisfied: rouge in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (1.0.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (2.0.1+cu118)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (0.4.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.19.0->auto-gptq) (5.9.5)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2022.12.7)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (1.11.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (2.0.0)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.13.0->auto-gptq) (3.25.0)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.13.0->auto-gptq) (15.0.7)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (12.0.1)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (2.0.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (3.3.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (0.70.15)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (3.8.5)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from rouge->auto-gptq) (1.16.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->auto-gptq) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->auto-gptq) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->auto-gptq) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->auto-gptq) (2023.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->auto-gptq) (1.2.1)\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.99\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install unidecode tiktoken transformers einops auto-gptq sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b5a187-7c86-4ea4-863a-55817112cfb5",
   "metadata": {},
   "source": [
    "## Time to install packages 4 mins 0 seconds\n",
    "## Time started: 23:59:00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4255ace1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, NavigableString\n",
    "from unidecode import unidecode\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime\n",
    "import unicodedata\n",
    "import itertools\n",
    "import tiktoken\n",
    "import ast\n",
    "import pprint\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af9777c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39e498bc1d2742079d9b15c7296e20ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)lve/main/config.json:   0%|          | 0.00/679 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7c61d2d299c444bb3fba6be4c25db62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)quantize_config.json:   0%|          | 0.00/183 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d13958dcb1cd406982933af58ca0ed05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)4bit--1g.safetensors:   0%|          | 0.00/35.3G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "skip module injection for FusedLlamaMLPForQuantizedModel not support integrate without triton yet.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, pipeline, logging\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "\n",
    "model_name_or_path = \"TheBloke/StableBeluga2-70B-GPTQ\"\n",
    "model_basename = \"gptq_model-4bit--1g\"\n",
    "model_name = 'StableBeluga2'\n",
    "file_code = '1807.00939'\n",
    "\n",
    "use_triton = False\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "\n",
    "model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n",
    "        model_basename=model_basename,\n",
    "        inject_fused_attention=False, # Required for Llama 2 70B models at this time.\n",
    "        use_safetensors=True,\n",
    "        trust_remote_code=False,\n",
    "        device=\"cuda:0\",\n",
    "        use_triton=use_triton,\n",
    "        quantize_config=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1f3d52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_messages(prompt_template, model=\"gpt-3.5-turbo\"):\n",
    "    return len(tokenizer(prompt_template, return_tensors='pt').input_ids.cuda().tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "604d244c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_messages(text):\n",
    "    # This is a placeholder for your actual implementation\n",
    "    return text.split(\"\\n\")\n",
    "\n",
    "def split_into_chunks(text, max_tokens=2000):\n",
    "    messages = split_into_messages(text)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_tokens = 0\n",
    "    for message in messages:\n",
    "        message_tokens = num_tokens_from_messages(message, model=\"gpt-3.5-turbo\")\n",
    "        if current_tokens + message_tokens > max_tokens:\n",
    "            # If adding this message would exceed the max tokens, start a new chunk\n",
    "            chunks.append('\\n'.join(current_chunk))\n",
    "            current_chunk = [message]\n",
    "            current_tokens = message_tokens\n",
    "        else:\n",
    "            # Otherwise, add the message to the current chunk\n",
    "            current_chunk.append(message)\n",
    "            current_tokens += message_tokens\n",
    "    # Don't forget the last chunk!\n",
    "    if current_chunk:\n",
    "        chunks.append('\\n'.join(current_chunk))\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e285df1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_string(input_str):\n",
    "    if \"</s>\" in input_str:\n",
    "        return input_str\n",
    "    else:\n",
    "        last_comma_index = input_str.rfind(',')\n",
    "        if last_comma_index == -1:\n",
    "            return input_str  # No comma found, return the string as is\n",
    "        else:\n",
    "            return input_str[:last_comma_index] + '}' + input_str[last_comma_index+1:]\n",
    "        \n",
    "def get_json_of_string(incorrect, pattern=r'\\{[^\\}]*\\}'):\n",
    "    match = re.search(r'{(.*)}', incorrect, re.DOTALL)\n",
    "    if match:\n",
    "        return \"{\" + match.group(1).replace('{', '[').replace('}', ']') + \"}\"\n",
    "    else:\n",
    "        return \"{}\"\n",
    "\n",
    "\n",
    "def string_to_dict(my_string):\n",
    "    # Load the JSON string into a list of tuples\n",
    "    tuples_list = json.JSONDecoder(object_pairs_hook=list).decode(my_string)\n",
    "\n",
    "    # Create a new dictionary to hold the final result\n",
    "    final_dict = {}\n",
    "\n",
    "    # Iterate over the list of tuples\n",
    "    for key, value in tuples_list:\n",
    "        # If the key is already in the final dictionary, append the value\n",
    "        # to the list of values for that key\n",
    "        if key in final_dict:\n",
    "            # Ensure the value is in a list form\n",
    "            if not isinstance(final_dict[key], list):\n",
    "                final_dict[key] = [final_dict[key]]\n",
    "            final_dict[key].append(value)\n",
    "        else:\n",
    "            # If the key is not in the final dictionary, add it with the value\n",
    "            final_dict[key] = value\n",
    "\n",
    "    return final_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53c1858e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dictionaries(dict1, dict2):\n",
    "    union_dict = dict1.copy()\n",
    "\n",
    "    for key, value in dict2.items():\n",
    "        if key in union_dict:\n",
    "            if isinstance(union_dict[key], list):\n",
    "                if value not in union_dict[key]:\n",
    "                    union_dict[key].append(value)\n",
    "            else:\n",
    "                if union_dict[key] != value:\n",
    "                    union_dict[key] = [union_dict[key], value]\n",
    "        else:\n",
    "            union_dict[key] = value\n",
    "\n",
    "    return union_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff36d705",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_from_tags(element):\n",
    "    if isinstance(element, NavigableString):\n",
    "        return element\n",
    "    if element.name == 'mi':\n",
    "        return str(element)\n",
    "    return ''.join(get_text_from_tags(child) for child in element.children)\n",
    "\n",
    "def parse_html(file_path, clean=True):\n",
    "    with open(file_path, 'r', encoding='utf-8') as html_file:\n",
    "        soup = BeautifulSoup(html_file, 'html.parser')\n",
    "\n",
    "    texts = get_text_from_tags(soup)\n",
    "    if clean:\n",
    "        matches = re.findall(r'<mi(.*?)</mi>', texts)\n",
    "        for match in matches:\n",
    "            original_string = f'<mi{match}</mi>'\n",
    "            replaced_string = re.sub(r'<.*?>(.*?)</.*?>', r'<|\\1|>', original_string)\n",
    "            texts = texts.replace(original_string, replaced_string)\n",
    "\n",
    "    return texts\n",
    "\n",
    "# call the function with your HTML file path\n",
    "page = parse_html(f'{file_code}.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb9f95a7-690d-49a1-947e-2d1c107a5f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(prompt):\n",
    "    return f'''### SYSTEM:\\n{prompt[0]['content']}\\n\\n\n",
    "### USER:\\n{prompt[1]['content']}\\n\\n\n",
    "### ASSISTANT:\\n{prompt[2]['content']}\\n\\n\n",
    "### USER:\\n{prompt[3]['content']}\\n\\n\n",
    "### ASSISTANT:\\n'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d0568d3-d765-4c53-ac27-c67650dad1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_pattern(input_string):\n",
    "    pattern = r\"<\\|[^<\\|>]*\\|>\"\n",
    "    if re.search(pattern, input_string):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f0f85cb-e25a-4a21-81a4-c8c143a47c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_output(output):\n",
    "    pos = output.index('Do not include the angle brackets in the dictionary')\n",
    "    end_of_string = output[pos:]\n",
    "    print(end_of_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abf7ee5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "1105 prompt tokens counted.\n"
     ]
    }
   ],
   "source": [
    "text = page\n",
    "chunks = split_into_chunks(text, max_tokens=512)\n",
    "i = 0\n",
    "\n",
    "while not contains_pattern(chunks[i]):\n",
    "    i += 1\n",
    "print(i)\n",
    "\n",
    "question = chunks[i]\n",
    "\n",
    "actual_total_tokens = 0\n",
    "completion_tokens = 0\n",
    "prompt_tokens = 0\n",
    "\n",
    "prompt = [\n",
    "        {'role': 'system',\n",
    "         'content': 'You are a helpful research assistant tasked with converting long paragraphs into a JSON '\n",
    "                    'dictionary. The goal is to identify and classify each individual mathematical symbol, variable,'\n",
    "                    ' and identifier in the text marked between \"<||>\". The dictionary should store the identifiers as '\n",
    "                    'keys and their corresponding definitions as values in an array format. '},\n",
    "        {'role': 'system', 'name': 'example_user', 'content': '''A relational model is a triple <|M|>‚Ä≤=(<|X|>,<|R|>,\n",
    "        <|v|>), where <|X|> is a set of states, <|R|><|‚äÜ|><|X|><|√ó|><|X|> is a binary relation on <|X|>, \n",
    "        and <|v|>:<|ùñØùóãùóàùóâ|>‚Üí2<|X|> is a valuation. Given a relational model <|M|>‚Ä≤, the satisfaction relation between \n",
    "        points <|x<|‚àà<|X<| and formulas <|œÜ<|‚àà<|‚Ñí<|<|ùñ™ùñ†<| is defined inductively by <|M|>‚Ä≤,<|x|>‚ä®<|ùñ™|><|œÜ|>‚áî for all \n",
    "        <|y|>‚àà<|X|>,<|x|><|R|><|y|> implies <|M|>‚Ä≤,<|y|>‚ä®<|œÜ|><|M|>‚Ä≤,<|x|>‚ä®<|ùñ†|><|œÜ|><||>‚áî for all <|y|>‚àà<|X|>,\n",
    "        <|M|>‚Ä≤,<|y|>‚ä®<|œÜ|>'''},\n",
    "        {'role': 'system', 'name': 'example_assistant', 'content': '''identifiers = {\n",
    "            \"M\": [\"Model\", \"Expertise Model\"],\n",
    "            \"M'\": \"Relational model\",\n",
    "            \"X\": \"Set of states\",\n",
    "            \"R\": \"Binary relation on X\",\n",
    "            \"v\": \"Valuation\",\n",
    "            \"ùñØùóãùóàùóâ\": \"Set of propositions\",\n",
    "            \"M'\": \"Relational model\",\n",
    "            \"x\": \"Point in X\",\n",
    "            \"œÜ\": \"Formula in ùñ™ùñ†\",\n",
    "            \"‚Ñí_{ùñ™ùñ†}\": \"Set of formulas\",\n",
    "            \"ùñ™\": \"Modal operator K\",\n",
    "            \"ùñ†\": \"Modal operator A\",\n",
    "            \"y\": \"Point in X\",\n",
    "            \"‚ä®\": \"Satisfaction relation\",\n",
    "            \"‚áî\": \"If and only if operator\",\n",
    "            \"‚àà\": \"Element of a set\",\n",
    "            \"‚äÜ\": \"Subset of a set\",\n",
    "            \"√ó\": \"Cartesian product operator\",\n",
    "            \"‚Üí\": \"Function or implication operator\",\n",
    "            \"for all\": \"Universal quantifier\"\n",
    "            }'''},\n",
    "        {'role': 'user', 'content': f'Generate a JSON dictionary for the following text\\n```txt\\n{question}```. '\n",
    "                                    'Only consider the mathematical identifiers inside \"<||>\" for the dictionary. '\n",
    "                                    'Do not consider any other identifier other than those marked. Consider all the '\n",
    "                                    'identifiers individually. Do not skip any identifier, mention all the identifiers '\n",
    "                                    'inside \"<||>\" in your dictionary. Do not include the angle brackets in the '\n",
    "                                    'dictionary.'}\n",
    "    ]\n",
    "\n",
    "\n",
    "open_prompt = get_prompt(prompt)\n",
    "\n",
    "prompt_size = num_tokens_from_messages(open_prompt)\n",
    "print(f\"{prompt_size} prompt tokens counted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "612640fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TheBloke/StableBeluga2-70B-GPTQ\n",
      "Do not include the angle brackets in the dictionary.\n",
      "\n",
      "\n",
      "### ASSISTANT:\n",
      " identifiers = {\n",
      "            \"LSTM RNN\": \"Long Short-Term Memory Recurrent Neural Network\",\n",
      "            \"ANOMALOUS\": \"Anomaly Detection Algorithm\",\n",
      "            \"NCC\": \"Normalized Cross Correlation\",\n",
      "            \"Corr\": \"Cross-Correlation\",\n",
      "            \"x\": \"First signal\",\n",
      "            \"y\": \"Second signal\",\n",
      "            \"n\": \"Time step\",\n",
      "            \"N\": \"Total number of time steps\",\n",
      "            \"x[n]\": \"Value of x at time step n\",\n",
      "            \"y[n]\": \"Value of y at time step n\",\n",
      "            \"sum\": \"Summation operator\",\n",
      "            \"1\": \"Constant 1\",\n",
      "            \"-1\": \"Constant -1\",\n",
      "            \"1\": \"Constant 1\",\n",
      "            \"N\": \"Total number of time steps\",\n",
      "            \"x[n]\": \"Value of x at time step n\",\n",
      "            \"y[n]\": \"Value of y at time step n\",\n",
      "            \"sum\": \"Summation operator\",\n",
      "            \"1\": \"Constant 1\",\n",
      "            \"-1\": \"Constant -1\",\n",
      "            \"1\": \"Constant 1\",\n",
      "            \"N\": \"Total number of time steps\",\n",
      "            \"x[n]\": \"Value of x at time step n\",\n",
      "            \"y[n]\": \"Value of y at time step n\",\n",
      "            \"sum\": \"Summation operator\",\n",
      "            \"1\": \"Constant 1\",\n",
      "            \"-1\": \"Constant -1\",\n",
      "            \"1\": \"Constant 1\",\n",
      "            \"N\": \"Total number of time steps\",\n",
      "            \"x[n]\": \"Value of x at time step n\",\n",
      "            \"y[n]\": \"Value of y at time step n\",\n",
      "            \"sum\": \"Summation operator\",\n",
      "            \"1\": \"Constant 1\",\n",
      "            \"-1\": \"Constant -1\",\n",
      "            \"1\": \"Constant 1\",\n",
      "            \"N\": \"Total number of time steps\",\n",
      "            \"x[n]\": \"Value of x at time step n\",\n",
      "            \"y[n]\": \"Value of y at time step n\",\n",
      "            \"sum\": \"\n",
      "Time taken: 0:00:46.353167\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using {model_name_or_path}\")\n",
    "start_time = datetime.now()\n",
    "\n",
    "input_ids = tokenizer(open_prompt, return_tensors='pt').input_ids.cuda()\n",
    "output = model.generate(inputs=input_ids, temperature=0.5, max_new_tokens=512)\n",
    "output_string = tokenizer.decode(output[0])\n",
    "print_output(output_string)\n",
    "\n",
    "total_time_taken = datetime.now() - start_time\n",
    "print(f\"Time taken: {total_time_taken}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "507e7993",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1614 509 1105\n"
     ]
    }
   ],
   "source": [
    "actual_total_tokens += num_tokens_from_messages(output_string)\n",
    "completion_tokens += num_tokens_from_messages(output_string) - num_tokens_from_messages(open_prompt)\n",
    "prompt_tokens += num_tokens_from_messages(open_prompt)\n",
    "print(actual_total_tokens, completion_tokens, prompt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ff56fd8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Safely convert the dictionary string to a dictionary using json.loads()\n",
    "try:\n",
    "    ind = output_string.index('Do not include the angle brackets in the dictionary.')\n",
    "    correct_output_string = modify_string(output_string[ind:])\n",
    "    dic_output_string = get_json_of_string(correct_output_string)\n",
    "    dictionary = [string_to_dict(dic_output_string)]\n",
    "except Exception as e:\n",
    "    dictionary = [{}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "acbf76ad-2acb-41dc-be3d-aab9c19c15c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'LSTM RNN': 'Long Short-Term Memory Recurrent Neural Network', 'ANOMALOUS': 'Anomaly Detection Algorithm', 'NCC': 'Normalized Cross Correlation', 'Corr': 'Cross-Correlation', 'x': 'First signal', 'y': 'Second signal', 'n': 'Time step', 'N': ['Total number of time steps', 'Total number of time steps', 'Total number of time steps', 'Total number of time steps', 'Total number of time steps'], 'x[n]': ['Value of x at time step n', 'Value of x at time step n', 'Value of x at time step n', 'Value of x at time step n', 'Value of x at time step n'], 'y[n]': ['Value of y at time step n', 'Value of y at time step n', 'Value of y at time step n', 'Value of y at time step n', 'Value of y at time step n'], 'sum': ['Summation operator', 'Summation operator', 'Summation operator', 'Summation operator'], '1': ['Constant 1', 'Constant 1', 'Constant 1', 'Constant 1', 'Constant 1', 'Constant 1', 'Constant 1', 'Constant 1'], '-1': ['Constant -1', 'Constant -1', 'Constant -1', 'Constant -1']}]\n"
     ]
    }
   ],
   "source": [
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d18ab90-38c2-4724-9a03-239fc73078d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_loop(prompt):\n",
    "    return f'''### SYSTEM:\\n{prompt[0]['content']}\\n\\n\n",
    "### USER:\\n{prompt[1]['content']}\\n\\n\n",
    "### ASSISTANT:\\n{prompt[2]['content']}\\n\\n\n",
    "### SYSTEM:\\n{prompt[3]['content']}\\n\\n\n",
    "### USER:\\n{prompt[4]['content']}\\n\\n\n",
    "### ASSISTANT:\\n'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5bab7543",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17 of 27\n",
      "Iteration 18 of 27\n",
      "Iteration 19 of 27\n",
      "Iteration 20 of 27\n",
      "Iteration 21 of 27\n",
      "Iteration 22 of 27\n",
      "Iteration 23 of 27\n",
      "Iteration 24 of 27\n",
      "Iteration 25 of 27\n",
      "Iteration 26 of 27\n",
      "Iteration 27 of 27\n",
      "Iteration 28 of 27\n",
      "Iteration 29 of 27\n",
      "Iteration 30 of 27\n",
      "Iteration 31 of 27\n",
      "Iteration 32 of 27\n",
      "Iteration 33 of 27\n",
      "Iteration 34 of 27\n",
      "Iteration 35 of 27\n",
      "\n",
      "\n",
      "\n",
      "1586 prompt tokens counted.\n",
      "\n",
      "Do not include the angle brackets in your dictionary.\n",
      "\n",
      "\n",
      "### ASSISTANT:\n",
      " {\n",
      "    \"x\": \"Vector of time series data (stock transaction volume)\",\n",
      "    \"y\": \"Vector of time series data (stock transaction volume)\",\n",
      "    \"N\": \"Number of days in the series\",\n",
      "    \"Corr\": \"Cross-correlation\",\n",
      "    \"NCC\": \"Normalized Cross Correlation\",\n",
      "    \"n\": \"Time step\",\n",
      "    \"x[n]\": \"Value of x at time step n\",\n",
      "    \"y[n]\": \"Value of y at time step n\",\n",
      "    \"sum\": \"Summation operator\",\n",
      "    \"1\": \"Constant 1\",\n",
      "    \"-1\": \"Constant -1\",\n",
      "    \"xcorr\": \"Built-in Matlab feature for normalized cross-correlation\"\n",
      "}</s>\n",
      "684 2691\n",
      "Actual total tokens till now: 3375\n",
      "Iteration 36 of 27\n",
      "\n",
      "\n",
      "\n",
      "1659 prompt tokens counted.\n",
      "\n",
      "Do not include the angle brackets in your dictionary.\n",
      "\n",
      "\n",
      "### ASSISTANT:\n",
      " {\n",
      "    \"day\": \"Day in the time series\",\n",
      "    \"window_size\": \"Size of the window\",\n",
      "    \"w\": \"Weight factor\",\n",
      "    \"d\": \"Day offset\",\n",
      "    \"p\": \"Pattern number in comparison\",\n",
      "    \"NCR\": \"Normalized Cross-Correlation\",\n",
      "    \"Figure 7\": \"Anomalous time series (window-based)\",\n",
      "    \"Figure 8\": \"NCR (window-based)\",\n",
      "    \"Figure 9\": \"Anomalous time series (day based)\",\n",
      "    \"Figure 10\": \"NCR (day based)\"\n",
      "}</s>\n",
      "827 4350\n",
      "Actual total tokens till now: 5177\n",
      "Iteration 37 of 27\n",
      "Iteration 38 of 27\n",
      "Iteration 39 of 27\n",
      "Iteration 40 of 27\n",
      "Iteration 41 of 27\n",
      "Iteration 42 of 27\n",
      "Iteration 43 of 27\n",
      "Time taken: 0:00:31.779829\n",
      "Total time taken: 0:01:18.132954\n",
      "5177 827 4350\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.now()\n",
    "number_of_dictionaries = 0\n",
    "for chunk in chunks:\n",
    "    print(f\"Iteration {i} of {len(chunks)}\")\n",
    "    i += 1\n",
    "    if chunk == question:\n",
    "        continue\n",
    "    if not contains_pattern(chunk):\n",
    "        continue\n",
    "    question = chunk\n",
    "    \n",
    "    if prompt_size > 1600:\n",
    "        number_of_dictionaries += 1\n",
    "        print(\"\\nNew dictionary\\n\")\n",
    "        dictionary.append({})\n",
    "    \n",
    "    prompt = [\n",
    "            {'role': 'system',\n",
    "             'content': 'You are a helpful research assistant tasked with converting long paragraphs into a JSON '\n",
    "                        'dictionary. '\n",
    "                        'The goal is to identify and classify each individual mathematical symbol, variable, '\n",
    "                        'and identifier in the text marked between \"<||>\"'\n",
    "                        'The dictionary should store the identifiers as keys and their corresponding definitions as '\n",
    "                        'values in an array format. '},\n",
    "            {'role': 'system', 'name': 'example_user', 'content': '''A relational model is a triple <|M|>‚Ä≤=(<|X|>,<|R|>,\n",
    "            <|v|>), where <|X|> is a set of states, <|R|><|‚äÜ|><|X|><|√ó|><|X|> is a binary relation on <|X|>, \n",
    "            and <|v|>:<|ùñØùóãùóàùóâ|>‚Üí2<|X|> is a valuation. Given a relational model <|M|>‚Ä≤, the satisfaction relation \n",
    "            between points <|x<|‚àà<|X<| and formulas <|œÜ<|‚àà<|‚Ñí<|<|ùñ™ùñ†<| is defined inductively by <|M|>‚Ä≤,\n",
    "            <|x|>‚ä®<|ùñ™|><|œÜ|>‚áî for all <|y|>‚àà<|X|>,<|x|><|R|><|y|> implies <|M|>‚Ä≤,<|y|>‚ä®<|œÜ|><|M|>‚Ä≤,\n",
    "            <|x|>‚ä®<|ùñ†|><|œÜ|><||>‚áî for all <|y|>‚àà<|X|>,<|M|>‚Ä≤,<|y|>‚ä®<|œÜ|>'''},\n",
    "            {'role': 'system', 'name': 'example_assistant', 'content': '''identifiers = {\n",
    "            \"M\": [\"Model\", \"Expertise Model\"],\n",
    "            \"M'\": \"Relational model\",\n",
    "            \"X\": \"Set of states\",\n",
    "            \"R\": \"Binary relation on X\",\n",
    "            \"v\": \"Valuation\",\n",
    "            \"ùñØùóãùóàùóâ\": \"Set of propositions\",\n",
    "            \"M'\": \"Relational model\",\n",
    "            \"x\": \"Point in X\",\n",
    "            \"œÜ\": \"Formula in ùñ™ùñ†\",\n",
    "            \"‚Ñí_{ùñ™ùñ†}\": \"Set of formulas\",\n",
    "            \"ùñ™\": \"Modal operator K\",\n",
    "            \"ùñ†\": \"Modal operator A\",\n",
    "            \"y\": \"Point in X\",\n",
    "            \"‚ä®\": \"Satisfaction relation\",\n",
    "            \"‚áî\": \"If and only if operator\",\n",
    "            \"‚àà\": \"Element of a set\",\n",
    "            \"‚äÜ\": \"Subset of a set\",\n",
    "            \"√ó\": \"Cartesian product operator\",\n",
    "            \"‚Üí\": \"Function or implication operator\",\n",
    "            \"for all\": \"Universal quantifier\"\n",
    "            }'''},\n",
    "            {'role': 'system',\n",
    "             'content': f'Given is already a pre existing dictionary. Your job is to extend this dictionary. Do not '\n",
    "                        f'remove any pre existing definitions from this dictionary.'\n",
    "                        f'\\n{dictionary[number_of_dictionaries]}. If there is nothing to mention, reply with an empty '\n",
    "                        f'dictionary'},\n",
    "            {'role': 'user', 'content': f'Generate a JSON dictionary for the following text: {question}. '\n",
    "                                        'Only consider the mathematical identifiers inside \"<||>\" for the dictionary. '\n",
    "                                        'Do not consider any other identifier other than those marked. '\n",
    "                                        'Consider all the identifiers individually. Do not skip any identifier, mention'\n",
    "                                        ' all the identifiers inside \"<||>\" in your dictionary. '\n",
    "                                        'Do not include the angle brackets in your dictionary.'}\n",
    "        ]\n",
    "    \n",
    "    open_prompt = get_prompt_loop(prompt)\n",
    "    \n",
    "    prompt_size = num_tokens_from_messages(open_prompt)\n",
    "    print(f\"\\n\\n\\n{prompt_size} prompt tokens counted.\\n\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            input_ids = tokenizer(open_prompt, return_tensors='pt').input_ids.cuda()\n",
    "            output = model.generate(inputs=input_ids, temperature=0.5, max_new_tokens=512, repetition_penalty=1.05)\n",
    "            output_string = tokenizer.decode(output[0])\n",
    "            \n",
    "            actual_total_tokens += num_tokens_from_messages(output_string)\n",
    "            completion_tokens += num_tokens_from_messages(output_string) - num_tokens_from_messages(open_prompt)\n",
    "            prompt_tokens += num_tokens_from_messages(open_prompt)\n",
    "\n",
    "            ind = output_string.index('Do not include the angle brackets in your dictionary.')\n",
    "            print(output_string[ind:])\n",
    "            \n",
    "            print(completion_tokens, prompt_tokens)\n",
    "    \n",
    "            print(f\"Actual total tokens till now: {actual_total_tokens}\")\n",
    "\n",
    "            try:\n",
    "                correct_output_string = modify_string(output_string[ind:])\n",
    "                dic_output_string = get_json_of_string(correct_output_string)\n",
    "                new_dictionary = string_to_dict(dic_output_string)\n",
    "            except Exception as e:\n",
    "                print(\"INCORRECT DICTIONARY\")\n",
    "            dictionary[number_of_dictionaries] = merge_dictionaries(dictionary[number_of_dictionaries], new_dictionary)\n",
    "            \n",
    "            break\n",
    "        except Exception as e:\n",
    "            number_of_dictionaries += 1\n",
    "            dictionary.append({})\n",
    "            print(f\"Exception occurred: {e}\")\n",
    "            print(\"Retrying...\")\n",
    "total_time_taken += (datetime.now() - start_time)\n",
    "print(f\"Time taken: {datetime.now() - start_time }\")\n",
    "print(f\"Total time taken: {total_time_taken}\")\n",
    "print(actual_total_tokens, completion_tokens, prompt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04278ee2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dct = {}\n",
    "for dic in dictionary:\n",
    "    dct = merge_dictionaries(dct, dic)\n",
    "# pprint.pprint(dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d4212dbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Corr': '['Cross-Correlation', 'Cross-correlation']'\n",
      "'x': '['First signal', 'Vector of time series data (stock transaction volume)']'\n",
      "'y': '['Second signal', 'Vector of time series data (stock transaction volume)']'\n",
      "'N': '['Total number of time steps', 'Total number of time steps', 'Total number of time steps', 'Total number of time steps', 'Total number of time steps', 'Number of days in the series']'\n",
      "'x[n]': '['Value of x at time step n', 'Value of x at time step n', 'Value of x at time step n', 'Value of x at time step n', 'Value of x at time step n']'\n",
      "'y[n]': '['Value of y at time step n', 'Value of y at time step n', 'Value of y at time step n', 'Value of y at time step n', 'Value of y at time step n']'\n",
      "'sum': '['Summation operator', 'Summation operator', 'Summation operator', 'Summation operator']'\n",
      "'1': '['Constant 1', 'Constant 1', 'Constant 1', 'Constant 1', 'Constant 1', 'Constant 1', 'Constant 1', 'Constant 1']'\n",
      "'-1': '['Constant -1', 'Constant -1', 'Constant -1', 'Constant -1']'\n"
     ]
    }
   ],
   "source": [
    "for key, value in dct.items():\n",
    "    if type(value) == list:\n",
    "        print(f\"'{key}': '{value}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed252bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_list(input_list):\n",
    "    output_list = []\n",
    "    for i in input_list:\n",
    "        if isinstance(i, list):\n",
    "            output_list.extend(flatten_list(i))\n",
    "        else:\n",
    "            output_list.append(i)\n",
    "    return output_list\n",
    "\n",
    "\n",
    "def remove_duplicates(input_list):\n",
    "    output_list = []\n",
    "    for item in input_list:\n",
    "        if item not in output_list:\n",
    "            output_list.append(item)\n",
    "    if len(output_list) == 1:\n",
    "        return output_list[0]\n",
    "    return output_list\n",
    "\n",
    "\n",
    "def process_value(v):\n",
    "    if isinstance(v, str):\n",
    "        new_v = v.replace('$', '')\n",
    "        while '\\\\\\\\' in new_v:\n",
    "            new_v = new_v.replace('\\\\\\\\', '\\\\').replace('\\n', '')\n",
    "    else:  # Assuming it's a list\n",
    "        new_v = flatten_list([process_value(val) for val in v])\n",
    "        \n",
    "    return remove_duplicates(new_v) if isinstance(new_v, list) else new_v\n",
    "\n",
    "\n",
    "def reduce_pairs(dictionary):\n",
    "    new_dict = {}\n",
    "    for k, v in dictionary.items():\n",
    "        # reduce key backslashes\n",
    "        new_k = k.replace('$', '')\n",
    "        while '\\\\\\\\' in new_k:\n",
    "            new_k = new_k.replace('\\\\\\\\', '\\\\')\n",
    "\n",
    "        # process value\n",
    "        new_v = process_value(v)\n",
    "\n",
    "        new_dict[new_k] = new_v\n",
    "\n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "85c8083f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dict_without_backslashes = reduce_pairs(dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ab472ef2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pprint.pprint(dict_without_backslashes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "09b426f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_json = dict_without_backslashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0dec9c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{file_code}_mcdict.json', 'r', encoding='utf-8') as f:\n",
    "    mc_dict_original = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4284b146",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to create a hex code (a binary representation of the key)\n",
    "def get_hex_code(key):\n",
    "    return key.encode().hex()\n",
    "\n",
    "mc_dict_original['_author'] = model_name_or_path\n",
    "\n",
    "# Iterate over your dictionary and fill the new one\n",
    "for key, values in parsed_json.items():\n",
    "    # Determine the base key and the affix\n",
    "    base_key = re.match(r\"^[^*'_^,(\\[]*\", key).group()\n",
    "    affix = key[len(base_key):]\n",
    "\n",
    "    hex_code = get_hex_code(base_key)\n",
    "    values = values if isinstance(values, list) else [values]\n",
    "\n",
    "    if hex_code in mc_dict_original[\"concepts\"]:\n",
    "        k = list(mc_dict_original[\"concepts\"][hex_code][\"identifiers\"].keys())[0]\n",
    "        new_identifier = []\n",
    "        for value in values:\n",
    "            mc_dict_original[\"concepts\"][hex_code][\"identifiers\"][k].append({\n",
    "                \"affixes\": [affix] if affix else [],\n",
    "                \"arity\": 0,\n",
    "                \"description\": value\n",
    "            })\n",
    "    else:\n",
    "        if hex_code not in mc_dict_original[\"concepts\"]:\n",
    "            mc_dict_original[\"concepts\"][hex_code] = {\n",
    "                \"_surface\": {\n",
    "                    \"text\": base_key,\n",
    "                    \"unicode_name\": base_key if len(base_key) != 1 else unicodedata.name(base_key)\n",
    "                },\n",
    "                \"identifiers\": {\n",
    "                    'default': []\n",
    "                }\n",
    "            }\n",
    "\n",
    "        for value in values:\n",
    "            mc_dict_original[\"concepts\"][hex_code][\"identifiers\"][\"default\"].append({\n",
    "                \"affixes\": [affix] if affix else [],\n",
    "                \"arity\": 0,\n",
    "                \"description\": value\n",
    "            })\n",
    "\n",
    "\n",
    "# Convert new dictionary to a sorted dictionary\n",
    "sorted_dict = dict(sorted(mc_dict_original[\"concepts\"].items(), key=lambda x: (len(x[0]), x[0])))\n",
    "mc_dict_original[\"concepts\"] = sorted_dict\n",
    "\n",
    "# Convert new dictionary to JSON\n",
    "json_str = json.dumps(mc_dict_original, indent=4, ensure_ascii=False)\n",
    "\n",
    "#print(json_str)\n",
    "\n",
    "with open(f'{file_code}-{model_name}_mcdict.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(mc_dict_original, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a79ec2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_from_tags(element):\n",
    "    if isinstance(element, NavigableString):\n",
    "        return element\n",
    "    if element.name == 'mi':\n",
    "        return str(element)\n",
    "    return ''.join(get_text_from_tags(child) for child in element.children)\n",
    "\n",
    "def parse_html(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as html_file:\n",
    "        soup = BeautifulSoup(html_file, 'html.parser')\n",
    "\n",
    "    texts = get_text_from_tags(soup)\n",
    "    return texts\n",
    "\n",
    "def find_mi_strings(text):\n",
    "    pattern = r'(<mi.*?</mi>)'\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "    return matches\n",
    "\n",
    "# call the function with your HTML file path\n",
    "page = parse_html(f'{file_code}.html')\n",
    "matches = find_mi_strings(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "64a0cba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_dict = mc_dict_original\n",
    "with open(f'{file_code}_anno.json', encoding='utf-8') as fp:\n",
    "    parsed_annotation = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "163356dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_index_from_char_index(message, key, char_index):\n",
    "    i = 0\n",
    "    index = -1\n",
    "    for word in message:\n",
    "        if key in word:\n",
    "            index = i\n",
    "        i += 1\n",
    "    return index\n",
    "\n",
    "def expand_string_to_tokens(message, index, num_tokens_right=25, num_tokens_left=75):\n",
    "    words = message.split()  # Split the message into words\n",
    "\n",
    "    # Start at the index where the center word is\n",
    "    left_index = right_index = index\n",
    "\n",
    "    tokens_counter_right = num_tokens_from_messages(words[right_index])\n",
    "    tokens_counter_left = num_tokens_from_messages(words[left_index])\n",
    "\n",
    "    # Expand to the left from the center index until you reach num_tokens_left\n",
    "    while tokens_counter_left < num_tokens_left and left_index > 0:\n",
    "        left_index -= 1\n",
    "        tokens_counter_left += num_tokens_from_messages(words[left_index])\n",
    "\n",
    "    # Expand to the right from the center index until you reach num_tokens_right\n",
    "    while tokens_counter_right < num_tokens_right and right_index < len(words) - 1:\n",
    "        right_index += 1\n",
    "        tokens_counter_right += num_tokens_from_messages(words[right_index])\n",
    "\n",
    "    # Combine the words back into a string and return\n",
    "    return ' '.join(words[left_index:right_index + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "01b6b971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_text(text, replacement, exception):\n",
    "    # Find all matches\n",
    "    matches = re.findall(r'<mi(.*?)</mi>', text)\n",
    "    \n",
    "    for match in matches:\n",
    "        original_string = f'<mi{match}</mi>'\n",
    "        \n",
    "        # Skip exception\n",
    "        if original_string == exception:\n",
    "            continue\n",
    "        \n",
    "        # Replace match\n",
    "        replaced_string = re.sub(r'<.*?>(.*?)</.*?>', r'\\1', original_string)\n",
    "        text = text.replace(original_string, replaced_string)\n",
    "\n",
    "    return text\n",
    "\n",
    "def get_context(match):\n",
    "    match_len = len(match)\n",
    "    new_page = replace_text(page, '', match)\n",
    "    char_index = new_page.index(match) + int(match_len/2)\n",
    "    word_index = get_word_index_from_char_index(new_page, char_index)\n",
    "    section = expand_string_to_tokens(new_page, word_index)\n",
    "    section = re.sub(r'<.*?>(.*?)</.*?>', r'<<\\1>>', section)\n",
    "    return match, section\n",
    "\n",
    "# Function to create a hex code (a binary representation of the key)\n",
    "def get_hex_code(key):\n",
    "    return key.encode('utf-8').hex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7706ec86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_trailing_tags(s):\n",
    "    parts = re.split('(<mi)', s)\n",
    "    for i in range(1, len(parts), 2):\n",
    "        if '>' not in parts[i + 1]:\n",
    "            parts[i] = ''\n",
    "            parts[i + 1] = ''\n",
    "    return ''.join(parts)\n",
    "\n",
    "def get_definition_of_id(dict_id, identifier):\n",
    "    \n",
    "    try:\n",
    "        hex_code = get_hex_code(identifier)\n",
    "        index = parsed_annotation['mi_anno'][dict_id]['concept_id']\n",
    "        key = list(parsed_dict['concepts'][hex_code]['identifiers'].keys())[0]\n",
    "        return f\"({parsed_dict['concepts'][hex_code]['identifiers'][key][index]['description']})\"\n",
    "    except Exception as e:\n",
    "        return \"\"\n",
    "\n",
    "def get_context(match):\n",
    "    key_word = page.index(match) + len(match)\n",
    "    last_index = min(len(page), key_word + 500)\n",
    "    first_index = max(0, key_word - 3000)\n",
    "    context_window = page[first_index:last_index]\n",
    "    \n",
    "    reg_matches = re.findall(r'<mi(.*?)</mi>', context_window)\n",
    "    \n",
    "    identifier = None\n",
    "    \n",
    "    for reg_match in reg_matches:\n",
    "        original_string = f'<mi{reg_match}</mi>'\n",
    "        soup = BeautifulSoup(original_string, 'html.parser')\n",
    "        \n",
    "        \n",
    "        tags = soup.find_all('mi')\n",
    "        \n",
    "        if original_string == match:\n",
    "            identifier = tags[0].text\n",
    "            continue\n",
    "\n",
    "        context_window = context_window.replace(original_string,\n",
    "                                                f\"{tags[0].text}{get_definition_of_id(tags[0].get('id'), tags[0].text)}\")\n",
    "    \n",
    "    context_window = re.sub(r'<mi.*?>(.*?)<\\/mi>', r'<<\\1>>', context_window)\n",
    "    \n",
    "    context_window = remove_trailing_tags(context_window)\n",
    "    context_window = re.sub(r'^(?!.*<mi.*).*<\\/mi>', '', context_window, flags=re.DOTALL)\n",
    "        \n",
    "    index = 0\n",
    "    for word in context_window.split():\n",
    "        if f\"<<{identifier}>>\" in word:\n",
    "            word_index = index\n",
    "        index += 1\n",
    "    \n",
    "    if word_index == -1:\n",
    "        return context_window\n",
    "    else:\n",
    "        context_window = expand_string_to_tokens(context_window, word_index)\n",
    "    return context_window\n",
    "\n",
    "#print(get_context('<mi id=\"S1.p2.1.m1.1.1.3\" xref=\"S1.p2.1.m1.1.1.3.cmml\">œÜ</mi>'))\n",
    "#get_context('<mi id=\"S1.p2.1.m1.1.1.2\" xref=\"S1.p2.1.m1.1.1.2.cmml\">ùñ§</mi>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "62ec5036-7177-46d0-9728-571edb9f6bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_anno(prompt):\n",
    "    return f'''### SYSTEM:\\n{prompt[0]['content']}\\n\\n\n",
    "### USER:\\n{prompt[1]['content']}\\n\\n\n",
    "### ASSISTANT:\\n'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f57696ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 of 35: ASSISTANT:\n",
      " 0</s>\n",
      "6 280\n",
      "0\n",
      "Iteration 2 of 35: ASSISTANT:\n",
      " 2</s>\n",
      "12 591\n",
      "2\n",
      "Iteration 3 of 35: ASSISTANT:\n",
      " 2</s>\n",
      "18 901\n",
      "2\n",
      "Iteration 4 of 35: 0\n",
      "Iteration 5 of 35: ASSISTANT:\n",
      " 0</s>\n",
      "24 1182\n",
      "0\n",
      "Iteration 6 of 35: ASSISTANT:\n",
      " 2</s>\n",
      "30 1501\n",
      "2\n",
      "Iteration 7 of 35: 0\n",
      "Iteration 8 of 35: ASSISTANT:\n",
      " 2</s>\n",
      "36 1819\n",
      "2\n",
      "Iteration 9 of 35: 0\n",
      "Iteration 10 of 35: None\n",
      "Iteration 11 of 35: ASSISTANT:\n",
      " 2</s>\n",
      "42 2128\n",
      "2\n",
      "Iteration 12 of 35: ASSISTANT:\n",
      " 2</s>\n",
      "48 2438\n",
      "2\n",
      "Iteration 13 of 35: 0\n",
      "Iteration 14 of 35: ASSISTANT:\n",
      " 0</s>\n",
      "54 2721\n",
      "0\n",
      "Iteration 15 of 35: ASSISTANT:\n",
      " 2</s>\n",
      "60 3034\n",
      "2\n",
      "Iteration 16 of 35: 0\n",
      "Iteration 17 of 35: ASSISTANT:\n",
      " 2</s>\n",
      "66 3351\n",
      "2\n",
      "Iteration 18 of 35: 0\n",
      "Iteration 19 of 35: 0\n",
      "Iteration 20 of 35: ASSISTANT:\n",
      " 0</s>\n",
      "72 3636\n",
      "0\n",
      "Iteration 21 of 35: ASSISTANT:\n",
      " 2</s>\n",
      "78 3956\n",
      "2\n",
      "Iteration 22 of 35: 0\n",
      "Iteration 23 of 35: ASSISTANT:\n",
      " 2</s>\n",
      "84 4272\n",
      "2\n",
      "Iteration 24 of 35: 0\n",
      "Iteration 25 of 35: 0\n",
      "Iteration 26 of 35: ASSISTANT:\n",
      " 0</s>\n",
      "90 4554\n",
      "0\n",
      "Iteration 27 of 35: ASSISTANT:\n",
      " 2</s>\n",
      "96 4870\n",
      "2\n",
      "Iteration 28 of 35: 0\n",
      "Iteration 29 of 35: ASSISTANT:\n",
      " 2</s>\n",
      "102 5189\n",
      "2\n",
      "Iteration 30 of 35: 0\n",
      "Iteration 31 of 35: 0\n",
      "Iteration 32 of 35: None\n",
      "Iteration 33 of 35: 0\n",
      "Iteration 34 of 35: None\n",
      "Iteration 35 of 35: 0\n",
      "Annotation completed\n",
      "Time taken: 0:00:29.395803\n",
      "Total time taken: 0:00:29.395762\n",
      "5291 102 5189\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.now()\n",
    "actual_total_tokens = 0\n",
    "completion_tokens = 0\n",
    "prompt_tokens = 0\n",
    "no_tags = 0\n",
    "no_keys = 0\n",
    "no_anno = 0\n",
    "i = 1\n",
    "for match in matches:\n",
    "    print(f\"Iteration {i} of {len(matches)}: \", end='')\n",
    "    i += 1\n",
    "    context = get_context(match)\n",
    "    match_variable = re.sub(r'<.*?>(.*?)</.*?>', r'\\1', match)\n",
    "    context_index = context.index(f\"<<{match_variable}>>\") + len(match_variable)\n",
    "    possible_affix = str(context[context_index+4:context_index+5]).replace(\"‚Ä≤\", \"'\")\n",
    "    soup = BeautifulSoup(match, 'html.parser')\n",
    "    mi_tag = soup.find('mi')\n",
    "    if mi_tag is not None and 'id' in mi_tag.attrs:\n",
    "        anno_id = mi_tag['id']\n",
    "    else:\n",
    "        print('TAG NOT FOUND', match)\n",
    "        no_tags += 1\n",
    "        continue\n",
    "    \n",
    "    hex_code = get_hex_code(match_variable)\n",
    "    if hex_code not in parsed_dict['concepts']:\n",
    "        match_variable = f\"{unidecode(match_variable)}\"\n",
    "        hex_code = get_hex_code(match_variable)\n",
    "        if hex_code not in parsed_dict['concepts']:\n",
    "            print(\"Key does not exist in the dictionary of concepts\", match_variable, hex_code)\n",
    "            no_keys += 1\n",
    "            continue\n",
    "    \n",
    "    if anno_id not in parsed_annotation['mi_anno']:\n",
    "        print(\"Annotation ID does not exist in annotation.json\", anno_id)\n",
    "        no_anno += 1\n",
    "        continue\n",
    "\n",
    "    k = list(parsed_dict[\"concepts\"][hex_code][\"identifiers\"].keys())[0]\n",
    "    mcdict = parsed_dict['concepts'][hex_code]['identifiers'][k]\n",
    "    \n",
    "    if len(mcdict) == 1:\n",
    "        parsed_annotation['mi_anno'][anno_id]['concept_id'] = 0\n",
    "        print('0')\n",
    "    elif len(mcdict) > 1:\n",
    "        prompt_mcdict = []\n",
    "\n",
    "        index = 0\n",
    "        for val in mcdict:\n",
    "            prompt_mcdict.append({'index': f\"{index}\", 'identifier': f\"{match_variable}{'' if len(val['affixes']) == 0 else val['affixes'][0]}\", 'description': val['description']})\n",
    "            index += 1\n",
    "            \n",
    "        prompt = [\n",
    "            {'role': 'system', 'content': 'You are a professional annotater API. Your job is to select a fitting annotation from a dictionary for a mathematical identifier.'},\n",
    "            {'role': 'user', 'content': f'''Given the following possible annotations:\\n```json\\n{prompt_mcdict}```.\n",
    "             Select the index for the most fitting description for the identifier <<{match_variable}>> from the following text.\n",
    "             The potential affix of the indentifier could be <<{possible_affix}>>. Take the affixes of the possible annotations into account.\n",
    "             Only return the value of the index and nothing else.\n",
    "             Do not add any explanation otherwise the API breaks.\n",
    "             The identifier has been marked with <<>>.\n",
    "             If you can't come up with an index, write 'None'\n",
    "             ```txt\n",
    "             {context}\n",
    "             ```'''}\n",
    "        ]\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                open_prompt = get_prompt_anno(prompt)\n",
    "                \n",
    "                input_ids = tokenizer(open_prompt, return_tensors='pt').input_ids.cuda()\n",
    "                output = model.generate(inputs=input_ids, temperature=0.5, max_new_tokens=512, repetition_penalty=1.05)\n",
    "                output_string = tokenizer.decode(output[0])\n",
    "                \n",
    "                actual_total_tokens += num_tokens_from_messages(output_string)\n",
    "                completion_tokens += num_tokens_from_messages(output_string) - num_tokens_from_messages(open_prompt)\n",
    "                prompt_tokens += num_tokens_from_messages(open_prompt)\n",
    "\n",
    "                ind = output_string.index('ASSISTANT:')\n",
    "                value = output_string[ind:]\n",
    "                print(value)\n",
    "                \n",
    "                print(completion_tokens, prompt_tokens)\n",
    "\n",
    "                try:\n",
    "                    index = int(int(re.search('\\d+', value).group()))\n",
    "                    print(index)\n",
    "                    parsed_annotation['mi_anno'][anno_id]['concept_id'] = index\n",
    "                except Exception as f:\n",
    "                    print(f)\n",
    "\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Exception occurred\\n{e}\")\n",
    "                print(\"Retrying...\")\n",
    "    else:\n",
    "        print('None')\n",
    "\n",
    "print('Annotation completed')\n",
    "\n",
    "    \n",
    "total_time_taken = (datetime.now() - start_time)\n",
    "print(f\"Time taken: {datetime.now() - start_time }\")\n",
    "print(f\"Total time taken: {total_time_taken}\")\n",
    "print(actual_total_tokens, completion_tokens, prompt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bfeab45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_annotation['_annotator'] = model_name_or_path\n",
    "with open(f'{file_code}-{model_name}_anno.json', 'w') as fp:\n",
    "    json.dump(parsed_annotation, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9180c363",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "items = 0\n",
    "for key, value in parsed_annotation['mi_anno'].items():\n",
    "    if value['concept_id'] is not None:\n",
    "        #print(key, value)\n",
    "        items += 1\n",
    "print(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffe4c27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
