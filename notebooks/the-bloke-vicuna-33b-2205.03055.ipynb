{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63e50065-dd79-48b6-9bc3-87eb6522a15f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unidecode\n",
      "  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m235.9/235.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tiktoken\n",
      "  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting transformers\n",
      "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting einops\n",
      "  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting auto-gptq\n",
      "  Downloading auto_gptq-0.3.2.tar.gz (63 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.5/63.5 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hDiscarding \u001b[4;34mhttps://files.pythonhosted.org/packages/1b/79/5a3a7d877a9b0a72f528e9977ec65cdb9fad800fa4f5110f87f2acaaf6fe/auto_gptq-0.3.2.tar.gz (from https://pypi.org/simple/auto-gptq/) (requires-python:>=3.8.0)\u001b[0m: \u001b[33mRequested auto-gptq from https://files.pythonhosted.org/packages/1b/79/5a3a7d877a9b0a72f528e9977ec65cdb9fad800fa4f5110f87f2acaaf6fe/auto_gptq-0.3.2.tar.gz has inconsistent version: expected '0.3.2', but metadata has '0.3.2+cu118'\u001b[0m\n",
      "  Downloading auto_gptq-0.3.1.tar.gz (63 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.5/63.5 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hDiscarding \u001b[4;34mhttps://files.pythonhosted.org/packages/3f/5c/28d57f833498e014e9d066ea0199a503b5e59d4c8e8e701a460223e143da/auto_gptq-0.3.1.tar.gz (from https://pypi.org/simple/auto-gptq/) (requires-python:>=3.8.0)\u001b[0m: \u001b[33mRequested auto-gptq from https://files.pythonhosted.org/packages/3f/5c/28d57f833498e014e9d066ea0199a503b5e59d4c8e8e701a460223e143da/auto_gptq-0.3.1.tar.gz has inconsistent version: expected '0.3.1', but metadata has '0.3.1+cu1180'\u001b[0m\n",
      "  Downloading auto_gptq-0.3.0.tar.gz (62 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting regex>=2022.1.18 (from tiktoken)\n",
      "  Downloading regex-2023.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (770 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m770.4/770.4 kB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.28.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.9.0)\n",
      "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
      "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
      "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m126.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tqdm>=4.27 (from transformers)\n",
      "  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting accelerate>=0.19.0 (from auto-gptq)\n",
      "  Downloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting datasets (from auto-gptq)\n",
      "  Downloading datasets-2.14.3-py3-none-any.whl (519 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m519.1/519.1 kB\u001b[0m \u001b[31m81.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting rouge (from auto-gptq)\n",
      "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (2.0.1+cu118)\n",
      "Collecting peft (from auto-gptq)\n",
      "  Downloading peft-0.4.0-py3-none-any.whl (72 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.19.0->auto-gptq) (5.9.5)\n",
      "Collecting fsspec (from huggingface-hub<1.0,>=0.14.1->transformers)\n",
      "  Downloading fsspec-2023.6.0-py3-none-any.whl (163 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2022.12.7)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (1.11.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (2.0.0)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.13.0->auto-gptq) (3.25.0)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.13.0->auto-gptq) (15.0.7)\n",
      "Collecting pyarrow>=8.0.0 (from datasets->auto-gptq)\n",
      "  Downloading pyarrow-12.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.9 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m38.9/38.9 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting dill<0.3.8,>=0.3.0 (from datasets->auto-gptq)\n",
      "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pandas (from datasets->auto-gptq)\n",
      "  Downloading pandas-2.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m123.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting xxhash (from datasets->auto-gptq)\n",
      "  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multiprocess (from datasets->auto-gptq)\n",
      "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiohttp (from datasets->auto-gptq)\n",
      "  Downloading aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m98.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six in /usr/lib/python3/dist-packages (from rouge->auto-gptq) (1.16.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (23.1.0)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets->auto-gptq)\n",
      "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets->auto-gptq)\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets->auto-gptq)\n",
      "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->datasets->auto-gptq)\n",
      "  Downloading frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (225 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m225.7/225.7 kB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->datasets->auto-gptq)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->auto-gptq) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->auto-gptq) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets->auto-gptq)\n",
      "  Downloading pytz-2023.3-py2.py3-none-any.whl (502 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m502.3/502.3 kB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tzdata>=2022.1 (from pandas->datasets->auto-gptq)\n",
      "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->auto-gptq) (1.2.1)\n",
      "Building wheels for collected packages: auto-gptq\n",
      "  Building wheel for auto-gptq (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for auto-gptq: filename=auto_gptq-0.3.0-cp310-cp310-linux_x86_64.whl size=5664438 sha256=4e7e987e449022a25a112145c129d74293fa0b9cbf5edac908016835d4ec60e5\n",
      "  Stored in directory: /root/.cache/pip/wheels/62/ca/da/632464db8c2071b514d3b659652383c789f53cc2fc917737bb\n",
      "Successfully built auto-gptq\n",
      "Installing collected packages: tokenizers, safetensors, pytz, xxhash, unidecode, tzdata, tqdm, rouge, regex, pyarrow, multidict, fsspec, frozenlist, einops, dill, async-timeout, yarl, tiktoken, pandas, multiprocess, huggingface-hub, aiosignal, transformers, aiohttp, datasets, accelerate, peft, auto-gptq\n",
      "Successfully installed accelerate-0.21.0 aiohttp-3.8.5 aiosignal-1.3.1 async-timeout-4.0.2 auto-gptq-0.3.0 datasets-2.14.3 dill-0.3.7 einops-0.6.1 frozenlist-1.4.0 fsspec-2023.6.0 huggingface-hub-0.16.4 multidict-6.0.4 multiprocess-0.70.15 pandas-2.0.3 peft-0.4.0 pyarrow-12.0.1 pytz-2023.3 regex-2023.6.3 rouge-1.0.1 safetensors-0.3.1 tiktoken-0.4.0 tokenizers-0.13.3 tqdm-4.65.0 transformers-4.31.0 tzdata-2023.3 unidecode-1.3.6 xxhash-3.3.0 yarl-1.9.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install unidecode tiktoken transformers einops auto-gptq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b5a187-7c86-4ea4-863a-55817112cfb5",
   "metadata": {},
   "source": [
    "## Time to install packages 4 mins 0 seconds\n",
    "## Time started: 23:59:00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4255ace1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, NavigableString\n",
    "from unidecode import unidecode\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime\n",
    "import unicodedata\n",
    "import itertools\n",
    "import tiktoken\n",
    "import ast\n",
    "import pprint\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af9777c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The safetensors archive passed at /root/.cache/huggingface/hub/models--TheBloke--Vicuna-33B-1-3-SuperHOT-8K-GPTQ/snapshots/6bd46c2caeac69d4329b48f66bbd05f8546998a8/vicuna-33b-1.3-superhot-8k-GPTQ-4bit--1g.act.order.safetensors does not contain metadata. Make sure to save your model with the `save_pretrained` method. Defaulting to 'pt' metadata.\n",
      "skip module injection for FusedLlamaMLPForQuantizedModel not support integrate without triton yet.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, pipeline, logging\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "import argparse\n",
    "\n",
    "model_name_or_path = \"TheBloke/Vicuna-33B-1-3-SuperHOT-8K-GPTQ\"\n",
    "model_basename = \"vicuna-33b-1.3-superhot-8k-GPTQ-4bit--1g.act.order\"\n",
    "model_name = 'vicuna-33b'\n",
    "file_code = '2205.03055'\n",
    "\n",
    "use_triton = False\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "\n",
    "model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n",
    "        model_basename=model_basename,\n",
    "        use_safetensors=True,\n",
    "        trust_remote_code=True,\n",
    "        device_map='auto',\n",
    "        use_triton=use_triton,\n",
    "        quantize_config=None)\n",
    "\n",
    "model.seqlen = 8192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1f3d52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_messages(prompt_template, model=\"gpt-3.5-turbo\"):\n",
    "    return len(tokenizer(prompt_template, return_tensors='pt').input_ids.cuda().tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "604d244c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_messages(text):\n",
    "    # This is a placeholder for your actual implementation\n",
    "    return text.split(\"\\n\")\n",
    "\n",
    "def split_into_chunks(text, max_tokens=2000):\n",
    "    messages = split_into_messages(text)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_tokens = 0\n",
    "    for message in messages:\n",
    "        message_tokens = num_tokens_from_messages(message, model=\"gpt-3.5-turbo\")\n",
    "        if current_tokens + message_tokens > max_tokens:\n",
    "            # If adding this message would exceed the max tokens, start a new chunk\n",
    "            chunks.append('\\n'.join(current_chunk))\n",
    "            current_chunk = [message]\n",
    "            current_tokens = message_tokens\n",
    "        else:\n",
    "            # Otherwise, add the message to the current chunk\n",
    "            current_chunk.append(message)\n",
    "            current_tokens += message_tokens\n",
    "    # Don't forget the last chunk!\n",
    "    if current_chunk:\n",
    "        chunks.append('\\n'.join(current_chunk))\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e285df1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_string(input_str):\n",
    "    if \"</s>\" in input_str:\n",
    "        return input_str\n",
    "    else:\n",
    "        last_comma_index = input_str.rfind(',')\n",
    "        if last_comma_index == -1:\n",
    "            return input_str  # No comma found, return the string as is\n",
    "        else:\n",
    "            return input_str[:last_comma_index] + '}' + input_str[last_comma_index+1:]\n",
    "        \n",
    "def get_json_of_string(incorrect, pattern=r'\\{[^\\}]*\\}'):\n",
    "    match = re.search(r'{(.*)}', incorrect, re.DOTALL)\n",
    "    if match:\n",
    "        return \"{\" + match.group(1).replace('{', '[').replace('}', ']') + \"}\"\n",
    "    else:\n",
    "        return \"{}\"\n",
    "\n",
    "\n",
    "def string_to_dict(my_string):\n",
    "    # Load the JSON string into a list of tuples\n",
    "    tuples_list = json.JSONDecoder(object_pairs_hook=list).decode(my_string)\n",
    "\n",
    "    # Create a new dictionary to hold the final result\n",
    "    final_dict = {}\n",
    "\n",
    "    # Iterate over the list of tuples\n",
    "    for key, value in tuples_list:\n",
    "        # If the key is already in the final dictionary, append the value\n",
    "        # to the list of values for that key\n",
    "        if key in final_dict:\n",
    "            # Ensure the value is in a list form\n",
    "            if not isinstance(final_dict[key], list):\n",
    "                final_dict[key] = [final_dict[key]]\n",
    "            final_dict[key].append(value)\n",
    "        else:\n",
    "            # If the key is not in the final dictionary, add it with the value\n",
    "            final_dict[key] = value\n",
    "\n",
    "    return final_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53c1858e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dictionaries(dict1, dict2):\n",
    "    union_dict = dict1.copy()\n",
    "\n",
    "    for key, value in dict2.items():\n",
    "        if key in union_dict:\n",
    "            if isinstance(union_dict[key], list):\n",
    "                if value not in union_dict[key]:\n",
    "                    union_dict[key].append(value)\n",
    "            else:\n",
    "                if union_dict[key] != value:\n",
    "                    union_dict[key] = [union_dict[key], value]\n",
    "        else:\n",
    "            union_dict[key] = value\n",
    "\n",
    "    return union_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff36d705",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_from_tags(element):\n",
    "    if isinstance(element, NavigableString):\n",
    "        return element\n",
    "    if element.name == 'mi':\n",
    "        return str(element)\n",
    "    return ''.join(get_text_from_tags(child) for child in element.children)\n",
    "\n",
    "def parse_html(file_path, clean=True):\n",
    "    with open(file_path, 'r', encoding='utf-8') as html_file:\n",
    "        soup = BeautifulSoup(html_file, 'html.parser')\n",
    "\n",
    "    texts = get_text_from_tags(soup)\n",
    "    if clean:\n",
    "        matches = re.findall(r'<mi(.*?)</mi>', texts)\n",
    "        for match in matches:\n",
    "            original_string = f'<mi{match}</mi>'\n",
    "            replaced_string = re.sub(r'<.*?>(.*?)</.*?>', r'<|\\1|>', original_string)\n",
    "            texts = texts.replace(original_string, replaced_string)\n",
    "\n",
    "    return texts\n",
    "\n",
    "# call the function with your HTML file path\n",
    "page = parse_html(f'{file_code}.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb9f95a7-690d-49a1-947e-2d1c107a5f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(prompt):\n",
    "    return f'''SYSTEM: {prompt[0]['content']}\n",
    "USER: {prompt[1]['content']}\n",
    "ASSISTANT: {prompt[2]['content']}\n",
    "USER: {prompt[3]['content']}\n",
    "ASSISTANT: '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d0568d3-d765-4c53-ac27-c67650dad1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_pattern(input_string):\n",
    "    pattern = r\"<\\|[^<\\|>]*\\|>\"\n",
    "    if re.search(pattern, input_string):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f0f85cb-e25a-4a21-81a4-c8c143a47c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_output(output):\n",
    "    pos = output.index('Do not include the angle brackets in the dictionary')\n",
    "    end_of_string = output[pos:]\n",
    "    print(end_of_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abf7ee5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1251 prompt tokens counted.\n"
     ]
    }
   ],
   "source": [
    "text = page\n",
    "chunks = split_into_chunks(text, max_tokens=512)\n",
    "i = 0\n",
    "\n",
    "while not contains_pattern(chunks[i]):\n",
    "    i += 1\n",
    "print(i)\n",
    "\n",
    "question = chunks[i]\n",
    "\n",
    "actual_total_tokens = 0\n",
    "completion_tokens = 0\n",
    "prompt_tokens = 0\n",
    "\n",
    "prompt = [\n",
    "        {'role': 'system',\n",
    "         'content': 'You are a helpful research assistant tasked with converting long paragraphs into a JSON '\n",
    "                    'dictionary. The goal is to identify and classify each individual mathematical symbol, variable,'\n",
    "                    ' and identifier in the text marked between \"<||>\". The dictionary should store the identifiers as '\n",
    "                    'keys and their corresponding definitions as values in an array format. '},\n",
    "        {'role': 'system', 'name': 'example_user', 'content': '''A relational model is a triple <|M|>â€²=(<|X|>,<|R|>,\n",
    "        <|v|>), where <|X|> is a set of states, <|R|><|âŠ†|><|X|><|Ã—|><|X|> is a binary relation on <|X|>, \n",
    "        and <|v|>:<|ğ–¯ğ—‹ğ—ˆğ—‰|>â†’2<|X|> is a valuation. Given a relational model <|M|>â€², the satisfaction relation between \n",
    "        points <|x<|âˆˆ<|X<| and formulas <|Ï†<|âˆˆ<|â„’<|<|ğ–ªğ– <| is defined inductively by <|M|>â€²,<|x|>âŠ¨<|ğ–ª|><|Ï†|>â‡” for all \n",
    "        <|y|>âˆˆ<|X|>,<|x|><|R|><|y|> implies <|M|>â€²,<|y|>âŠ¨<|Ï†|><|M|>â€²,<|x|>âŠ¨<|ğ– |><|Ï†|><||>â‡” for all <|y|>âˆˆ<|X|>,\n",
    "        <|M|>â€²,<|y|>âŠ¨<|Ï†|>'''},\n",
    "        {'role': 'system', 'name': 'example_assistant', 'content': '''identifiers = {\n",
    "            \"M\": [\"Model\", \"Expertise Model\"],\n",
    "            \"M'\": \"Relational model\",\n",
    "            \"X\": \"Set of states\",\n",
    "            \"R\": \"Binary relation on X\",\n",
    "            \"v\": \"Valuation\",\n",
    "            \"ğ–¯ğ—‹ğ—ˆğ—‰\": \"Set of propositions\",\n",
    "            \"M'\": \"Relational model\",\n",
    "            \"x\": \"Point in X\",\n",
    "            \"Ï†\": \"Formula in ğ–ªğ– \",\n",
    "            \"â„’_{ğ–ªğ– }\": \"Set of formulas\",\n",
    "            \"ğ–ª\": \"Modal operator K\",\n",
    "            \"ğ– \": \"Modal operator A\",\n",
    "            \"y\": \"Point in X\",\n",
    "            \"âŠ¨\": \"Satisfaction relation\",\n",
    "            \"â‡”\": \"If and only if operator\",\n",
    "            \"âˆˆ\": \"Element of a set\",\n",
    "            \"âŠ†\": \"Subset of a set\",\n",
    "            \"Ã—\": \"Cartesian product operator\",\n",
    "            \"â†’\": \"Function or implication operator\",\n",
    "            \"for all\": \"Universal quantifier\"\n",
    "            }'''},\n",
    "        {'role': 'user', 'content': f'Generate a JSON dictionary for the following text\\n```txt\\n{question}```. '\n",
    "                                    'Only consider the mathematical identifiers inside \"<||>\" for the dictionary. '\n",
    "                                    'Do not consider any other identifier other than those marked. Consider all the '\n",
    "                                    'identifiers individually. Do not skip any identifier, mention all the identifiers '\n",
    "                                    'inside \"<||>\" in your dictionary. Do not include the angle brackets in the '\n",
    "                                    'dictionary.'}\n",
    "    ]\n",
    "\n",
    "\n",
    "open_prompt = get_prompt(prompt)\n",
    "\n",
    "prompt_size = num_tokens_from_messages(open_prompt)\n",
    "print(f\"{prompt_size} prompt tokens counted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "612640fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TheBloke/Vicuna-33B-1-3-SuperHOT-8K-GPTQ\n",
      "Do not include the angle brackets in the dictionary.\n",
      "ASSISTANT: \n",
      "identifiers = {\n",
      "\"BB\": \"Binbin Yang\",\n",
      "\"XC\": \"Xinchi Deng\",\n",
      "\"HS\": \"Han Shi\",\n",
      "\"CL\": \"Changlin Li\",\n",
      "\"GWZ\": \"Gengwei Zhang\",\n",
      "\"HX\": \"Hang Xu\",\n",
      "\"SZ\": \"Shen Zhao\",\n",
      "\"LL\": \"Liang Lin\",\n",
      "\"XY\": \"Xiaodan Liang\",\n",
      "\"YSU\": \"Sun Yat-sen University\",\n",
      "\"HKUST\": \"The Hong Kong University of Science and Technology\",\n",
      "\"RELER\": \"AAII, UTS\",\n",
      "\"HHL\": \"Huawei Noah's Ark Lab\",\n",
      "\"YAT\": \"Yat-sen University\",\n",
      "\"SYSU\": \"Sun Yat-sen University\",\n",
      "\"ZGW\": \"Zhang Gengwei\",\n",
      "\"LIN\": \"Liang Lin\",\n",
      "\"ZHA\": \"Shen Zhao\",\n",
      "\"LI\": \"Liang Lin\",\n",
      "\"XL\": \"Xiaodan Liang\",\n",
      "\"U\": \"University\",\n",
      "\"A\": \"and\",\n",
      "\"I\": \"in\",\n",
      "\"II\": \"AAII, UTS\",\n",
      "\"RL\": \"ReLER\",\n",
      "\"CLR\": \"Changlin Li\",\n",
      "\"GWZR\": \"Gengwei Zhang\",\n",
      "\"XYL\": \"Xiaodan Liang\",\n",
      "\"YSUY\": \"Sun Yat-sen University\",\n",
      "\"YCL\": \"Yat-sen University\",\n",
      "\"YSUCL\": \"Sun Yat-sen University\",\n",
      "\"YSUYCL\": \"Sun Yat-sen University\",\n",
      "\"YCLY\": \"Yat-sen University\",\n",
      "\"YSUYCLY\": \"Sun Yat-sen University\",\n",
      "\"YCLYSU\": \"Yat-sen University\",\n",
      "\"YSUYCLYS\": \"Sun Yat-sen University\",\n",
      "\"YCLYSUY\": \"Yat-sen University\",\n",
      "\"YSUYCLYSY\": \"Sun Yat-sen University\",\n",
      "\"YCLYSUYSU\": \"Yat-sen University\",\n",
      "\"YSUYCLYSYSU\": \"Sun Yat-sen University\",\n",
      "Time taken: 0:00:32.100152\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using {model_name_or_path}\")\n",
    "start_time = datetime.now()\n",
    "\n",
    "input_ids = tokenizer(open_prompt, return_tensors='pt').input_ids.cuda()\n",
    "output = model.generate(inputs=input_ids, temperature=0.5, max_new_tokens=512, repetition_penalty=1.05)\n",
    "output_string = tokenizer.decode(output[0])\n",
    "print_output(output_string)\n",
    "\n",
    "total_time_taken = datetime.now() - start_time\n",
    "print(f\"Time taken: {total_time_taken}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "507e7993",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1764 513 1251\n"
     ]
    }
   ],
   "source": [
    "actual_total_tokens += num_tokens_from_messages(output_string)\n",
    "completion_tokens += num_tokens_from_messages(output_string) - num_tokens_from_messages(open_prompt)\n",
    "prompt_tokens += num_tokens_from_messages(open_prompt)\n",
    "print(actual_total_tokens, completion_tokens, prompt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ff56fd8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Safely convert the dictionary string to a dictionary using json.loads()\n",
    "try:\n",
    "    ind = output_string.index('Do not include the angle brackets in the dictionary.')\n",
    "    correct_output_string = modify_string(output_string[ind:])\n",
    "    dic_output_string = get_json_of_string(correct_output_string)\n",
    "    dictionary = [string_to_dict(dic_output_string)]\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    dictionary = [{}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "acbf76ad-2acb-41dc-be3d-aab9c19c15c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'BB': 'Binbin Yang', 'XC': 'Xinchi Deng', 'HS': 'Han Shi', 'CL': 'Changlin Li', 'GWZ': 'Gengwei Zhang', 'HX': 'Hang Xu', 'SZ': 'Shen Zhao', 'LL': 'Liang Lin', 'XY': 'Xiaodan Liang', 'YSU': 'Sun Yat-sen University', 'HKUST': 'The Hong Kong University of Science and Technology', 'RELER': 'AAII, UTS', 'HHL': \"Huawei Noah's Ark Lab\", 'YAT': 'Yat-sen University', 'SYSU': 'Sun Yat-sen University', 'ZGW': 'Zhang Gengwei', 'LIN': 'Liang Lin', 'ZHA': 'Shen Zhao', 'LI': 'Liang Lin', 'XL': 'Xiaodan Liang', 'U': 'University', 'A': 'and', 'I': 'in', 'II': 'AAII, UTS', 'RL': 'ReLER', 'CLR': 'Changlin Li', 'GWZR': 'Gengwei Zhang', 'XYL': 'Xiaodan Liang', 'YSUY': 'Sun Yat-sen University', 'YCL': 'Yat-sen University', 'YSUCL': 'Sun Yat-sen University', 'YSUYCL': 'Sun Yat-sen University', 'YCLY': 'Yat-sen University', 'YSUYCLY': 'Sun Yat-sen University', 'YCLYSU': 'Yat-sen University', 'YSUYCLYS': 'Sun Yat-sen University', 'YCLYSUY': 'Yat-sen University', 'YSUYCLYSY': 'Sun Yat-sen University', 'YCLYSUYSU': 'Yat-sen University', 'YSUYCLYSYSU': 'Sun Yat-sen University'}]\n"
     ]
    }
   ],
   "source": [
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d18ab90-38c2-4724-9a03-239fc73078d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_loop(prompt):\n",
    "    return f'''SYSTEM: {prompt[0]['content']}\n",
    "USER: {prompt[1]['content']}\n",
    "ASSISTANT: {prompt[2]['content']}\n",
    "SYSTEM: {prompt[3]['content']}\n",
    "USER: {prompt[4]['content']}\n",
    "ASSISTANT: '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5bab7543",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 of 38\n",
      "Iteration 1 of 38\n",
      "Iteration 2 of 38\n",
      "Iteration 3 of 38\n",
      "Iteration 4 of 38\n",
      "Iteration 5 of 38\n",
      "Iteration 6 of 38\n",
      "Iteration 7 of 38\n",
      "Iteration 8 of 38\n",
      "\n",
      "\n",
      "\n",
      "1647 prompt tokens counted.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2081 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do not include the angle brackets in your dictionary.\n",
      "ASSISTANT: \n",
      "```json\n",
      "{\n",
      "\"Gating\": \"Gating mechanism\",\n",
      "\"Continual\": \"continual image classification\",\n",
      "\"Dynamic\": \"dynamic inference\",\n",
      "\"Inference\": \"static binary gates\",\n",
      "\"Tasks\": \"tasks\",\n",
      "\"Gradient\": \"optimization\",\n",
      "\"Soft\": \"dynamic soft gates\",\n",
      "\"Training\": \"training time\",\n",
      "\"Discretize\": \"binary gates\",\n",
      "\"Methodology\": \"Methodology\",\n",
      "\"Continual\": \"Continual Object Detection\",\n",
      "\"Challenges\": \"Challenges\",\n",
      "\"Goal\": \"object detector\",\n",
      "\"Sequential\": \"sequential tasks\",\n",
      "\"Dataset\": \"dataset\",\n",
      "\"T\": \"task\",\n",
      "\"D\": \"dataset\",\n",
      "\"X\": \"input images\",\n",
      "\"Y\": \"annotations\",\n",
      "\"Catastrophic\": \"Catastrophic Forgetting\",\n",
      "\"Models\": \"models\",\n",
      "\"Data\": \"datasets\",\n",
      "\"History\": \"historical data\",\n",
      "\"Problem\": \"Problem\",\n",
      "\"Categories\": \"categories\",\n",
      "\"Detector\": \"object detector\",\n",
      "\"Task\": \"task\",\n",
      "\"<|ğ’¯|>\": \"tasks\",\n",
      "\"<|t|>\": \"task\",\n",
      "\"<|ğ’°|>\": \"datasets\",\n",
      "\"<|Î”|>\": \"datasets\",\n",
      "\"<|Î”|>\": \"tasks\",\n",
      "\"<|Î”|>\": \"datasets\",\n",
      "\"<|C|><|t|>\": \"categories\",\n",
      "\"<|X|><|t|>\": \"input images\",\n",
      "\"<|Y|><|t|>\": \"annotations\",\n",
      "\"<|C|><|t|>\": \"categories\",\n",
      "\"<|C|><|t|>\": \"set of categories\",\n",
      "\"<|C|><|t|>\": \"object of interests\"\n",
      "}\n",
      "```</s>\n",
      "947 2898\n",
      "Actual total tokens till now: 3845\n",
      "Iteration 9 of 38\n",
      "\n",
      "New dictionary\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1208 prompt tokens counted.\n",
      "\n",
      "Do not include the angle brackets in your dictionary.\n",
      "ASSISTANT: \n",
      "identifiers = {\n",
      "\"object_detection\": \"Object detection\",\n",
      "\"ROSETTA\": \"Prototype-based task correlation guided gating mechanism\",\n",
      "\"memory_bank\": \"Memory bank for storing task-specific gates and prototypes\",\n",
      "\"gating_strategy\": \"Strategy for querying previously learned knowledge\",\n",
      "\"class\\_embeddings\": \"Embeddings of class labels\",\n",
      "\"fully_connected\": \"Type of layer for task embeddings\",\n",
      "\"max\\_pooling\": \"Type of layer for task embeddings\",\n",
      "\"task\\_embedding\": \"Embedding of task-specific information\",\n",
      "\"MLP\": \"Multilayer perceptron\",\n",
      "\"dynamic\\_soft\\_gate\": \"Gating mechanism for selecting relevant features\",\n",
      "\"g\\_lc\": \"Soft gate for layer c\",\n",
      "\"g\\_l\\_t\": \"Soft gate for task t\",\n",
      "\"convolutional\\_layers\": \"Layers in the convolutional module\",\n",
      "\"convolutional\\_layer\": \"Individual layer in the convolutional module\",\n",
      "\"input\": \"Input to the convolutional layer\",\n",
      "\"output\": \"Output from the convolutional layer\",\n",
      "\"activate\": \"Function of the MLP\",\n",
      "\"sub-model\": \"Partial model for a specific task\",\n",
      "\"activating\": \"Process of enabling a sub-model\",\n",
      "\"<||>\": \"Identifier inside the paragraph\"\n",
      "}</s>\n",
      "1267 4106\n",
      "Actual total tokens till now: 5373\n",
      "INCORRECT DICTIONARY Invalid \\escape: line 6 column 7 (char 269)\n",
      "Iteration 10 of 38\n",
      "\n",
      "\n",
      "\n",
      "1013 prompt tokens counted.\n",
      "\n",
      "Do not include the angle brackets in your dictionary.\n",
      "ASSISTANT: ğ’‡ = {\n",
      "l = layer, t = task, F = convolutional operation, G = gating module, Î» = channels, h = height, w = width, c = colors, âˆˆ = element of a set, Ã— = Cartesian product operator, â‰¥ = inequality operator, = = assignment operator, â‹… = dot product operator, âˆ‘ = summation operator, âˆ« = integration operator, âˆ‚ = partial derivative operator, d/dx = partial derivative with respect to x, Î” = Laplacian operator, âˆ‡ = gradient operator, âˆ‡Â² = divergence operator, âˆ‡Â· = dot product, âˆ‡âŠ¥ = cross product, âˆ£âˆ£ = norm, | | = absolute value, âˆš = square root, âˆ = infinity, Î“ = gamma function, ! = factorial, Î£ = sum, P = probability, E [ Â· ] = expected value, Var [ Â· ] = variance, Std[ Â· ] = standard deviation, log = logarithm, ln = natural logarithm, e = base of natural logarithm, Ï€ = pi, sin = sine, cos = cosine, tan = tangent, sec = secant, csc = cosecant, cot = cotangent, âˆ = infinity, Î“ = gamma function, ! = factorial, Î£ = sum, P = probability, E [ Â· ] = expected value, Var [ Â· ] = variance, Std[ Â· ] = standard deviation, ln = natural logarithm, e = base of natural logarithm, Ï€ = pi, sin = sine, cos = cosine, tan = tangent, sec = secant, csc = cosecant, cot = cotangent, âˆ = infinity, Î“ = gamma function, ! = factorial, Î£ = sum, P = probability, E [ Â· ] = expected value, Var [ Â· ] = variance, Std[ Â· ] = standard deviation, ln = natural logarithm, e = base of natural logarithm, Ï€ = pi, sin = sine, cos = cosine, tan = tangent, sec = secant, csc = cosecant, cot = cotangent, âˆ = infinity, Î“ = gamma function, ! = factorial, Î£ = sum, P = probability,\n",
      "1780 5119\n",
      "Actual total tokens till now: 6899\n",
      "INCORRECT DICTIONARY Expecting property name enclosed in double quotes: line 2 column 1 (char 2)\n",
      "Iteration 11 of 38\n",
      "\n",
      "\n",
      "\n",
      "1260 prompt tokens counted.\n",
      "\n",
      "Do not include the angle brackets in your dictionary.\n",
      "ASSISTANT: ğŸ“Œ Here is a JSON dictionary for the mathematical identifiers inside \"<||>\":\n",
      "```json\n",
      "{\n",
      "    \"âŠ™\": \"channel-wise multiplication\",\n",
      "    \"<|ğ’‡|><|l|>+1<|t|>\": \"element-wise plus one\",\n",
      "    \"<|ğ’‡|><|l|><|t|>\": \"input feature maps of gated convolution\",\n",
      "    \"<|F|><|l|><|t|>\": \"output of convolution\",\n",
      "    \"<|G|><|l|><|t|>\": \"channel gates of the lth layer\",\n",
      "    \"<|g|><|l|><|c|>\": \"channel gate for the lth layer\",\n",
      "    \"<|g|><|l|><|t|>\": \"task embedding ğ’†\",\n",
      "    \"<|ğ’†|><|ğ’•|>\": \"learned task embedding ğ’†\",\n",
      "    \"<|C|><|t|>\": \"class embeddings\",\n",
      "    \"<|C|><|t|>\": \"class concatenation\",\n",
      "    \"<|c|>+1\": \"next channel\",\n",
      "    \"<|w|>+1\": \"next word\",\n",
      "    \"<|l|>+1\": \"next layer\",\n",
      "    \"<|t|>\": \"time\"\n",
      "}\n",
      "```\n",
      "Each identifier is represented as a key-value pair, where the key is the identifier and the value is its definition.</s>\n",
      "2117 6379\n",
      "Actual total tokens till now: 8496\n",
      "Iteration 12 of 38\n",
      "\n",
      "\n",
      "\n",
      "1529 prompt tokens counted.\n",
      "\n",
      "Do not include the angle brackets in your dictionary.\n",
      "ASSISTANT: ğŸ“Œ You are providing an incomplete example. Please provide the full example or formula you would like me to analyze.</s>\n",
      "2145 7908\n",
      "Actual total tokens till now: 10053\n",
      "Iteration 13 of 38\n",
      "\n",
      "\n",
      "\n",
      "1453 prompt tokens counted.\n",
      "\n",
      "Do not include the angle brackets in your dictionary.\n",
      "ASSISTANT: ğ•€ = {\n",
      "\"ğ•€\": \"indicator function\",\n",
      "\"ğ•‡\": \"gated convolution\",\n",
      "\"ğ’ˆ\": \"binary gates\",\n",
      "\"ğ’ˆ^\": \"binary gate values\",\n",
      "\"ğ’ˆ^l_t\": \"binary gate values for layer l at time t\",\n",
      "\"ğ’ˆ^l_c\": \"binary gate values for channel gates of the lth layer at time t\",\n",
      "\"ğ’ˆ^l_t+1\": \"next channel gate values at layer l at time t\",\n",
      "\"ğ’ˆ^l_t+1+1\": \"next channel gate values at layer l at time t\",\n",
      "\"ğ’ˆ^l_t+1+2\": \"next channel gate values at layer l at time t\",\n",
      "\"ğ’ˆ^l_t+1+â€¦\": \"next channel gate values at layer l at time t\",\n",
      "\"ğ’ˆ^l_c+1\": \"next channel gate values at channel gates of the lth layer at time t\",\n",
      "\"ğ’ˆ^l_c+1+1\": \"next channel gate values at channel gates of the lth layer at time t\",\n",
      "\"ğ’ˆ^l_c+1+2\": \"next channel gate values at channel gates of the lth layer at time t\",\n",
      "\"ğ’ˆ^l_c+1+â€¦\": \"next channel gate values at channel gates of the lth layer at time t\",\n",
      "\"ğ’ˆ^l_c+1+â€¦+1\": \"next channel gate values at channel gates of the lth layer at time t\",\n",
      "\"ğ’ˆ^l_c+1+â€¦+2\": \"next channel gate values at channel gates of the lth layer at time t\",\n",
      "\"ğ’ˆ^l_c+1+â€¦+â€¦\": \"next channel gate values at channel gates of the lth layer at time t\",\n",
      "\"ğ’ˆ^l_c+1+â€¦+â€¦+1\": \"next channel gate values at channel gates of the lth layer at time t\",\n",
      "\"ğ’ˆ^l_c+1+â€¦+â€¦+2\": \"next channel gate values at channel gates of the lth layer at time t\",\n",
      "\"ï¿½\n",
      "2658 9361\n",
      "Actual total tokens till now: 12019\n",
      "Iteration 14 of 38\n",
      "\n",
      "\n",
      "\n",
      "2028 prompt tokens counted.\n",
      "\n",
      "Do not include the angle brackets in your dictionary.\n",
      "ASSISTANT: ğ’œ = {\"â„’\": \"Loss function\", \"ğ‘ ğ‘ğ‘ğ‘Ÿğ‘ ğ‘–ğ‘¡\": \"Sparse gradient\", \"ğ”¼\": \"Indicator function\", \"x\": \"Input\", \"y\": \"Output\", \"D\": \"Discrete\", \"t\": \"Time\", \"L\": \"Number of layers\", \"â„’\": \"Gated convolution\", \"ğ’‡\": \"Feature maps\", \"MSE\": \"Mean square error\", \"F\": \"Convolution\", \"l\": \"Layer\", \"âˆˆ\": \"Element of a set\", \"âˆ‘\": \"Summation\", \"â‰¥\": \"Greater than or equal to\", \"â‰¤\": \"Less than or equal to\", \"â‰ˆ\": \"Approximately\", \"âŠ¨\": \"Satisfies\", \"âŠ¨\": \"Implied by\", \"â‡’\": \"Implies\", \"=\": \"Equal to\", \"<\": \"Less than\", \">\": \"Greater than\", \"âˆ£\": \"Mod\", \"|\": \"Absolute value\", \"Â°\": \"Gradient\", \"âˆ¶\": \"Sparse\", \"Î©\": \"Integral\", \"âˆ´\": \"Causal\", \"âˆ¶\": \"Sparse\", \"Î©\": \"Integral\", \"âˆ´\": \"Causal\"}</s>\n",
      "2972 11389\n",
      "Actual total tokens till now: 14361\n",
      "Iteration 15 of 38\n",
      "\n",
      "New dictionary\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1218 prompt tokens counted.\n",
      "\n",
      "Do not include the angle brackets in your dictionary.\n",
      "ASSISTANT: \n",
      "identifiers = {\n",
      "\"task\": \"Task Correlation Guided Gating Diversity\",\n",
      "\"gated\": \"Gating Mechanism\",\n",
      "\"module\": \"Sub-models' weights\",\n",
      "\"channels\": \"Frozen and learnable channel weights\",\n",
      "\"tasks\": \"Previous and new tasks\",\n",
      "\"knowledge\": \"Stored in sub-models' weights\",\n",
      "\"retrieved\": \"By binary gates\",\n",
      "\"sub-model\": \"New task's sub-model\",\n",
      "\"frozen\": \"Unchangeable channel weights\",\n",
      "\"inactivated\": \"Learnable channel weights\",\n",
      "\"learn\": \"New task learning\",\n",
      "\"share\": \"Knowledge sharing among tasks\",\n",
      "\"foreground\": \"Objects in images\",\n",
      "\"background\": \"Backgrounds in images\",\n",
      "\"domains\": \"Differences between tasks\",\n",
      "\"catastrophic\": \"Forgetting in continual object detection\",\n",
      "\"gating\": \"Mechanism proposed in 3.2\",\n",
      "\"degradation\": \"Performance decrease\",\n",
      "\"observable\": \"Aware knowledge sharing\",\n",
      "\"diversity\": \"Channel gates activation\",\n",
      "\"entropy\": \"Diversity loss <|â„’|><|ğ‘‘ğ‘–ğ‘£ğ‘’ğ‘Ÿğ‘ ğ‘–ğ‘¡ğ‘¦|>\",\n",
      "\"exclusive\": \"Unique knowledge compared to existing\"\n",
      "}</s>\n",
      "3299 12607\n",
      "Actual total tokens till now: 15906\n",
      "Iteration 16 of 38\n",
      "\n",
      "\n",
      "\n",
      "1543 prompt tokens counted.\n",
      "\n",
      "Do not include the angle brackets in your dictionary.\n",
      "ASSISTANT: ğ‘‘ğ‘–ğ‘£ğ‘’ğ‘Ÿğ‘ ğ‘–ğ‘¡ğ‘¦ = {\n",
      "logl = logt,\n",
      "t = q >= l,\n",
      "âˆ‘i=1mltgtg,c,\n",
      "g = gating diversity loss,\n",
      "Î· = hyperparameter threshold,\n",
      "q = ratio of newly activated gates,\n",
      "l = layer,\n",
      "t = task,\n",
      "m = total number of previously inactivated channels,\n",
      "g = function,\n",
      "c = channel,\n",
      "t = time,\n",
      "log = logarithmic,\n",
      "}</s>\n",
      "3438 14150\n",
      "Actual total tokens till now: 17588\n",
      "INCORRECT DICTIONARY Expecting property name enclosed in double quotes: line 2 column 1 (char 2)\n",
      "Iteration 17 of 38\n",
      "\n",
      "\n",
      "\n",
      "1482 prompt tokens counted.\n",
      "\n",
      "Do not include the angle brackets in your dictionary.\n",
      "ASSISTANT: \n",
      "identifiers = {\n",
      "\"correlation\": \"Class-to-class correlation\",\n",
      "\"prototypical\": \"Prototype-based\",\n",
      "\"diversity\": \"Gating diversity\",\n",
      "\"controller\": \"Task Correlation Guided Gating Diversity Controller\",\n",
      "\"task\": \"Class-specific prototypes\",\n",
      "\"activated\": \"Exclusive knowledge\",\n",
      "\"similar\": \"New task's sub-model\",\n",
      "\"features\": \"Region-of-Interest\",\n",
      "\"class-to-class\": \"Matrix <|ğŒ|><|m|>â†’<|n|>\",\n",
      "\"class\": \"Category\",\n",
      "\"object\": \"Inactivated gates\",\n",
      "\"learning\": \"New task learning\",\n",
      "\"foreground\": \"Objects in images\",\n",
      "\"background\": \"Backgrounds in images\",\n",
      "\"distance\": \"Matrix <|MSE|><|m|>â†’<|n|>\",\n",
      "\"mse\": \"Mean Squared Error\",\n",
      "\"between\": \"Based on class-specific prototypes\",\n",
      "\"expect\": \"Automatically determine\",\n",
      "\"activation\": \"Adaptively adjust\",\n",
      "\"entropy\": \"Diversity loss <|â„’|><|ğ‘‘ğ‘–ğ‘£ğ‘’ğ‘Ÿğ‘ ğ‘–ğ‘¡ğ‘¦|>\",\n",
      "\"exclusive\": \"Unique knowledge compared to existing\",\n",
      "\"different\": \"Tasks\",\n",
      "\"consecutive\": \"Taking the correlation\",\n",
      "\"assumption\": \"Few exclusive knowledge\",\n",
      "\"needed\": \"When two tasks\",\n",
      "\"similar or almost identical\",\n",
      "\"ROSETTA\": \"ROSETTA is expected to\",\n",
      "\"detection\": \"Continual object detection\",\n",
      "\"forgotten\": \"Forgetting in\",\n",
      "\"performance\": \"Decrease\",\n",
      "\"aware\": \"Knowledge sharing among tasks\",\n",
      "\"shares\": \"Shares\",\n",
      "\"matrix\": \"Matrix <|ğŒ|><|m|>Ã—<|n|>\",\n",
      "\"class-to-class\": \"Depict the correlation\",\n",
      "\"in\": \"Class-to-class correlation\",\n",
      "\"out\": \"Depict the correlation\",\n",
      "\"goes\": \"One step further\",\n",
      "\"generates\n",
      "3951 15632\n",
      "Actual total tokens till now: 19583\n",
      "INCORRECT DICTIONARY Expecting ':' delimiter: line 27 column 30 (char 984)\n",
      "Iteration 18 of 38\n",
      "\n",
      "\n",
      "\n",
      "1570 prompt tokens counted.\n",
      "\n",
      "Do not include the angle brackets in your dictionary.\n",
      "ASSISTANT: ğŸ“Œ Here is a JSON dictionary for the given text, focusing only on the mathematical identifiers inside the less-than and greater-than signs:\n",
      "\n",
      "mathIdentifiers = {\n",
      "\"C\": [\"Class to be recognized\", \"Classes for task\"],\n",
      "\"t\": [\"Task\", \"Tasks\"],\n",
      "\"n\": [\"New task\", \"Tasks\"],\n",
      "\"m\": [\"Old task\", \"Tasks\"],\n",
      "\"â„Œ\": [\"Class-to-task correlation\", \"Correlation\"],\n",
      "\"â„›\": [\"Task-to-task correlation\", \"Correlation\"],\n",
      "\"â„‘\": [\"Class\", \"Category\"],\n",
      "\"â„’\": [\"Class-to-task correlation\", \"Correlation\"],\n",
      "\"â„œ\": [\"Task-to-task correlation\", \"Correlation\"],\n",
      "\"â„Œ\": [\"Class-to-task correlation\", \"Correlation\"],\n",
      "\"â„›\": [\"Task-to-task correlation\", \"Correlation\"],\n",
      "\"â„’\": [\"Class-to-task correlation\", \"Correlation\"],\n",
      "\"â„œ\": [\"Task-to-task correlation\", \"Correlation\"],\n",
      "\"â„š\": [\"Set\", \"Collection\"],\n",
      "\"â„Œ\": [\"Class-to-task correlation\", \"Correlation\"],\n",
      "\"â„›\": [\"Task-to-task correlation\", \"Correlation\"],\n",
      "\"â„’\": [\"Class-to-task correlation\", \"Correlation\"],\n",
      "\"â„œ\": [\"Task-to-task correlation\", \"Correlation\"],\n",
      "\"â„\": [\"Aggregation\", \"Sum\"],\n",
      "\"â„Œ\": [\"Class-to-task correlation\", \"Correlation\"],\n",
      "\"â„›\": [\"Task-to-task correlation\", \"Correlation\"],\n",
      "\"â„’\": [\"Class-to-task correlation\", \"Correlation\"],\n",
      "\"â„œ\": [\"Task-to-task correlation\", \"Correlation\"],\n",
      "\"â„\": [\"Average\", \"Mean\"]\n",
      "}</s>\n",
      "4381 17202\n",
      "Actual total tokens till now: 21583\n",
      "Iteration 19 of 38\n",
      "\n",
      "\n",
      "\n",
      "1750 prompt tokens counted.\n",
      "\n",
      "Do not include the angle brackets in your dictionary.\n",
      "ASSISTANT: math_identifiers = {\n",
      "\"â„›\": \"Prototypical task correlation\",\n",
      "\"â„’\": \"Class-to-task correlation\",\n",
      "\"â„‘\": \"Class/category\",\n",
      "\"â„Œ\": \"Class-to-task correlation\",\n",
      "\"â„\": \"Aggregation/sum\",\n",
      "\"â„\": \"Set/collection\",\n",
      "\"â„\": \"Average/mean\",\n",
      "\"â„œ\": \"Task-to-task correlation\",\n",
      "\"â„•\": \"Diversity loss\",\n",
      "\"â„”\": \"Gating mechanism\",\n",
      "\"â„™\": \"Gating mechanism proposed in 3.2\",\n",
      "\"â„š\": \"Gating mechanism degradation\",\n",
      "\"â„›\": \"Gating diversity controller\",\n",
      "\"â„›\": \"Weight for diversity loss\"\n",
      "}</s>\n",
      "4569 18952\n",
      "Actual total tokens till now: 23521\n",
      "Iteration 20 of 38\n",
      "\n",
      "New dictionary\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1069 prompt tokens counted.\n",
      "\n",
      "Do not include the angle brackets in your dictionary.\n",
      "ASSISTANT: Mathematical identifiers from the text:\n",
      "```css\n",
      "<â„’></s>\n",
      "4588 20021\n",
      "Actual total tokens till now: 24609\n",
      "Iteration 21 of 38\n",
      "\n",
      "\n",
      "\n",
      "1305 prompt tokens counted.\n",
      "\n",
      "Do not include the angle brackets in your dictionary.\n",
      "ASSISTANT: ğŸ“Œ Here is a JSON dictionary for the mathematical identifiers inside \"<||>\":\n",
      "```json\n",
      "{\n",
      "    \"D\": [\"Datasets\", \"Data\", \"Incremental object detection\"],\n",
      "    \"i\": [\"Index\", \"Dataset\"],\n",
      "    \"j\": [\"Index\", \"Dataset\"],\n",
      "    \"C\": [\"Categories\", \"Classes\"],\n",
      "    \"t\": [\"Class\", \"Category\"],\n",
      "    \"t+\": [\"Class\", \"Subclass\"],\n",
      "    \"COCO\": [\"COCO dataset\"],\n",
      "    \"VOC\": [\"Pascal VOC dataset\"],\n",
      "    \"KITTI\": [\"KITTI dataset\"],\n",
      "    \"Kitchen\": [\"Kitchen dataset\"],\n",
      "    \"23\": [\"Reference for class-incremental object detection\"],\n",
      "    \"34\": [\"Reference for task-incremental object detection\"],\n",
      "    \"11\": [\"Pascal VOC dataset\"],\n",
      "    \"14\": [\"KITTI dataset\"],\n",
      "    \"15\": [\"Kitchen dataset\"],\n",
      "    \"30\": [\"COCO dataset\"],\n",
      "    \"1+1\": [\"Incremental training task\"],\n",
      "    \"1+10\": [\"Incremental training task\"],\n",
      "    \"1+15\": [\"Incremental training task\"],\n",
      "    \"1+19\": [\"Incremental training task\"]\n",
      "}\n",
      "```\n",
      "Each identifier in the mathematical expressions within \"<||>\" is included in the JSON dictionary, with their respective definitions as keys and values.</s>\n",
      "4914 21326\n",
      "Actual total tokens till now: 26240\n",
      "Iteration 22 of 38\n",
      "Iteration 23 of 38\n",
      "Iteration 24 of 38\n",
      "Iteration 25 of 38\n",
      "Iteration 26 of 38\n",
      "Iteration 27 of 38\n",
      "\n",
      "\n",
      "\n",
      "1704 prompt tokens counted.\n",
      "\n",
      "Do not include the angle brackets in your dictionary.\n",
      "ASSISTANT: ğŸ“Œ Here is a JSON dictionary for the mathematical identifiers inside \"<||>\" in the given text:\n",
      "```arduino\n",
      "identifiers = {\n",
      "    \"â„’\": {\"Sets\", \"Modules\", \"Variables\"},\n",
      "    \"ğ‘ ğ‘ğ‘ğ‘Ÿğ‘ ğ‘–ğ‘¡ğ‘¦\": \"Set of variables\",\n",
      "    \"ğ‘˜ğ‘‘\": \"Variable\",\n",
      "    \"ğ‘‘ğ‘–ğ‘£ğ‘’ğ‘Ÿğ‘ ğ‘–ğ‘¡ğ‘¦\": \"Set of variables\",\n",
      "    \"ğšğšŠğšœğš”ğŸ·\": \"Variable\",\n",
      "    \"ğšğšŠğšœğš”ğŸ¸\": \"Variable\",\n",
      "    \"ğšğšŠğšœğš”ğŸ·\": \"Variable\",\n",
      "    \"ğšğšŠğšœğš”ğŸ¸\": \"Variable\",\n",
      "    \"ğš\": \"Gated modules\",\n",
      "    \"ğŸ¿\": \"Task\",\n",
      "    \"ğŸ±\": \"Task\",\n",
      "    \"ğŸ²\": \"Task\",\n",
      "    \"ğŸ³\": \"Task\",\n",
      "    \"ğŸ´\": \"Task\",\n",
      "    \"ğŸµ\": \"Task\",\n",
      "    \"ğŸ¶\": \"Task\",\n",
      "    \"ğŸ·\": \"Task\",\n",
      "    \"ğŸ¸\": \"Task\",\n",
      "    \"ğŸ¹\": \"Task\",\n",
      "    \"ğŸº\": \"Task\",\n",
      "    \"ğŸ»\": \"Task\",\n",
      "    \"ğŸ½\": \"Task\",\n",
      "    \"ğŸ¾\": \"Task\",\n",
      "    \"ğŸ¿\": \"Task\",\n",
      "    \"ğŸ±\": \"Task\",\n",
      "    \"ğŸ²\": \"Task\",\n",
      "    \"ğŸ³\": \"Task\",\n",
      "    \"ğŸ´\": \"Task\",\n",
      "    \"ğŸµ\": \"Task\",\n",
      "    \"ğŸ¶\": \"Task\",\n",
      "    \"ğŸ·\n",
      "5427 23030\n",
      "Actual total tokens till now: 28457\n",
      "Iteration 28 of 38\n",
      "\n",
      "New dictionary\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1255 prompt tokens counted.\n",
      "\n",
      "Do not include the angle brackets in your dictionary.\n",
      "ASSISTANT: ğŸ“Œ identifiers = {\n",
      "\"ğŒ\": [\"Matrix\", \"Normalized prototypical correlation matrix\"],\n",
      "\"VOC\": [\"Set\", \"VOC tasks\"],\n",
      "\"KITTI\": [\"Set\", \"KITTI tasks\"],\n",
      "\"ğšğšŠğšœğš”ğŸ·\": [\"Set\", \"Gates\"],\n",
      "\"ğšğšŠğšœğš”ğŸ¸\": [\"Set\", \"Inherited gates\"],\n",
      "\"domain gap\": [\"Measure\", \"Task differences\"],\n",
      "\"activated channels\": [\"Count\", \"Channel activation\"]\n",
      "}</s>\n",
      "5576 24285\n",
      "Actual total tokens till now: 29861\n",
      "Iteration 29 of 38\n",
      "Iteration 30 of 38\n",
      "Iteration 31 of 38\n",
      "Iteration 32 of 38\n",
      "Iteration 33 of 38\n",
      "Iteration 34 of 38\n",
      "Iteration 35 of 38\n",
      "Iteration 36 of 38\n",
      "Iteration 37 of 38\n",
      "Time taken: 0:05:30.054139\n",
      "Total time taken: 0:06:02.154240\n",
      "29861 5576 24285\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.now()\n",
    "number_of_dictionaries = 0\n",
    "for chunk in chunks:\n",
    "    print(f\"Iteration {i} of {len(chunks)}\")\n",
    "    i += 1\n",
    "    if chunk == question:\n",
    "        continue\n",
    "    if not contains_pattern(chunk):\n",
    "        continue\n",
    "    question = chunk\n",
    "    \n",
    "    if prompt_size > 1600:\n",
    "        number_of_dictionaries += 1\n",
    "        print(\"\\nNew dictionary\\n\")\n",
    "        dictionary.append({})\n",
    "    \n",
    "    prompt = [\n",
    "            {'role': 'system',\n",
    "             'content': 'You are a helpful research assistant tasked with converting long paragraphs into a JSON '\n",
    "                        'dictionary. '\n",
    "                        'The goal is to identify and classify each individual mathematical symbol, variable, '\n",
    "                        'and identifier in the text marked between \"<||>\"'\n",
    "                        'The dictionary should store the identifiers as keys and their corresponding definitions as '\n",
    "                        'values in an array format. '},\n",
    "            {'role': 'system', 'name': 'example_user', 'content': '''A relational model is a triple <|M|>â€²=(<|X|>,<|R|>,\n",
    "            <|v|>), where <|X|> is a set of states, <|R|><|âŠ†|><|X|><|Ã—|><|X|> is a binary relation on <|X|>, \n",
    "            and <|v|>:<|ğ–¯ğ—‹ğ—ˆğ—‰|>â†’2<|X|> is a valuation. Given a relational model <|M|>â€², the satisfaction relation \n",
    "            between points <|x<|âˆˆ<|X<| and formulas <|Ï†<|âˆˆ<|â„’<|<|ğ–ªğ– <| is defined inductively by <|M|>â€²,\n",
    "            <|x|>âŠ¨<|ğ–ª|><|Ï†|>â‡” for all <|y|>âˆˆ<|X|>,<|x|><|R|><|y|> implies <|M|>â€²,<|y|>âŠ¨<|Ï†|><|M|>â€²,\n",
    "            <|x|>âŠ¨<|ğ– |><|Ï†|><||>â‡” for all <|y|>âˆˆ<|X|>,<|M|>â€²,<|y|>âŠ¨<|Ï†|>'''},\n",
    "            {'role': 'system', 'name': 'example_assistant', 'content': '''identifiers = {\n",
    "            \"M\": [\"Model\", \"Expertise Model\"],\n",
    "            \"M'\": \"Relational model\",\n",
    "            \"X\": \"Set of states\",\n",
    "            \"R\": \"Binary relation on X\",\n",
    "            \"v\": \"Valuation\",\n",
    "            \"ğ–¯ğ—‹ğ—ˆğ—‰\": \"Set of propositions\",\n",
    "            \"M'\": \"Relational model\",\n",
    "            \"x\": \"Point in X\",\n",
    "            \"Ï†\": \"Formula in ğ–ªğ– \",\n",
    "            \"â„’_{ğ–ªğ– }\": \"Set of formulas\",\n",
    "            \"ğ–ª\": \"Modal operator K\",\n",
    "            \"ğ– \": \"Modal operator A\",\n",
    "            \"y\": \"Point in X\",\n",
    "            \"âŠ¨\": \"Satisfaction relation\",\n",
    "            \"â‡”\": \"If and only if operator\",\n",
    "            \"âˆˆ\": \"Element of a set\",\n",
    "            \"âŠ†\": \"Subset of a set\",\n",
    "            \"Ã—\": \"Cartesian product operator\",\n",
    "            \"â†’\": \"Function or implication operator\",\n",
    "            \"for all\": \"Universal quantifier\"\n",
    "            }'''},\n",
    "            {'role': 'system',\n",
    "             'content': f'Given is already a pre existing dictionary. Your job is to extend this dictionary. Do not '\n",
    "                        f'remove any pre existing definitions from this dictionary.'\n",
    "                        f'\\n{dictionary[number_of_dictionaries]}. If there is nothing to mention, reply with an empty '\n",
    "                        f'dictionary'},\n",
    "            {'role': 'user', 'content': f'Generate a JSON dictionary for the following text: {question}. '\n",
    "                                        'Only consider the mathematical identifiers inside \"<||>\" for the dictionary. '\n",
    "                                        'Do not consider any other identifier other than those marked. '\n",
    "                                        'Consider all the identifiers individually. Do not skip any identifier, mention'\n",
    "                                        ' all the identifiers inside \"<||>\" in your dictionary. '\n",
    "                                        'Do not include the angle brackets in your dictionary.'}\n",
    "        ]\n",
    "    \n",
    "    open_prompt = get_prompt_loop(prompt)\n",
    "    \n",
    "    prompt_size = num_tokens_from_messages(open_prompt)\n",
    "    print(f\"\\n\\n\\n{prompt_size} prompt tokens counted.\\n\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            input_ids = tokenizer(open_prompt, return_tensors='pt').input_ids.cuda()\n",
    "            output = model.generate(inputs=input_ids, temperature=0.5, max_new_tokens=512, repetition_penalty=1.05)\n",
    "            output_string = tokenizer.decode(output[0])\n",
    "            \n",
    "            actual_total_tokens += num_tokens_from_messages(output_string)\n",
    "            completion_tokens += num_tokens_from_messages(output_string) - num_tokens_from_messages(open_prompt)\n",
    "            prompt_tokens += num_tokens_from_messages(open_prompt)\n",
    "\n",
    "            ind = output_string.index('Do not include the angle brackets in your dictionary.')\n",
    "            print(output_string[ind:])\n",
    "            \n",
    "            print(completion_tokens, prompt_tokens)\n",
    "    \n",
    "            print(f\"Actual total tokens till now: {actual_total_tokens}\")\n",
    "\n",
    "            new_dictionary = {}\n",
    "\n",
    "            try:\n",
    "                correct_output_string = modify_string(output_string[ind:])\n",
    "                dic_output_string = get_json_of_string(correct_output_string)\n",
    "                new_dictionary = string_to_dict(dic_output_string)\n",
    "            except Exception as e:\n",
    "                print(\"INCORRECT DICTIONARY\", e)\n",
    "            dictionary[number_of_dictionaries] = merge_dictionaries(dictionary[number_of_dictionaries], new_dictionary)\n",
    "            \n",
    "            break\n",
    "        except Exception as e:\n",
    "            number_of_dictionaries += 1\n",
    "            dictionary.append({})\n",
    "            print(f\"Exception occurred: {e}\")\n",
    "            print(\"Retrying...\")\n",
    "total_time_taken += (datetime.now() - start_time)\n",
    "print(f\"Time taken: {datetime.now() - start_time }\")\n",
    "print(f\"Total time taken: {total_time_taken}\")\n",
    "print(actual_total_tokens, completion_tokens, prompt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04278ee2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dct = {}\n",
    "for dic in dictionary:\n",
    "    dct = merge_dictionaries(dct, dic)\n",
    "# pprint.pprint(dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d4212dbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Continual': '['continual image classification', 'Continual Object Detection']'\n",
      "'D': '['dataset', 'Discrete', ['Datasets', 'Data', 'Incremental object detection']]'\n",
      "'<|t|>': '['task', 'time']'\n",
      "'<|Î”|>': '['datasets', 'tasks', 'datasets']'\n",
      "'<|C|><|t|>': '['categories', 'categories', 'set of categories', 'object of interests', ['class embeddings', 'class concatenation']]'\n",
      "'â„’': '['Loss function', 'Gated convolution', ['Class-to-task correlation', 'Correlation', ['Class-to-task correlation', 'Correlation'], ['Class-to-task correlation', 'Correlation'], ['Class-to-task correlation', 'Correlation']], ['Sets', 'Modules', 'Variables']]'\n",
      "'t': '['Time', ['Task', 'Tasks'], ['Class', 'Category']]'\n",
      "'âŠ¨': '['Satisfies', 'Implied by']'\n",
      "'âˆ¶': '['Sparse', 'Sparse']'\n",
      "'Î©': '['Integral', 'Integral']'\n",
      "'âˆ´': '['Causal', 'Causal']'\n",
      "'C': '['Class to be recognized', 'Classes for task', ['Categories', 'Classes']]'\n",
      "'n': '['New task', 'Tasks']'\n",
      "'m': '['Old task', 'Tasks']'\n",
      "'â„Œ': '['Class-to-task correlation', 'Correlation', ['Class-to-task correlation', 'Correlation'], ['Class-to-task correlation', 'Correlation'], ['Class-to-task correlation', 'Correlation']]'\n",
      "'â„›': '['Task-to-task correlation', 'Correlation', ['Task-to-task correlation', 'Correlation'], ['Task-to-task correlation', 'Correlation'], ['Task-to-task correlation', 'Correlation'], ['Prototypical task correlation', 'Gating diversity controller', 'Weight for diversity loss']]'\n",
      "'â„‘': '['Class', 'Category', 'Class/category']'\n",
      "'â„œ': '['Task-to-task correlation', 'Correlation', ['Task-to-task correlation', 'Correlation'], ['Task-to-task correlation', 'Correlation'], ['Task-to-task correlation', 'Correlation']]'\n",
      "'â„š': '['Set', 'Collection', 'Gating mechanism degradation']'\n",
      "'â„': '['Aggregation', 'Sum', 'Set/collection']'\n",
      "'â„': '['Average', 'Mean', ['Aggregation/sum', 'Average/mean']]'\n",
      "'i': '['Index', 'Dataset']'\n",
      "'j': '['Index', 'Dataset']'\n",
      "'t+': '['Class', 'Subclass']'\n",
      "'COCO': '['COCO dataset']'\n",
      "'VOC': '['Pascal VOC dataset', ['Set', 'VOC tasks']]'\n",
      "'KITTI': '['KITTI dataset', ['Set', 'KITTI tasks']]'\n",
      "'Kitchen': '['Kitchen dataset']'\n",
      "'23': '['Reference for class-incremental object detection']'\n",
      "'34': '['Reference for task-incremental object detection']'\n",
      "'11': '['Pascal VOC dataset']'\n",
      "'14': '['KITTI dataset']'\n",
      "'15': '['Kitchen dataset']'\n",
      "'30': '['COCO dataset']'\n",
      "'1+1': '['Incremental training task']'\n",
      "'1+10': '['Incremental training task']'\n",
      "'1+15': '['Incremental training task']'\n",
      "'1+19': '['Incremental training task']'\n",
      "'ğšğšŠğšœğš”ğŸ·': '['Variable', 'Variable', ['Set', 'Gates']]'\n",
      "'ğšğšŠğšœğš”ğŸ¸': '['Variable', 'Variable', ['Set', 'Inherited gates']]'\n",
      "'ğŸ¿': '['Task', 'Task']'\n",
      "'ğŸ±': '['Task', 'Task']'\n",
      "'ğŸ²': '['Task', 'Task']'\n",
      "'ğŸ³': '['Task', 'Task']'\n",
      "'ğŸ´': '['Task', 'Task']'\n",
      "'ğŸµ': '['Task', 'Task']'\n",
      "'ğŸ¶': '['Task', 'Task']'\n",
      "'ğŒ': '['Matrix', 'Normalized prototypical correlation matrix']'\n",
      "'domain gap': '['Measure', 'Task differences']'\n",
      "'activated channels': '['Count', 'Channel activation']'\n"
     ]
    }
   ],
   "source": [
    "for key, value in dct.items():\n",
    "    if type(value) == list:\n",
    "        print(f\"'{key}': '{value}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed252bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_list(input_list):\n",
    "    output_list = []\n",
    "    for i in input_list:\n",
    "        if isinstance(i, list):\n",
    "            output_list.extend(flatten_list(i))\n",
    "        else:\n",
    "            output_list.append(i)\n",
    "    return output_list\n",
    "\n",
    "\n",
    "def remove_duplicates(input_list):\n",
    "    output_list = []\n",
    "    for item in input_list:\n",
    "        if item not in output_list:\n",
    "            output_list.append(item)\n",
    "    if len(output_list) == 1:\n",
    "        return output_list[0]\n",
    "    return output_list\n",
    "\n",
    "\n",
    "def process_value(v):\n",
    "    if isinstance(v, str):\n",
    "        new_v = v.replace('$', '')\n",
    "        while '\\\\\\\\' in new_v:\n",
    "            new_v = new_v.replace('\\\\\\\\', '\\\\').replace('\\n', '')\n",
    "    else:  # Assuming it's a list\n",
    "        new_v = flatten_list([process_value(val) for val in v])\n",
    "        \n",
    "    return remove_duplicates(new_v) if isinstance(new_v, list) else new_v\n",
    "\n",
    "\n",
    "def reduce_pairs(dictionary):\n",
    "    new_dict = {}\n",
    "    for k, v in dictionary.items():\n",
    "        # reduce key backslashes\n",
    "        new_k = k.replace('$', '')\n",
    "        while '\\\\\\\\' in new_k:\n",
    "            new_k = new_k.replace('\\\\\\\\', '\\\\')\n",
    "\n",
    "        # process value\n",
    "        new_v = process_value(v)\n",
    "\n",
    "        new_dict[new_k] = new_v\n",
    "\n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "85c8083f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dict_without_backslashes = reduce_pairs(dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ab472ef2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pprint.pprint(dict_without_backslashes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "09b426f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_json = dict_without_backslashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0dec9c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{file_code}_mcdict.json', 'r', encoding='utf-8') as f:\n",
    "    mc_dict_original = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4284b146",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to create a hex code (a binary representation of the key)\n",
    "def get_hex_code(key):\n",
    "    return key.encode().hex()\n",
    "\n",
    "mc_dict_original['_author'] = model_name_or_path\n",
    "\n",
    "# Iterate over your dictionary and fill the new one\n",
    "for key, values in parsed_json.items():\n",
    "    # Determine the base key and the affix\n",
    "    base_key = re.match(r\"^[^*'_^,(\\[]*\", key).group()\n",
    "    affix = key[len(base_key):]\n",
    "\n",
    "    hex_code = get_hex_code(base_key)\n",
    "    values = values if isinstance(values, list) else [values]\n",
    "\n",
    "    if hex_code in mc_dict_original[\"concepts\"]:\n",
    "        k = list(mc_dict_original[\"concepts\"][hex_code][\"identifiers\"].keys())[0]\n",
    "        new_identifier = []\n",
    "        for value in values:\n",
    "            mc_dict_original[\"concepts\"][hex_code][\"identifiers\"][k].append({\n",
    "                \"affixes\": [affix] if affix else [],\n",
    "                \"arity\": 0,\n",
    "                \"description\": value\n",
    "            })\n",
    "    else:\n",
    "        if hex_code not in mc_dict_original[\"concepts\"]:\n",
    "            try:\n",
    "                mc_dict_original[\"concepts\"][hex_code] = {\n",
    "                    \"_surface\": {\n",
    "                        \"text\": base_key,\n",
    "                        \"unicode_name\": base_key if len(base_key) != 1 else unicodedata.name(base_key)\n",
    "                    },\n",
    "                    \"identifiers\": {\n",
    "                        'default': []\n",
    "                    }\n",
    "                }\n",
    "            except ValueError:\n",
    "                mc_dict_original[\"concepts\"][hex_code] = {\n",
    "                    \"_surface\": {\n",
    "                        \"text\": base_key,\n",
    "                        \"unicode_name\": base_key\n",
    "                    },\n",
    "                    \"identifiers\": {\n",
    "                        'default': []\n",
    "                    }\n",
    "                }\n",
    "\n",
    "        for value in values:\n",
    "            mc_dict_original[\"concepts\"][hex_code][\"identifiers\"][\"default\"].append({\n",
    "                \"affixes\": [affix] if affix else [],\n",
    "                \"arity\": 0,\n",
    "                \"description\": value\n",
    "            })\n",
    "\n",
    "\n",
    "# Convert new dictionary to a sorted dictionary\n",
    "sorted_dict = dict(sorted(mc_dict_original[\"concepts\"].items(), key=lambda x: (len(x[0]), x[0])))\n",
    "mc_dict_original[\"concepts\"] = sorted_dict\n",
    "\n",
    "# Convert new dictionary to JSON\n",
    "json_str = json.dumps(mc_dict_original, indent=4, ensure_ascii=False)\n",
    "\n",
    "#print(json_str)\n",
    "\n",
    "with open(f'{file_code}-{model_name}_mcdict.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(mc_dict_original, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a79ec2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_from_tags(element):\n",
    "    if isinstance(element, NavigableString):\n",
    "        return element\n",
    "    if element.name == 'mi':\n",
    "        return str(element)\n",
    "    return ''.join(get_text_from_tags(child) for child in element.children)\n",
    "\n",
    "def parse_html(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as html_file:\n",
    "        soup = BeautifulSoup(html_file, 'html.parser')\n",
    "\n",
    "    texts = get_text_from_tags(soup)\n",
    "    return texts\n",
    "\n",
    "def find_mi_strings(text):\n",
    "    pattern = r'(<mi.*?</mi>)'\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "    return matches\n",
    "\n",
    "# call the function with your HTML file path\n",
    "page = parse_html(f'{file_code}.html')\n",
    "matches = find_mi_strings(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "64a0cba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_dict = mc_dict_original\n",
    "with open(f'{file_code}_anno.json', encoding='utf-8') as fp:\n",
    "    parsed_annotation = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "163356dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_index_from_char_index(message, key, char_index):\n",
    "    i = 0\n",
    "    index = -1\n",
    "    for word in message:\n",
    "        if key in word:\n",
    "            index = i\n",
    "        i += 1\n",
    "    return index\n",
    "\n",
    "def expand_string_to_tokens(message, index, num_tokens_right=25, num_tokens_left=75):\n",
    "    words = message.split()  # Split the message into words\n",
    "\n",
    "    # Start at the index where the center word is\n",
    "    left_index = right_index = index\n",
    "\n",
    "    tokens_counter_right = num_tokens_from_messages(words[right_index])\n",
    "    tokens_counter_left = num_tokens_from_messages(words[left_index])\n",
    "\n",
    "    # Expand to the left from the center index until you reach num_tokens_left\n",
    "    while tokens_counter_left < num_tokens_left and left_index > 0:\n",
    "        left_index -= 1\n",
    "        tokens_counter_left += num_tokens_from_messages(words[left_index])\n",
    "\n",
    "    # Expand to the right from the center index until you reach num_tokens_right\n",
    "    while tokens_counter_right < num_tokens_right and right_index < len(words) - 1:\n",
    "        right_index += 1\n",
    "        tokens_counter_right += num_tokens_from_messages(words[right_index])\n",
    "\n",
    "    # Combine the words back into a string and return\n",
    "    return ' '.join(words[left_index:right_index + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "01b6b971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_text(text, replacement, exception):\n",
    "    # Find all matches\n",
    "    matches = re.findall(r'<mi(.*?)</mi>', text)\n",
    "    \n",
    "    for match in matches:\n",
    "        original_string = f'<mi{match}</mi>'\n",
    "        \n",
    "        # Skip exception\n",
    "        if original_string == exception:\n",
    "            continue\n",
    "        \n",
    "        # Replace match\n",
    "        replaced_string = re.sub(r'<.*?>(.*?)</.*?>', r'\\1', original_string)\n",
    "        text = text.replace(original_string, replaced_string)\n",
    "\n",
    "    return text\n",
    "\n",
    "def get_context(match):\n",
    "    match_len = len(match)\n",
    "    new_page = replace_text(page, '', match)\n",
    "    char_index = new_page.index(match) + int(match_len/2)\n",
    "    word_index = get_word_index_from_char_index(new_page, char_index)\n",
    "    section = expand_string_to_tokens(new_page, word_index)\n",
    "    section = re.sub(r'<.*?>(.*?)</.*?>', r'<<\\1>>', section)\n",
    "    return match, section\n",
    "\n",
    "# Function to create a hex code (a binary representation of the key)\n",
    "def get_hex_code(key):\n",
    "    return key.encode('utf-8').hex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7706ec86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_trailing_tags(s):\n",
    "    parts = re.split('(<mi)', s)\n",
    "    for i in range(1, len(parts), 2):\n",
    "        if '>' not in parts[i + 1]:\n",
    "            parts[i] = ''\n",
    "            parts[i + 1] = ''\n",
    "    return ''.join(parts)\n",
    "\n",
    "def get_definition_of_id(dict_id, identifier):\n",
    "    \n",
    "    try:\n",
    "        hex_code = get_hex_code(identifier)\n",
    "        index = parsed_annotation['mi_anno'][dict_id]['concept_id']\n",
    "        key = list(parsed_dict['concepts'][hex_code]['identifiers'].keys())[0]\n",
    "        return f\"({parsed_dict['concepts'][hex_code]['identifiers'][key][index]['description']})\"\n",
    "    except Exception as e:\n",
    "        return \"\"\n",
    "\n",
    "def get_context(match):\n",
    "    key_word = page.index(match) + len(match)\n",
    "    last_index = min(len(page), key_word + 500)\n",
    "    first_index = max(0, key_word - 3000)\n",
    "    context_window = page[first_index:last_index]\n",
    "    \n",
    "    reg_matches = re.findall(r'<mi(.*?)</mi>', context_window)\n",
    "    \n",
    "    identifier = None\n",
    "    \n",
    "    for reg_match in reg_matches:\n",
    "        original_string = f'<mi{reg_match}</mi>'\n",
    "        soup = BeautifulSoup(original_string, 'html.parser')\n",
    "        \n",
    "        \n",
    "        tags = soup.find_all('mi')\n",
    "        \n",
    "        if original_string == match:\n",
    "            identifier = tags[0].text\n",
    "            continue\n",
    "\n",
    "        context_window = context_window.replace(original_string,\n",
    "                                                f\"{tags[0].text}{get_definition_of_id(tags[0].get('id'), tags[0].text)}\")\n",
    "    \n",
    "    context_window = re.sub(r'<mi.*?>(.*?)<\\/mi>', r'<<\\1>>', context_window)\n",
    "    \n",
    "    context_window = remove_trailing_tags(context_window)\n",
    "    context_window = re.sub(r'^(?!.*<mi.*).*<\\/mi>', '', context_window, flags=re.DOTALL)\n",
    "        \n",
    "    index = 0\n",
    "    for word in context_window.split():\n",
    "        if f\"<<{identifier}>>\" in word:\n",
    "            word_index = index\n",
    "        index += 1\n",
    "    \n",
    "    if word_index == -1:\n",
    "        return context_window\n",
    "    else:\n",
    "        context_window = expand_string_to_tokens(context_window, word_index)\n",
    "    return context_window\n",
    "\n",
    "#print(get_context('<mi id=\"S1.p2.1.m1.1.1.3\" xref=\"S1.p2.1.m1.1.1.3.cmml\">Ï†</mi>'))\n",
    "#get_context('<mi id=\"S1.p2.1.m1.1.1.2\" xref=\"S1.p2.1.m1.1.1.2.cmml\">ğ–¤</mi>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "62ec5036-7177-46d0-9728-571edb9f6bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_anno(prompt):\n",
    "    return f'''SYSTEM: {prompt[0]['content']}\n",
    "USER: {prompt[1]['content']}\n",
    "ASSISTANT:'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f57696ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 of 486: Key does not exist in the dictionary of concepts  \n",
      "Iteration 2 of 486: Key does not exist in the dictionary of concepts  \n",
      "Iteration 3 of 486: Key does not exist in the dictionary of concepts  \n",
      "Iteration 4 of 486: None\n",
      "Iteration 5 of 486: ASSISTANT: 1</s>\n",
      "6 315\n",
      "1\n",
      "Iteration 6 of 486: ASSISTANT: 14</s>\n",
      "13 834\n",
      "14\n",
      "Iteration 7 of 486: ASSISTANT: 1</s>\n",
      "19 1144\n",
      "1\n",
      "Iteration 8 of 486: ASSISTANT: 1</s>\n",
      "25 1421\n",
      "1\n",
      "Iteration 9 of 486: ASSISTANT: 1</s>\n",
      "31 1737\n",
      "1\n",
      "Iteration 10 of 486: ASSISTANT: 1</s>\n",
      "37 2021\n",
      "1\n",
      "Iteration 11 of 486: ASSISTANT: 4</s>\n",
      "43 2336\n",
      "4\n",
      "Iteration 12 of 486: ASSISTANT: 1</s>\n",
      "49 2623\n",
      "1\n",
      "Iteration 13 of 486: ASSISTANT: 1</s>\n",
      "55 2942\n",
      "1\n",
      "Iteration 14 of 486: ASSISTANT: 1</s>\n",
      "61 3229\n",
      "1\n",
      "Iteration 15 of 486: ASSISTANT: 1</s>\n",
      "67 3548\n",
      "1\n",
      "Iteration 16 of 486: ASSISTANT: 4</s>\n",
      "73 3861\n",
      "4\n",
      "Iteration 17 of 486: ASSISTANT: 2</s>\n",
      "79 4166\n",
      "2\n",
      "Iteration 18 of 486: ASSISTANT: 1</s>\n",
      "85 4482\n",
      "1\n",
      "Iteration 19 of 486: None\n",
      "Iteration 20 of 486: ASSISTANT: 4</s>\n",
      "91 4788\n",
      "4\n",
      "Iteration 21 of 486: None\n",
      "Iteration 22 of 486: ASSISTANT: 4</s>\n",
      "97 5096\n",
      "4\n",
      "Iteration 23 of 486: 0\n",
      "Iteration 24 of 486: 0\n",
      "Iteration 25 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"t\" is:\n",
      "\n",
      "index: 4 (Category)</s>\n",
      "128 5412\n",
      "4\n",
      "Iteration 26 of 486: None\n",
      "Iteration 27 of 486: 0\n",
      "Iteration 28 of 486: None\n",
      "Iteration 29 of 486: ASSISTANT: 3</s>\n",
      "134 5726\n",
      "3\n",
      "Iteration 30 of 486: 0\n",
      "Iteration 31 of 486: 0\n",
      "Iteration 32 of 486: ASSISTANT: Based on the provided text and possible annotations, the most fitting description for the identifier \"t\" is:\n",
      "\n",
      "index: 3 (Class)</s>\n",
      "168 6030\n",
      "3\n",
      "Iteration 33 of 486: None\n",
      "Iteration 34 of 486: None\n",
      "Iteration 35 of 486: 0\n",
      "Iteration 36 of 486: None\n",
      "Iteration 37 of 486: 0\n",
      "Iteration 38 of 486: None\n",
      "Iteration 39 of 486: 0\n",
      "Iteration 40 of 486: 0\n",
      "Iteration 41 of 486: ASSISTANT: 2</s>\n",
      "174 6352\n",
      "2\n",
      "Iteration 42 of 486: 0\n",
      "Iteration 43 of 486: 0\n",
      "Iteration 44 of 486: ASSISTANT: 2</s>\n",
      "180 6673\n",
      "2\n",
      "Iteration 45 of 486: None\n",
      "Iteration 46 of 486: 0\n",
      "Iteration 47 of 486: 0\n",
      "Iteration 48 of 486: 0\n",
      "Iteration 49 of 486: ASSISTANT: 4</s>\n",
      "186 6985\n",
      "4\n",
      "Iteration 50 of 486: None\n",
      "Iteration 51 of 486: 0\n",
      "Iteration 52 of 486: ASSISTANT: 2</s>\n",
      "192 7310\n",
      "2\n",
      "Iteration 53 of 486: 0\n",
      "Iteration 54 of 486: 0\n",
      "Iteration 55 of 486: ASSISTANT: 4</s>\n",
      "198 7630\n",
      "4\n",
      "Iteration 56 of 486: ASSISTANT: Based on the provided text and possible annotations, the most fitting description for the identifier C is \"Classes for task\". Therefore, the index is 1.</s>\n",
      "232 7939\n",
      "1\n",
      "Iteration 57 of 486: ASSISTANT: 2</s>\n",
      "238 8260\n",
      "2\n",
      "Iteration 58 of 486: 0\n",
      "Iteration 59 of 486: 0\n",
      "Iteration 60 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"t\" from the possible annotations is:\n",
      "\n",
      "index: 4 (Category)</s>\n",
      "273 8588\n",
      "4\n",
      "Iteration 61 of 486: 0\n",
      "Iteration 62 of 486: 0\n",
      "Iteration 63 of 486: ASSISTANT: 2</s>\n",
      "279 8934\n",
      "2\n",
      "Iteration 64 of 486: 0\n",
      "Iteration 65 of 486: 0\n",
      "Iteration 66 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"t\" is:\n",
      "\n",
      "index: 2 (Tasks)</s>\n",
      "310 9271\n",
      "2\n",
      "Iteration 67 of 486: None\n",
      "Iteration 68 of 486: None\n",
      "Iteration 69 of 486: 0\n",
      "Iteration 70 of 486: None\n",
      "Iteration 71 of 486: 0\n",
      "Iteration 72 of 486: None\n",
      "Iteration 73 of 486: 0\n",
      "Iteration 74 of 486: 0\n",
      "Iteration 75 of 486: 0\n",
      "Iteration 76 of 486: ASSISTANT: 2</s>\n",
      "316 9593\n",
      "2\n",
      "Iteration 77 of 486: 0\n",
      "Iteration 78 of 486: 0\n",
      "Iteration 79 of 486: ASSISTANT: 2</s>\n",
      "322 9930\n",
      "2\n",
      "Iteration 80 of 486: None\n",
      "Iteration 81 of 486: None\n",
      "Iteration 82 of 486: 0\n",
      "Iteration 83 of 486: None\n",
      "Iteration 84 of 486: 0\n",
      "Iteration 85 of 486: None\n",
      "Iteration 86 of 486: 0\n",
      "Iteration 87 of 486: None\n",
      "Iteration 88 of 486: 0\n",
      "Iteration 89 of 486: ASSISTANT: 3</s>\n",
      "328 10264\n",
      "3\n",
      "Iteration 90 of 486: 0\n",
      "Iteration 91 of 486: 0\n",
      "Iteration 92 of 486: ASSISTANT: 2</s>\n",
      "334 10589\n",
      "2\n",
      "Iteration 93 of 486: ASSISTANT: 2</s>\n",
      "340 10905\n",
      "2\n",
      "Iteration 94 of 486: ASSISTANT: 2</s>\n",
      "346 11239\n",
      "2\n",
      "Iteration 95 of 486: None\n",
      "Iteration 96 of 486: 0\n",
      "Iteration 97 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"t\" from the possible annotations is:\n",
      "\n",
      "index: 3, identifier: t, description: Class</s>\n",
      "386 11583\n",
      "3\n",
      "Iteration 98 of 486: None\n",
      "Iteration 99 of 486: 0\n",
      "Iteration 100 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"t\" is \"Class\". Therefore, the index is 3.</s>\n",
      "417 11905\n",
      "3\n",
      "Iteration 101 of 486: Key does not exist in the dictionary of concepts ... 2e2e2e\n",
      "Iteration 102 of 486: None\n",
      "Iteration 103 of 486: 0\n",
      "Iteration 104 of 486: None\n",
      "Iteration 105 of 486: 0\n",
      "Iteration 106 of 486: ASSISTANT: 2</s>\n",
      "423 12231\n",
      "2\n",
      "Iteration 107 of 486: 0\n",
      "Iteration 108 of 486: None\n",
      "Iteration 109 of 486: 0\n",
      "Iteration 110 of 486: None\n",
      "Iteration 111 of 486: ASSISTANT: 2</s>\n",
      "429 12571\n",
      "2\n",
      "Iteration 112 of 486: None\n",
      "Iteration 113 of 486: None\n",
      "Iteration 114 of 486: ASSISTANT: 2</s>\n",
      "435 12880\n",
      "2\n",
      "Iteration 115 of 486: ASSISTANT: 2</s>\n",
      "441 13199\n",
      "2\n",
      "Iteration 116 of 486: ASSISTANT: 2</s>\n",
      "447 13499\n",
      "2\n",
      "Iteration 117 of 486: ASSISTANT: Based on the provided text and possible annotations, the most fitting description for the identifier \"t\" is:\n",
      "\n",
      "index: 4, identifier: t, description: Category</s>\n",
      "486 13811\n",
      "4\n",
      "Iteration 118 of 486: 0\n",
      "Iteration 119 of 486: 0\n",
      "Iteration 120 of 486: ASSISTANT: 4</s>\n",
      "492 14122\n",
      "4\n",
      "Iteration 121 of 486: None\n",
      "Iteration 122 of 486: 0\n",
      "Iteration 123 of 486: None\n",
      "Iteration 124 of 486: ASSISTANT: 1</s>\n",
      "498 14437\n",
      "1\n",
      "Iteration 125 of 486: None\n",
      "Iteration 126 of 486: 0\n",
      "Iteration 127 of 486: None\n",
      "Iteration 128 of 486: ASSISTANT: 4</s>\n",
      "504 14754\n",
      "4\n",
      "Iteration 129 of 486: None\n",
      "Iteration 130 of 486: 0\n",
      "Iteration 131 of 486: None\n",
      "Iteration 132 of 486: ASSISTANT: 4</s>\n",
      "510 15069\n",
      "4\n",
      "Iteration 133 of 486: 0\n",
      "Iteration 134 of 486: 0\n",
      "Iteration 135 of 486: 0\n",
      "Iteration 136 of 486: ASSISTANT: 14</s>\n",
      "517 15600\n",
      "14\n",
      "Iteration 137 of 486: None\n",
      "Iteration 138 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"t\" from the possible annotations is:\n",
      "\n",
      "index: 4 (Category)</s>\n",
      "552 15926\n",
      "4\n",
      "Iteration 139 of 486: None\n",
      "Iteration 140 of 486: 0\n",
      "Iteration 141 of 486: None\n",
      "Iteration 142 of 486: ASSISTANT: Based on the provided text and possible annotations, the most fitting description for the identifier t (considering the context and potential affixes) is:\n",
      "\n",
      "index: 4\n",
      "identifier: t, description: Category</s>\n",
      "601 16256\n",
      "4\n",
      "Iteration 143 of 486: ASSISTANT: 14</s>\n",
      "608 16791\n",
      "14\n",
      "Iteration 144 of 486: None\n",
      "Iteration 145 of 486: ASSISTANT: 2</s>\n",
      "614 17123\n",
      "2\n",
      "Iteration 146 of 486: ASSISTANT: 2</s>\n",
      "620 17458\n",
      "2\n",
      "Iteration 147 of 486: None\n",
      "Iteration 148 of 486: None\n",
      "Iteration 149 of 486: 0\n",
      "Iteration 150 of 486: None\n",
      "Iteration 151 of 486: 0\n",
      "Iteration 152 of 486: None\n",
      "Iteration 153 of 486: None\n",
      "Iteration 154 of 486: 0\n",
      "Iteration 155 of 486: None\n",
      "Iteration 156 of 486: ASSISTANT: 4</s>\n",
      "626 17767\n",
      "4\n",
      "Iteration 157 of 486: ASSISTANT: 2</s>\n",
      "632 18058\n",
      "2\n",
      "Iteration 158 of 486: None\n",
      "Iteration 159 of 486: 0\n",
      "Iteration 160 of 486: None\n",
      "Iteration 161 of 486: ASSISTANT: 2</s>\n",
      "638 18372\n",
      "2\n",
      "Iteration 162 of 486: None\n",
      "Iteration 163 of 486: 0\n",
      "Iteration 164 of 486: None\n",
      "Iteration 165 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"t\" is:\n",
      "\n",
      "index: 4 (Category)</s>\n",
      "669 18686\n",
      "4\n",
      "Iteration 166 of 486: ASSISTANT: 1</s>\n",
      "675 18998\n",
      "1\n",
      "Iteration 167 of 486: 0\n",
      "Iteration 168 of 486: 0\n",
      "Iteration 169 of 486: ASSISTANT: 4</s>\n",
      "681 19304\n",
      "4\n",
      "Iteration 170 of 486: ASSISTANT: 12</s>\n",
      "688 20198\n",
      "12\n",
      "Iteration 171 of 486: 0\n",
      "Iteration 172 of 486: ASSISTANT: 4</s>\n",
      "694 20511\n",
      "4\n",
      "Iteration 173 of 486: 0\n",
      "Iteration 174 of 486: 0\n",
      "Iteration 175 of 486: ASSISTANT: 2</s>\n",
      "700 20829\n",
      "2\n",
      "Iteration 176 of 486: 0\n",
      "Iteration 177 of 486: 0\n",
      "Iteration 178 of 486: ASSISTANT: 3</s>\n",
      "706 21173\n",
      "3\n",
      "Iteration 179 of 486: ASSISTANT: 13</s>\n",
      "713 22112\n",
      "13\n",
      "Iteration 180 of 486: 0\n",
      "Iteration 181 of 486: ASSISTANT: 2</s>\n",
      "719 22422\n",
      "2\n",
      "Iteration 182 of 486: None\n",
      "Iteration 183 of 486: 0\n",
      "Iteration 184 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"t\" is:\n",
      "\n",
      "index: 3, description: Class</s>\n",
      "751 22743\n",
      "3\n",
      "Iteration 185 of 486: None\n",
      "Iteration 186 of 486: 0\n",
      "Iteration 187 of 486: ASSISTANT: 2</s>\n",
      "757 23058\n",
      "2\n",
      "Iteration 188 of 486: Key does not exist in the dictionary of concepts ... 2e2e2e\n",
      "Iteration 189 of 486: None\n",
      "Iteration 190 of 486: 0\n",
      "Iteration 191 of 486: None\n",
      "Iteration 192 of 486: 0\n",
      "Iteration 193 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"t\" is:\n",
      "\n",
      "index: 3 (Tasks)</s>\n",
      "788 23374\n",
      "3\n",
      "Iteration 194 of 486: ASSISTANT: 5</s>\n",
      "794 23763\n",
      "5\n",
      "Iteration 195 of 486: 0\n",
      "Iteration 196 of 486: 0\n",
      "Iteration 197 of 486: 0\n",
      "Iteration 198 of 486: 0\n",
      "Iteration 199 of 486: ASSISTANT: 12</s>\n",
      "801 24307\n",
      "12\n",
      "Iteration 200 of 486: ASSISTANT: 1</s>\n",
      "807 24652\n",
      "1\n",
      "Iteration 201 of 486: 0\n",
      "Iteration 202 of 486: 0\n",
      "Iteration 203 of 486: 0\n",
      "Iteration 204 of 486: None\n",
      "Iteration 205 of 486: 0\n",
      "Iteration 206 of 486: 0\n",
      "Iteration 207 of 486: 0\n",
      "Iteration 208 of 486: ASSISTANT: 4</s>\n",
      "813 24990\n",
      "4\n",
      "Iteration 209 of 486: ASSISTANT: Based on the provided text, I would choose index 3 (categories) for the identifier C.</s>\n",
      "836 25317\n",
      "3\n",
      "Iteration 210 of 486: ASSISTANT: 1</s>\n",
      "842 25656\n",
      "1\n",
      "Iteration 211 of 486: None\n",
      "Iteration 212 of 486: 0\n",
      "Iteration 213 of 486: 0\n",
      "Iteration 214 of 486: ASSISTANT: 3</s>\n",
      "848 26033\n",
      "3\n",
      "Iteration 215 of 486: 0\n",
      "Iteration 216 of 486: 0\n",
      "Iteration 217 of 486: 0\n",
      "Iteration 218 of 486: 0\n",
      "Iteration 219 of 486: 0\n",
      "Iteration 220 of 486: 0\n",
      "Iteration 221 of 486: 0\n",
      "Iteration 222 of 486: ASSISTANT: 2</s>\n",
      "854 26366\n",
      "2\n",
      "Iteration 223 of 486: 0\n",
      "Iteration 224 of 486: 0\n",
      "Iteration 225 of 486: ASSISTANT: 2</s>\n",
      "860 26711\n",
      "2\n",
      "Iteration 226 of 486: 0\n",
      "Iteration 227 of 486: 0\n",
      "Iteration 228 of 486: ASSISTANT: 4</s>\n",
      "866 27034\n",
      "4\n",
      "Iteration 229 of 486: 0\n",
      "Iteration 230 of 486: 0\n",
      "Iteration 231 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"t\" from the possible annotations is:\n",
      "\n",
      "index: 3 (Class)</s>\n",
      "901 27350\n",
      "3\n",
      "Iteration 232 of 486: 0\n",
      "Iteration 233 of 486: 0\n",
      "Iteration 234 of 486: ASSISTANT: 2</s>\n",
      "907 27673\n",
      "2\n",
      "Iteration 235 of 486: 0\n",
      "Iteration 236 of 486: 0\n",
      "Iteration 237 of 486: ASSISTANT: 2</s>\n",
      "913 28000\n",
      "2\n",
      "Iteration 238 of 486: 0\n",
      "Iteration 239 of 486: 0\n",
      "Iteration 240 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"t\" is:\n",
      "index: 3</s>\n",
      "940 28330\n",
      "3\n",
      "Iteration 241 of 486: ASSISTANT: 3</s>\n",
      "946 28709\n",
      "3\n",
      "Iteration 242 of 486: 0\n",
      "Iteration 243 of 486: ASSISTANT: 3</s>\n",
      "952 29134\n",
      "3\n",
      "Iteration 244 of 486: 0\n",
      "Iteration 245 of 486: 0\n",
      "Iteration 246 of 486: ASSISTANT: 2</s>\n",
      "958 29455\n",
      "2\n",
      "Iteration 247 of 486: None\n",
      "Iteration 248 of 486: 0\n",
      "Iteration 249 of 486: ASSISTANT: 2</s>\n",
      "964 29785\n",
      "2\n",
      "Iteration 250 of 486: None\n",
      "Iteration 251 of 486: None\n",
      "Iteration 252 of 486: 0\n",
      "Iteration 253 of 486: ASSISTANT: 2</s>\n",
      "970 30132\n",
      "2\n",
      "Iteration 254 of 486: None\n",
      "Iteration 255 of 486: 0\n",
      "Iteration 256 of 486: ASSISTANT: 2</s>\n",
      "976 30487\n",
      "2\n",
      "Iteration 257 of 486: None\n",
      "Iteration 258 of 486: None\n",
      "Iteration 259 of 486: 0\n",
      "Iteration 260 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"t\" from the possible annotations is:\n",
      "\n",
      "index: 1 (Task)</s>\n",
      "1011 30847\n",
      "1\n",
      "Iteration 261 of 486: None\n",
      "Iteration 262 of 486: 0\n",
      "Iteration 263 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"t\" from the possible annotations is:\n",
      "\n",
      "index: 3, identifier: t, description: Class</s>\n",
      "1051 31179\n",
      "3\n",
      "Iteration 264 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier <<i>> is \"Class\". Therefore, the index for this annotation would be '2'.</s>\n",
      "1087 31487\n",
      "2\n",
      "Iteration 265 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier <<m>> is \"Tasks\". Therefore, the index to be selected is 1.</s>\n",
      "1121 31783\n",
      "1\n",
      "Iteration 266 of 486: 0\n",
      "Iteration 267 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"t\" (with potential affix \"g\") from the possible annotations is:\n",
      "\n",
      "index: 3, identifier: t, description: Class</s>\n",
      "1169 32140\n",
      "3\n",
      "Iteration 268 of 486: None\n",
      "Iteration 269 of 486: 0\n",
      "Iteration 270 of 486: None\n",
      "Iteration 271 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"t\" from the possible annotations is:\n",
      "\n",
      "index: 3, identifier: t, description: Class</s>\n",
      "1209 32512\n",
      "3\n",
      "Iteration 272 of 486: ASSISTANT: I cannot determine the most fitting annotation index for the identifier <<ğ•€>> based on the provided text.</s>\n",
      "1235 32872\n",
      "'NoneType' object has no attribute 'group'\n",
      "Iteration 273 of 486: None\n",
      "Iteration 274 of 486: 0\n",
      "Iteration 275 of 486: None\n",
      "Iteration 276 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"t\" from the possible annotations is:\n",
      "\n",
      "index: 4 (Category)</s>\n",
      "1270 33254\n",
      "4\n",
      "Iteration 277 of 486: None\n",
      "Iteration 278 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"i\" is \"Index\". Therefore, the index value for this annotation is 1.</s>\n",
      "1305 33527\n",
      "1\n",
      "Iteration 279 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"m\" is \"Tasks\". Therefore, the index to be selected is 1.</s>\n",
      "1339 33800\n",
      "1\n",
      "Iteration 280 of 486: 0\n",
      "Iteration 281 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"t\" (with potential affix \"g\") from the possible annotations is:\n",
      "\n",
      "index: 3, identifier: t, description: Class</s>\n",
      "1387 34133\n",
      "3\n",
      "Iteration 282 of 486: None\n",
      "Iteration 283 of 486: 0\n",
      "Iteration 284 of 486: None\n",
      "Iteration 285 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"t\" is:\n",
      "\n",
      "index: 3, identifier: t, description: Class</s>\n",
      "1423 34472\n",
      "3\n",
      "Iteration 286 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"m\" is the one with index '1', as it corresponds to \"Tasks\". Therefore, the index to be selected is 1.</s>\n",
      "1468 34777\n",
      "1\n",
      "Iteration 287 of 486: 0\n",
      "Iteration 288 of 486: ASSISTANT: 2</s>\n",
      "1474 35139\n",
      "2\n",
      "Iteration 289 of 486: 0\n",
      "Iteration 290 of 486: ASSISTANT: 2</s>\n",
      "1480 35530\n",
      "2\n",
      "Iteration 291 of 486: ASSISTANT: 1</s>\n",
      "1486 35907\n",
      "1\n",
      "Iteration 292 of 486: None\n",
      "Iteration 293 of 486: None\n",
      "Iteration 294 of 486: 0\n",
      "Iteration 295 of 486: ASSISTANT: Based on the provided text and possible annotations, the most fitting description for the identifier \"t\" is:\n",
      "\n",
      "index: 3, identifier: t, description: Class</s>\n",
      "1525 36219\n",
      "3\n",
      "Iteration 296 of 486: ASSISTANT: 2</s>\n",
      "1531 36533\n",
      "2\n",
      "Iteration 297 of 486: 0\n",
      "Iteration 298 of 486: None\n",
      "Iteration 299 of 486: 0\n",
      "Iteration 300 of 486: ASSISTANT: 4</s>\n",
      "1537 36842\n",
      "4\n",
      "Iteration 301 of 486: ASSISTANT: 2</s>\n",
      "1543 37154\n",
      "2\n",
      "Iteration 302 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"i\" is \"Class prototype\". Therefore, the index for this annotation would be 1, which corresponds to \"Dataset\".</s>\n",
      "1587 37414\n",
      "1\n",
      "Iteration 303 of 486: None\n",
      "Iteration 304 of 486: ASSISTANT: Based on the provided text and possible annotations, the most fitting description for the identifier \"i\" is \"Index\". Therefore, the index value for this annotation is 1.</s>\n",
      "1625 37672\n",
      "1\n",
      "Iteration 305 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"t\" from the possible annotations is:\n",
      "\n",
      "index: 3 (Class)</s>\n",
      "1660 37983\n",
      "3\n",
      "Iteration 306 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"i\" is \"Dataset\". Therefore, the index is 1.</s>\n",
      "1692 38252\n",
      "1\n",
      "Iteration 307 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier <<ğŒ>> is <<index: 1, identifier: ğŒ, description: Normalized prototypical correlation matrix>>>.</s>\n",
      "1742 38523\n",
      "1\n",
      "Iteration 308 of 486: ASSISTANT: Based on the provided text and possible annotations, the most fitting description for the identifier <<m>> is Â«TasksÂ». Therefore, the index to be selected is 1.</s>\n",
      "1779 38781\n",
      "1\n",
      "Iteration 309 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier <<n>> is \"Tasks\". Therefore, the index to be returned is 1.</s>\n",
      "1813 39042\n",
      "1\n",
      "Iteration 310 of 486: None\n",
      "Iteration 311 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier <<C>> is \"Classes for task\". Therefore, the index is 1.</s>\n",
      "1846 39345\n",
      "1\n",
      "Iteration 312 of 486: ASSISTANT: Based on the provided text and possible annotations, the most fitting description for the identifier \"m\" is \"Tasks\". Therefore, the index to be selected is 1.</s>\n",
      "1883 39611\n",
      "1\n",
      "Iteration 313 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier <<C>> is index 2 (Categories).</s>\n",
      "1911 39918\n",
      "2\n",
      "Iteration 314 of 486: ASSISTANT: Based on the provided text and possible annotations, the most fitting description for the identifier \"n\" is index 1, which corresponds to the annotation: {'index': '1', 'identifier': 'n', 'description': 'Tasks'}.</s>\n",
      "1964 40184\n",
      "1\n",
      "Iteration 315 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"m\" is \"Tasks\". Therefore, the index to be selected is 1.</s>\n",
      "1998 40459\n",
      "1\n",
      "Iteration 316 of 486: ASSISTANT: Based on the provided text and possible annotations, the most fitting description for the identifier <<n>> is <<Tasks>>, which corresponds to the index <<1>>.</s>\n",
      "2034 40733\n",
      "1\n",
      "Iteration 317 of 486: ASSISTANT: Based on the provided text and possible annotations, the most fitting description for the identifier \"m\" is \"Tasks\". Therefore, the index to be selected is 1.</s>\n",
      "2071 41013\n",
      "1\n",
      "Iteration 318 of 486: ASSISTANT: Based on the provided text and possible annotations, the most fitting description for the identifier \"n\" is \"Tasks\". Therefore, the index to be returned is 1.</s>\n",
      "2108 41302\n",
      "1\n",
      "Iteration 319 of 486: ASSISTANT: 1</s>\n",
      "2114 41575\n",
      "1\n",
      "Iteration 320 of 486: ASSISTANT: Based on the provided text and possible annotations, the most fitting description for the identifier \"i\" is \"Index\". Therefore, the index for this annotation is 1.</s>\n",
      "2151 41836\n",
      "1\n",
      "Iteration 321 of 486: ASSISTANT: Based on the provided text and possible annotations, the most fitting description for the identifier \"j\" is \"Index\". Therefore, the index for this annotation is 1.</s>\n",
      "2188 42098\n",
      "1\n",
      "Iteration 322 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"m\" is \"Tasks\". Therefore, the index to be selected is 1.</s>\n",
      "2222 42362\n",
      "1\n",
      "Iteration 323 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"n\" is \"Tasks\". Therefore, the index to be returned is 1.</s>\n",
      "2256 42630\n",
      "1\n",
      "Iteration 324 of 486: 0\n",
      "Iteration 325 of 486: None\n",
      "Iteration 326 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"i\" is \"Dataset\". Therefore, the index to be returned is 1.</s>\n",
      "2291 42903\n",
      "1\n",
      "Iteration 327 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"m\" is \"Tasks\". Therefore, the index to be selected is 1.</s>\n",
      "2325 43170\n",
      "1\n",
      "Iteration 328 of 486: None\n",
      "Iteration 329 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"j\" is \"Dataset\". Therefore, the index to be returned is 1.</s>\n",
      "2360 43444\n",
      "1\n",
      "Iteration 330 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"n\" is \"Tasks\". Therefore, the index to be selected is 1.</s>\n",
      "2394 43717\n",
      "1\n",
      "Iteration 331 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"i\" is \"Index\". Therefore, the index for this annotation would be 1, as it corresponds to the \"Dataset\" entry in the JSON dictionary.</s>\n",
      "2444 43988\n",
      "1\n",
      "Iteration 332 of 486: ASSISTANT: I would select index 3 because the context seems to be related to categories or classes of objects. In this case, it's important to note that the text discusses mean square error with respect to different classes or categories of tasks.</s>\n",
      "2495 44303\n",
      "3\n",
      "Iteration 333 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"m\" is \"Tasks\". Therefore, the index to be selected is 1.</s>\n",
      "2529 44580\n",
      "1\n",
      "Iteration 334 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"j\" is \"Dataset\". Therefore, the index to be returned is 1.</s>\n",
      "2564 44860\n",
      "1\n",
      "Iteration 335 of 486: ASSISTANT: Based on the provided text, I would choose index 3 (categories) for the identifier C.</s>\n",
      "2587 45185\n",
      "3\n",
      "Iteration 336 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"n\" is \"Tasks\". Therefore, the index to be selected is 1.</s>\n",
      "2621 45472\n",
      "1\n",
      "Iteration 337 of 486: ASSISTANT: 2</s>\n",
      "2627 45793\n",
      "2\n",
      "Iteration 338 of 486: ASSISTANT: 2</s>\n",
      "2633 46125\n",
      "2\n",
      "Iteration 339 of 486: ASSISTANT: 2</s>\n",
      "2639 46477\n",
      "2\n",
      "Iteration 340 of 486: ASSISTANT: 1</s>\n",
      "2645 46798\n",
      "1\n",
      "Iteration 341 of 486: ASSISTANT: Based on the provided text and possible annotations, the most fitting description for the identifier \"m\" is \"Tasks\". Therefore, the index to be selected is 1.</s>\n",
      "2682 47116\n",
      "1\n",
      "Iteration 342 of 486: ASSISTANT: Based on the provided text and possible annotations, the most fitting description for the identifier \"n\" is \"Tasks\". Therefore, the index to be returned is 1.</s>\n",
      "2719 47436\n",
      "1\n",
      "Iteration 343 of 486: ASSISTANT: Based on the provided text and possible annotations, the most fitting description for the identifier \"j\" is \"Index\". Therefore, the index value for the annotation is 1.</s>\n",
      "2757 47708\n",
      "1\n",
      "Iteration 344 of 486: ASSISTANT: Based on the provided annotations, I will select the index for the most fitting description for the identifier \"n\" from the text as follows:\n",
      "\n",
      "index: 1\n",
      "\n",
      "The chosen index corresponds to the description \"Tasks\" which best fits the context of the identifier \"n\" in the given text.</s>\n",
      "2822 47978\n",
      "1\n",
      "Iteration 345 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"m\" is \"Tasks\". Therefore, the index to be selected is 1.</s>\n",
      "2856 48276\n",
      "1\n",
      "Iteration 346 of 486: None\n",
      "Iteration 347 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"m\" is \"Tasks\". Therefore, the index to be selected is 1.</s>\n",
      "2890 48537\n",
      "1\n",
      "Iteration 348 of 486: None\n",
      "Iteration 349 of 486: ASSISTANT: Based on the provided text and possible annotations, the most fitting description for the identifier \"m\" is \"Tasks\". Therefore, the index to be selected is 1.</s>\n",
      "2927 48793\n",
      "1\n",
      "Iteration 350 of 486: None\n",
      "Iteration 351 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"m\" is \"Tasks\". Therefore, the index to be selected is 1.</s>\n",
      "2961 49051\n",
      "1\n",
      "Iteration 352 of 486: Key does not exist in the dictionary of concepts ... 2e2e2e\n",
      "Iteration 353 of 486: None\n",
      "Iteration 354 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"C\" is \"Classes for task\". Therefore, the index is 1.</s>\n",
      "2994 49351\n",
      "1\n",
      "Iteration 355 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"m\" is \"Tasks\". Therefore, the index to be selected is 1.</s>\n",
      "3028 49629\n",
      "1\n",
      "Iteration 356 of 486: ASSISTANT: Based on the provided text and possible annotations, the most fitting description for the identifier \"m\" is \"Tasks\". Therefore, the index to be selected is 1.</s>\n",
      "3065 49914\n",
      "1\n",
      "Iteration 357 of 486: ASSISTANT: 2</s>\n",
      "3071 50270\n",
      "2\n",
      "Iteration 358 of 486: None\n",
      "Iteration 359 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"j\" is \"Dataset\". Therefore, the index is 1.</s>\n",
      "3103 50546\n",
      "1\n",
      "Iteration 360 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier <<n>> is \"Tasks\". Therefore, the index to be returned is 1.</s>\n",
      "3137 50827\n",
      "1\n",
      "Iteration 361 of 486: None\n",
      "Iteration 362 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"m\" is \"Tasks\". Therefore, the index to be selected is 1.</s>\n",
      "3171 51111\n",
      "1\n",
      "Iteration 363 of 486: None\n",
      "Iteration 364 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"Tasks\" is the index 1, which corresponds to \"Dataset\". Therefore, the index value to be returned is 1.</s>\n",
      "3215 51397\n",
      "1\n",
      "Iteration 365 of 486: ASSISTANT: Based on the provided text, I would choose index 3 (Categories) for the identifier C.</s>\n",
      "3239 51728\n",
      "3\n",
      "Iteration 366 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"m\" is \"Tasks\". Therefore, the index to be selected is 1.</s>\n",
      "3273 52020\n",
      "1\n",
      "Iteration 367 of 486: ASSISTANT: I would select index 1 because the normalized prototypical correlation matrix is a more specific and fitting description for the identifier <<ğŒ>> in the given context.</s>\n",
      "3313 52314\n",
      "1\n",
      "Iteration 368 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"i\" is \"Index\". Therefore, the index for this annotation would be 1.</s>\n",
      "3348 52583\n",
      "1\n",
      "Iteration 369 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"j\" is \"Dataset\". Therefore, the index for this annotation would be 1.</s>\n",
      "3384 52856\n",
      "1\n",
      "Iteration 370 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"m\" is \"Tasks\". Therefore, the index to be selected is 1.</s>\n",
      "3418 53135\n",
      "1\n",
      "Iteration 371 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"n\" is \"Tasks\". Therefore, the index I would select is 1.</s>\n",
      "3452 53418\n",
      "1\n",
      "Iteration 372 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"m\" is \"Tasks\". Therefore, the index to be selected is 1.</s>\n",
      "3486 53705\n",
      "1\n",
      "Iteration 373 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"n\" is \"Tasks\". Therefore, the index I would select is 1.</s>\n",
      "3520 53997\n",
      "1\n",
      "Iteration 374 of 486: None\n",
      "Iteration 375 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"i\" is \"Index\". Therefore, the index for this annotation would be 1.</s>\n",
      "3555 54297\n",
      "1\n",
      "Iteration 376 of 486: ASSISTANT: Based on the provided text, I would choose index 3 (Categories) as the most fitting description for the identifier C.</s>\n",
      "3584 54644\n",
      "3\n",
      "Iteration 377 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"m\" is \"Tasks\". Therefore, the index to select is 1.</s>\n",
      "3617 54954\n",
      "1\n",
      "Iteration 378 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"i\" is \"Index\". Therefore, the index for this annotation would be 1.</s>\n",
      "3652 55223\n",
      "1\n",
      "Iteration 379 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier <<j>> is \"Index\". Therefore, the index value for the annotation is 1.</s>\n",
      "3687 55501\n",
      "1\n",
      "Iteration 380 of 486: ASSISTANT: I cannot find a single annotation that perfectly fits the given text. However, I can provide you with the closest match based on the available annotations.\n",
      "\n",
      "annotation\\_index: 1 (Matrix)\n",
      "\n",
      "Even though the text contains normalized prototypical correlations, the matrix annotation is the closest and most relevant to the context of the code.</s>\n",
      "3763 55790\n",
      "1\n",
      "Iteration 381 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"i\" is \"Index\". Therefore, the index for this annotation would be 1.</s>\n",
      "3798 56081\n",
      "1\n",
      "Iteration 382 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"j\" from the given annotations is \"Dataset\". Therefore, the index value for this annotation is 1.</s>\n",
      "3838 56371\n",
      "1\n",
      "Iteration 383 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"m\" is \"Tasks\". Therefore, the index to be selected is 1.</s>\n",
      "3872 56664\n",
      "1\n",
      "Iteration 384 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"n\" is \"Tasks\". Therefore, the index to be returned is 1.</s>\n",
      "3906 56956\n",
      "1\n",
      "Iteration 385 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"m\" is \"Tasks\". Therefore, the index to be selected is 1.</s>\n",
      "3940 57249\n",
      "1\n",
      "Iteration 386 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"n\" is \"Tasks\". Therefore, the index to be returned is 1.</s>\n",
      "3974 57545\n",
      "1\n",
      "Iteration 387 of 486: ASSISTANT: 1</s>\n",
      "3980 57867\n",
      "1\n",
      "Iteration 388 of 486: ASSISTANT: Based on the provided text and possible annotations, the most fitting description for the identifier \"n\" is \"Tasks\". Therefore, the index to be returned is 1.</s>\n",
      "4017 58197\n",
      "1\n",
      "Iteration 389 of 486: ASSISTANT: 1</s>\n",
      "4023 58539\n",
      "1\n",
      "Iteration 390 of 486: None\n",
      "Iteration 391 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"n\" is \"Tasks\". Therefore, the index to be returned is 1.</s>\n",
      "4057 58799\n",
      "1\n",
      "Iteration 392 of 486: None\n",
      "Iteration 393 of 486: ASSISTANT: Based on the provided text and possible annotations, the most fitting description for the identifier \"m\" is \"Tasks\". Therefore, the index to be selected is 1.</s>\n",
      "4094 59059\n",
      "1\n",
      "Iteration 394 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"C\" is \"Classes for task\". Therefore, the index is 1.</s>\n",
      "4127 59364\n",
      "1\n",
      "Iteration 395 of 486: ASSISTANT: Based on the provided text and possible annotations, the most fitting description for the identifier \"n\" is the one with index 1, which corresponds to \"Tasks\".</s>\n",
      "4164 59631\n",
      "1\n",
      "Iteration 396 of 486: ASSISTANT: Based on the provided text and possible annotations, the most fitting description for the identifier \"j\" is \"Dataset\". Therefore, the index is 1.</s>\n",
      "4199 59900\n",
      "1\n",
      "Iteration 397 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"C\" is \"Classes for task\". Therefore, the index is 1.</s>\n",
      "4232 60214\n",
      "1\n",
      "Iteration 398 of 486: ASSISTANT: Based on the provided text and possible annotations, the most fitting description for the identifier \"n\" is \"Tasks\". Therefore, the index to be returned is 1.</s>\n",
      "4269 60487\n",
      "1\n",
      "Iteration 399 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier <<â„›>> is \"Correlation\". Therefore, the index is 1.</s>\n",
      "4303 60853\n",
      "1\n",
      "Iteration 400 of 486: None\n",
      "Iteration 401 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"j\" is \"Dataset\". Therefore, the index value is 1.</s>\n",
      "4336 61141\n",
      "1\n",
      "Iteration 402 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"n\" is \"Tasks\". Therefore, the index to be selected is 1.</s>\n",
      "4370 61433\n",
      "1\n",
      "Iteration 403 of 486: None\n",
      "Iteration 404 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"m\" is \"Tasks\". Therefore, the index to be selected is 1.</s>\n",
      "4404 61727\n",
      "1\n",
      "Iteration 405 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"m\" is \"Tasks\". Therefore, the index to be selected is 1.</s>\n",
      "4438 62023\n",
      "1\n",
      "Iteration 406 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"n\" is \"Tasks\". Therefore, the index to be selected is 1.</s>\n",
      "4472 62322\n",
      "1\n",
      "Iteration 407 of 486: ASSISTANT: 3</s>\n",
      "4478 62691\n",
      "3\n",
      "Iteration 408 of 486: None\n",
      "Iteration 409 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"n\" is \"Tasks\". Therefore, the index to be returned is 1.</s>\n",
      "4512 62982\n",
      "1\n",
      "Iteration 410 of 486: None\n",
      "Iteration 411 of 486: ASSISTANT: Based on the prototypical task correlation â„›(Gating diversity controller)â¢(ğ’‘n(Tasks),ğ’‘m(Tasks)),m(Tasks)â‰¤n(Tasks). (11)</s>\n",
      "4569 63273\n",
      "11\n",
      "Iteration 412 of 486: None\n",
      "Iteration 413 of 486: None\n",
      "Iteration 414 of 486: ASSISTANT: Based on the provided text and possible annotations, the most fitting description for the identifier <<n>> is \"Tasks\". Please note that this information is crucial to understand the context of the text.\n",
      "\n",
      "However, as per the API's constraints, I am obliged to only provide the index value, which in this case is:\n",
      "\n",
      "index: 1\n",
      "\n",
      "Please remember that this index corresponds to the second possible annotation, which provides the description \"Tasks\" for the identifier <<n>>.</s>\n",
      "4673 63530\n",
      "1\n",
      "Iteration 415 of 486: None\n",
      "Iteration 416 of 486: ASSISTANT: Based on the provided text and possible annotations, the most fitting description for the identifier <<m>> is \"Tasks\". Therefore, the index to be selected is 1.</s>\n",
      "4710 63791\n",
      "1\n",
      "Iteration 417 of 486: None\n",
      "Iteration 418 of 486: ASSISTANT: 3</s>\n",
      "4716 64143\n",
      "3\n",
      "Iteration 419 of 486: None\n",
      "Iteration 420 of 486: ASSISTANT: Based on the provided text and possible annotations, the most fitting description for the identifier \"n\" is \"Tasks\". Therefore, the index to be returned is 1.</s>\n",
      "4753 64415\n",
      "1\n",
      "Iteration 421 of 486: None\n",
      "Iteration 422 of 486: ASSISTANT: Based on the provided text and possible annotations, the most fitting description for the identifier \"m\" is \"Tasks\". Therefore, the index to be selected is 1.</s>\n",
      "4790 64690\n",
      "1\n",
      "Iteration 423 of 486: ASSISTANT: 2</s>\n",
      "4796 65065\n",
      "2\n",
      "Iteration 424 of 486: None\n",
      "Iteration 425 of 486: ASSISTANT: Based on the provided text and possible annotations, the most fitting description for the identifier <<m>> is \"Tasks\". Therefore, the index to be selected is 1.</s>\n",
      "4833 65335\n",
      "1\n",
      "Iteration 426 of 486: None\n",
      "Iteration 427 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"m\" is \"Tasks\". Therefore, the index to be selected is 1.</s>\n",
      "4867 65609\n",
      "1\n",
      "Iteration 428 of 486: ASSISTANT: 4</s>\n",
      "4873 65971\n",
      "4\n",
      "Iteration 429 of 486: None\n",
      "Iteration 430 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"n\" is \"Tasks\". Therefore, the index to be selected is 1.</s>\n",
      "4907 66258\n",
      "1\n",
      "Iteration 431 of 486: None\n",
      "Iteration 432 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"m\" is \"Tasks\". Therefore, the index to be selected is 1.</s>\n",
      "4941 66543\n",
      "1\n",
      "Iteration 433 of 486: ASSISTANT: Based on the provided text and possible annotations, the most fitting description for the identifier <<m>> is index 1, which corresponds to \"Tasks\".</s>\n",
      "4975 66813\n",
      "1\n",
      "Iteration 434 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"n\" is \"Tasks\". Therefore, the index to be selected is 1.</s>\n",
      "5009 67089\n",
      "1\n",
      "Iteration 435 of 486: ASSISTANT: Based on the provided text and possible annotations, the most fitting description for the identifier \"n\" is \"Tasks\". Therefore, the index to be returned is 1.</s>\n",
      "5046 67345\n",
      "1\n",
      "Iteration 436 of 486: ASSISTANT: 4</s>\n",
      "5052 67708\n",
      "4\n",
      "Iteration 437 of 486: ASSISTANT: 4</s>\n",
      "5058 68069\n",
      "4\n",
      "Iteration 438 of 486: ASSISTANT: 5</s>\n",
      "5064 68465\n",
      "5\n",
      "Iteration 439 of 486: 0\n",
      "Iteration 440 of 486: 0\n",
      "Iteration 441 of 486: ASSISTANT: 3</s>\n",
      "5070 68802\n",
      "3\n",
      "Iteration 442 of 486: 0\n",
      "Iteration 443 of 486: 0\n",
      "Iteration 444 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"i\" is \"Dataset\". Therefore, the index to be returned is 1.</s>\n",
      "5105 69105\n",
      "1\n",
      "Iteration 445 of 486: ASSISTANT: 2</s>\n",
      "5111 69469\n",
      "2\n",
      "Iteration 446 of 486: None\n",
      "Iteration 447 of 486: None\n",
      "Iteration 448 of 486: ASSISTANT: 2</s>\n",
      "5117 69798\n",
      "2\n",
      "Iteration 449 of 486: None\n",
      "Iteration 450 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"i\" is \"Dataset\". Therefore, the index value for this annotation is 1.</s>\n",
      "5153 70078\n",
      "1\n",
      "Iteration 451 of 486: ASSISTANT: 5</s>\n",
      "5159 70482\n",
      "5\n",
      "Iteration 452 of 486: 0\n",
      "Iteration 453 of 486: 0\n",
      "Iteration 454 of 486: ASSISTANT: 1</s>\n",
      "5165 70839\n",
      "1\n",
      "Iteration 455 of 486: ASSISTANT: 14</s>\n",
      "5172 71359\n",
      "14\n",
      "Iteration 456 of 486: ASSISTANT: Based on the provided text and possible annotations, the most fitting description for the identifier \"i\" is \"Index\". Therefore, the index value is 1.</s>\n",
      "5207 71620\n",
      "1\n",
      "Iteration 457 of 486: ASSISTANT: 14</s>\n",
      "5214 72147\n",
      "14\n",
      "Iteration 458 of 486: ASSISTANT: 1</s>\n",
      "5220 72413\n",
      "1\n",
      "Iteration 459 of 486: ASSISTANT: 1</s>\n",
      "5226 72682\n",
      "1\n",
      "Iteration 460 of 486: ASSISTANT: Based on the provided text and possible annotations, the most fitting description for the identifier \"j\" is \"Dataset\". Therefore, the index is 1.</s>\n",
      "5261 72946\n",
      "1\n",
      "Iteration 461 of 486: ASSISTANT: 2</s>\n",
      "5267 73252\n",
      "2\n",
      "Iteration 462 of 486: ASSISTANT: 2</s>\n",
      "5273 73570\n",
      "2\n",
      "Iteration 463 of 486: ASSISTANT: 2</s>\n",
      "5279 73865\n",
      "2\n",
      "Iteration 464 of 486: ASSISTANT: 4</s>\n",
      "5285 74173\n",
      "4\n",
      "Iteration 465 of 486: ASSISTANT: 5</s>\n",
      "5291 74555\n",
      "5\n",
      "Iteration 466 of 486: 0\n",
      "Iteration 467 of 486: ASSISTANT: 3</s>\n",
      "5297 74983\n",
      "3\n",
      "Iteration 468 of 486: 0\n",
      "Iteration 469 of 486: ASSISTANT: Based on the provided text, I would choose index 4 for the identifier <<â„’>> as it corresponds to the context of \"Sets\".</s>\n",
      "5332 75408\n",
      "4\n",
      "Iteration 470 of 486: 0\n",
      "Iteration 471 of 486: ASSISTANT: 3</s>\n",
      "5338 75793\n",
      "3\n",
      "Iteration 472 of 486: 0\n",
      "Iteration 473 of 486: ASSISTANT: 3</s>\n",
      "5344 76222\n",
      "3\n",
      "Iteration 474 of 486: 0\n",
      "Iteration 475 of 486: ASSISTANT: 3</s>\n",
      "5350 76648\n",
      "3\n",
      "Iteration 476 of 486: 0\n",
      "Iteration 477 of 486: ASSISTANT: 3</s>\n",
      "5356 77037\n",
      "3\n",
      "Iteration 478 of 486: 0\n",
      "Iteration 479 of 486: ASSISTANT: 3</s>\n",
      "5362 77418\n",
      "3\n",
      "Iteration 480 of 486: 0\n",
      "Iteration 481 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier <<ğŒ>> is \"Normalized prototypical correlation matrix\". Therefore, the index to be returned is 1.</s>\n",
      "5406 77693\n",
      "1\n",
      "Iteration 482 of 486: ASSISTANT: 1</s>\n",
      "5412 77985\n",
      "1\n",
      "Iteration 483 of 486: ASSISTANT: Based on the provided text, the most fitting description for the identifier <<KITTI>> is index 1 (KITTI, Set).</s>\n",
      "5445 78279\n",
      "1\n",
      "Iteration 484 of 486: ASSISTANT: 1</s>\n",
      "5451 78556\n",
      "1\n",
      "Iteration 485 of 486: ASSISTANT: 2</s>\n",
      "5457 78849\n",
      "2\n",
      "Iteration 486 of 486: ASSISTANT: 2</s>\n",
      "5463 79149\n",
      "2\n",
      "Annotation completed\n",
      "Time taken: 0:08:36.643085\n",
      "Total time taken: 0:08:36.643044\n",
      "84612 5463 79149\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.now()\n",
    "actual_total_tokens = 0\n",
    "completion_tokens = 0\n",
    "prompt_tokens = 0\n",
    "no_tags = 0\n",
    "no_keys = 0\n",
    "no_anno = 0\n",
    "i = 1\n",
    "for match in matches:\n",
    "    print(f\"Iteration {i} of {len(matches)}: \", end='')\n",
    "    i += 1\n",
    "    context = get_context(match)\n",
    "    match_variable = re.sub(r'<.*?>(.*?)</.*?>', r'\\1', match)\n",
    "    context_index = context.index(f\"<<{match_variable}>>\") + len(match_variable)\n",
    "    possible_affix = str(context[context_index+4:context_index+5]).replace(\"â€²\", \"'\")\n",
    "    soup = BeautifulSoup(match, 'html.parser')\n",
    "    mi_tag = soup.find('mi')\n",
    "    if mi_tag is not None and 'id' in mi_tag.attrs:\n",
    "        anno_id = mi_tag['id']\n",
    "    else:\n",
    "        print('TAG NOT FOUND', match)\n",
    "        no_tags += 1\n",
    "        continue\n",
    "    \n",
    "    hex_code = get_hex_code(match_variable)\n",
    "    if hex_code not in parsed_dict['concepts']:\n",
    "        match_variable = f\"{unidecode(match_variable)}\"\n",
    "        hex_code = get_hex_code(match_variable)\n",
    "        if hex_code not in parsed_dict['concepts']:\n",
    "            print(\"Key does not exist in the dictionary of concepts\", match_variable, hex_code)\n",
    "            no_keys += 1\n",
    "            continue\n",
    "    \n",
    "    if anno_id not in parsed_annotation['mi_anno']:\n",
    "        print(\"Annotation ID does not exist in annotation.json\", anno_id)\n",
    "        no_anno += 1\n",
    "        continue\n",
    "\n",
    "    k = list(parsed_dict[\"concepts\"][hex_code][\"identifiers\"].keys())[0]\n",
    "    mcdict = parsed_dict['concepts'][hex_code]['identifiers'][k]\n",
    "    \n",
    "    if len(mcdict) == 1:\n",
    "        parsed_annotation['mi_anno'][anno_id]['concept_id'] = 0\n",
    "        print('0')\n",
    "    elif len(mcdict) > 1:\n",
    "        prompt_mcdict = []\n",
    "\n",
    "        index = 0\n",
    "        for val in mcdict:\n",
    "            prompt_mcdict.append({'index': f\"{index}\", 'identifier': f\"{match_variable}{'' if len(val['affixes']) == 0 else val['affixes'][0]}\", 'description': val['description']})\n",
    "            index += 1\n",
    "            \n",
    "        prompt = [\n",
    "            {'role': 'system', 'content': 'You are a professional annotater API. Your job is to select a fitting annotation from a dictionary for a mathematical identifier.'},\n",
    "            {'role': 'user', 'content': f'''Given the following possible annotations:\\n```json\\n{prompt_mcdict}```.\n",
    "             Select the index for the most fitting description for the identifier <<{match_variable}>> from the following text.\n",
    "             The potential affix of the indentifier could be <<{possible_affix}>>. Take the affixes of the possible annotations into account.\n",
    "             Only return the value of the index and nothing else.\n",
    "             Do not add any explanation otherwise the API breaks.\n",
    "             The identifier has been marked with <<>>.\n",
    "             If you can't come up with an index, write 'None'\n",
    "             ```txt\n",
    "             {context}\n",
    "             ```'''}\n",
    "        ]\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                open_prompt = get_prompt_anno(prompt)\n",
    "                \n",
    "                input_ids = tokenizer(open_prompt, return_tensors='pt').input_ids.cuda()\n",
    "                output = model.generate(inputs=input_ids, temperature=0.5, max_new_tokens=512, repetition_penalty=1.05)\n",
    "                output_string = tokenizer.decode(output[0])\n",
    "                \n",
    "                actual_total_tokens += num_tokens_from_messages(output_string)\n",
    "                completion_tokens += num_tokens_from_messages(output_string) - num_tokens_from_messages(open_prompt)\n",
    "                prompt_tokens += num_tokens_from_messages(open_prompt)\n",
    "\n",
    "                ind = output_string.index('ASSISTANT:')\n",
    "                value = output_string[ind:]\n",
    "                print(value)\n",
    "                \n",
    "                print(completion_tokens, prompt_tokens)\n",
    "\n",
    "                try:\n",
    "                    index = int(int(re.search('\\d+', value).group()))\n",
    "                    print(index)\n",
    "                    parsed_annotation['mi_anno'][anno_id]['concept_id'] = index\n",
    "                except Exception as f:\n",
    "                    print(f)\n",
    "\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Exception occurred\\n{e}\")\n",
    "                print(\"Retrying...\")\n",
    "    else:\n",
    "        print('None')\n",
    "\n",
    "print('Annotation completed')\n",
    "\n",
    "    \n",
    "total_time_taken = (datetime.now() - start_time)\n",
    "print(f\"Time taken: {datetime.now() - start_time }\")\n",
    "print(f\"Total time taken: {total_time_taken}\")\n",
    "print(actual_total_tokens, completion_tokens, prompt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bfeab45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_annotation['_annotator'] = model_name_or_path\n",
    "with open(f'{file_code}-{model_name}_anno.json', 'w') as fp:\n",
    "    json.dump(parsed_annotation, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9180c363",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "377\n"
     ]
    }
   ],
   "source": [
    "items = 0\n",
    "for key, value in parsed_annotation['mi_anno'].items():\n",
    "    if value['concept_id'] is not None:\n",
    "        #print(key, value)\n",
    "        items += 1\n",
    "print(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffe4c27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
