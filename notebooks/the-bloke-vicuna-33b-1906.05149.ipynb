{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63e50065-dd79-48b6-9bc3-87eb6522a15f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unidecode\n",
      "  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.9/235.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tiktoken\n",
      "  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting transformers\n",
      "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting einops\n",
      "  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting auto-gptq\n",
      "  Downloading auto_gptq-0.3.2.tar.gz (63 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.5/63.5 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hDiscarding \u001b[4;34mhttps://files.pythonhosted.org/packages/1b/79/5a3a7d877a9b0a72f528e9977ec65cdb9fad800fa4f5110f87f2acaaf6fe/auto_gptq-0.3.2.tar.gz (from https://pypi.org/simple/auto-gptq/) (requires-python:>=3.8.0)\u001b[0m: \u001b[33mRequested auto-gptq from https://files.pythonhosted.org/packages/1b/79/5a3a7d877a9b0a72f528e9977ec65cdb9fad800fa4f5110f87f2acaaf6fe/auto_gptq-0.3.2.tar.gz has inconsistent version: expected '0.3.2', but metadata has '0.3.2+cu118'\u001b[0m\n",
      "  Downloading auto_gptq-0.3.1.tar.gz (63 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.5/63.5 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hDiscarding \u001b[4;34mhttps://files.pythonhosted.org/packages/3f/5c/28d57f833498e014e9d066ea0199a503b5e59d4c8e8e701a460223e143da/auto_gptq-0.3.1.tar.gz (from https://pypi.org/simple/auto-gptq/) (requires-python:>=3.8.0)\u001b[0m: \u001b[33mRequested auto-gptq from https://files.pythonhosted.org/packages/3f/5c/28d57f833498e014e9d066ea0199a503b5e59d4c8e8e701a460223e143da/auto_gptq-0.3.1.tar.gz has inconsistent version: expected '0.3.1', but metadata has '0.3.1+cu1180'\u001b[0m\n",
      "  Downloading auto_gptq-0.3.0.tar.gz (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting regex>=2022.1.18 (from tiktoken)\n",
      "  Downloading regex-2023.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (770 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m770.4/770.4 kB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.28.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.9.0)\n",
      "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
      "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
      "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m126.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tqdm>=4.27 (from transformers)\n",
      "  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting accelerate>=0.19.0 (from auto-gptq)\n",
      "  Downloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting datasets (from auto-gptq)\n",
      "  Downloading datasets-2.14.3-py3-none-any.whl (519 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.1/519.1 kB\u001b[0m \u001b[31m81.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting rouge (from auto-gptq)\n",
      "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (2.0.1+cu118)\n",
      "Collecting peft (from auto-gptq)\n",
      "  Downloading peft-0.4.0-py3-none-any.whl (72 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.19.0->auto-gptq) (5.9.5)\n",
      "Collecting fsspec (from huggingface-hub<1.0,>=0.14.1->transformers)\n",
      "  Downloading fsspec-2023.6.0-py3-none-any.whl (163 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2022.12.7)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (1.11.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (2.0.0)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.13.0->auto-gptq) (3.25.0)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.13.0->auto-gptq) (15.0.7)\n",
      "Collecting pyarrow>=8.0.0 (from datasets->auto-gptq)\n",
      "  Downloading pyarrow-12.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.9/38.9 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting dill<0.3.8,>=0.3.0 (from datasets->auto-gptq)\n",
      "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pandas (from datasets->auto-gptq)\n",
      "  Downloading pandas-2.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m123.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting xxhash (from datasets->auto-gptq)\n",
      "  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multiprocess (from datasets->auto-gptq)\n",
      "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiohttp (from datasets->auto-gptq)\n",
      "  Downloading aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m98.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six in /usr/lib/python3/dist-packages (from rouge->auto-gptq) (1.16.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (23.1.0)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets->auto-gptq)\n",
      "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets->auto-gptq)\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets->auto-gptq)\n",
      "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->datasets->auto-gptq)\n",
      "  Downloading frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (225 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.7/225.7 kB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->datasets->auto-gptq)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->auto-gptq) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->auto-gptq) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets->auto-gptq)\n",
      "  Downloading pytz-2023.3-py2.py3-none-any.whl (502 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.3/502.3 kB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tzdata>=2022.1 (from pandas->datasets->auto-gptq)\n",
      "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->auto-gptq) (1.2.1)\n",
      "Building wheels for collected packages: auto-gptq\n",
      "  Building wheel for auto-gptq (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for auto-gptq: filename=auto_gptq-0.3.0-cp310-cp310-linux_x86_64.whl size=5664438 sha256=4e7e987e449022a25a112145c129d74293fa0b9cbf5edac908016835d4ec60e5\n",
      "  Stored in directory: /root/.cache/pip/wheels/62/ca/da/632464db8c2071b514d3b659652383c789f53cc2fc917737bb\n",
      "Successfully built auto-gptq\n",
      "Installing collected packages: tokenizers, safetensors, pytz, xxhash, unidecode, tzdata, tqdm, rouge, regex, pyarrow, multidict, fsspec, frozenlist, einops, dill, async-timeout, yarl, tiktoken, pandas, multiprocess, huggingface-hub, aiosignal, transformers, aiohttp, datasets, accelerate, peft, auto-gptq\n",
      "Successfully installed accelerate-0.21.0 aiohttp-3.8.5 aiosignal-1.3.1 async-timeout-4.0.2 auto-gptq-0.3.0 datasets-2.14.3 dill-0.3.7 einops-0.6.1 frozenlist-1.4.0 fsspec-2023.6.0 huggingface-hub-0.16.4 multidict-6.0.4 multiprocess-0.70.15 pandas-2.0.3 peft-0.4.0 pyarrow-12.0.1 pytz-2023.3 regex-2023.6.3 rouge-1.0.1 safetensors-0.3.1 tiktoken-0.4.0 tokenizers-0.13.3 tqdm-4.65.0 transformers-4.31.0 tzdata-2023.3 unidecode-1.3.6 xxhash-3.3.0 yarl-1.9.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install unidecode tiktoken transformers einops auto-gptq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b5a187-7c86-4ea4-863a-55817112cfb5",
   "metadata": {},
   "source": [
    "## Time to install packages 4 mins 0 seconds\n",
    "## Time started: 23:59:00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4255ace1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, NavigableString\n",
    "from unidecode import unidecode\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime\n",
    "import unicodedata\n",
    "import itertools\n",
    "import tiktoken\n",
    "import ast\n",
    "import pprint\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af9777c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The safetensors archive passed at /root/.cache/huggingface/hub/models--TheBloke--Vicuna-33B-1-3-SuperHOT-8K-GPTQ/snapshots/6bd46c2caeac69d4329b48f66bbd05f8546998a8/vicuna-33b-1.3-superhot-8k-GPTQ-4bit--1g.act.order.safetensors does not contain metadata. Make sure to save your model with the `save_pretrained` method. Defaulting to 'pt' metadata.\n",
      "skip module injection for FusedLlamaMLPForQuantizedModel not support integrate without triton yet.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, pipeline, logging\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "import argparse\n",
    "\n",
    "model_name_or_path = \"TheBloke/Vicuna-33B-1-3-SuperHOT-8K-GPTQ\"\n",
    "model_basename = \"vicuna-33b-1.3-superhot-8k-GPTQ-4bit--1g.act.order\"\n",
    "model_name = 'vicuna-33b'\n",
    "file_code = '1906.05149'\n",
    "\n",
    "use_triton = False\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "\n",
    "model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n",
    "        model_basename=model_basename,\n",
    "        use_safetensors=True,\n",
    "        trust_remote_code=True,\n",
    "        device_map='auto',\n",
    "        use_triton=use_triton,\n",
    "        quantize_config=None)\n",
    "\n",
    "model.seqlen = 8192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1f3d52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_messages(prompt_template, model=\"gpt-3.5-turbo\"):\n",
    "    return len(tokenizer(prompt_template, return_tensors='pt').input_ids.cuda().tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "604d244c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_messages(text):\n",
    "    # This is a placeholder for your actual implementation\n",
    "    return text.split(\"\\n\")\n",
    "\n",
    "def split_into_chunks(text, max_tokens=2000):\n",
    "    messages = split_into_messages(text)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_tokens = 0\n",
    "    for message in messages:\n",
    "        message_tokens = num_tokens_from_messages(message, model=\"gpt-3.5-turbo\")\n",
    "        if current_tokens + message_tokens > max_tokens:\n",
    "            # If adding this message would exceed the max tokens, start a new chunk\n",
    "            chunks.append('\\n'.join(current_chunk))\n",
    "            current_chunk = [message]\n",
    "            current_tokens = message_tokens\n",
    "        else:\n",
    "            # Otherwise, add the message to the current chunk\n",
    "            current_chunk.append(message)\n",
    "            current_tokens += message_tokens\n",
    "    # Don't forget the last chunk!\n",
    "    if current_chunk:\n",
    "        chunks.append('\\n'.join(current_chunk))\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e285df1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_string(input_str):\n",
    "    if \"</s>\" in input_str:\n",
    "        return input_str\n",
    "    else:\n",
    "        last_comma_index = input_str.rfind(',')\n",
    "        if last_comma_index == -1:\n",
    "            return input_str  # No comma found, return the string as is\n",
    "        else:\n",
    "            return input_str[:last_comma_index] + '}' + input_str[last_comma_index+1:]\n",
    "        \n",
    "def get_json_of_string(incorrect, pattern=r'\\{[^\\}]*\\}'):\n",
    "    match = re.search(r'{(.*)}', incorrect, re.DOTALL)\n",
    "    if match:\n",
    "        return \"{\" + match.group(1).replace('{', '[').replace('}', ']') + \"}\"\n",
    "    else:\n",
    "        return \"{}\"\n",
    "\n",
    "\n",
    "def string_to_dict(my_string):\n",
    "    # Load the JSON string into a list of tuples\n",
    "    tuples_list = json.JSONDecoder(object_pairs_hook=list).decode(my_string)\n",
    "\n",
    "    # Create a new dictionary to hold the final result\n",
    "    final_dict = {}\n",
    "\n",
    "    # Iterate over the list of tuples\n",
    "    for key, value in tuples_list:\n",
    "        # If the key is already in the final dictionary, append the value\n",
    "        # to the list of values for that key\n",
    "        if key in final_dict:\n",
    "            # Ensure the value is in a list form\n",
    "            if not isinstance(final_dict[key], list):\n",
    "                final_dict[key] = [final_dict[key]]\n",
    "            final_dict[key].append(value)\n",
    "        else:\n",
    "            # If the key is not in the final dictionary, add it with the value\n",
    "            final_dict[key] = value\n",
    "\n",
    "    return final_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53c1858e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dictionaries(dict1, dict2):\n",
    "    union_dict = dict1.copy()\n",
    "\n",
    "    for key, value in dict2.items():\n",
    "        if key in union_dict:\n",
    "            if isinstance(union_dict[key], list):\n",
    "                if value not in union_dict[key]:\n",
    "                    union_dict[key].append(value)\n",
    "            else:\n",
    "                if union_dict[key] != value:\n",
    "                    union_dict[key] = [union_dict[key], value]\n",
    "        else:\n",
    "            union_dict[key] = value\n",
    "\n",
    "    return union_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff36d705",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_from_tags(element):\n",
    "    if isinstance(element, NavigableString):\n",
    "        return element\n",
    "    if element.name == 'mi':\n",
    "        return str(element)\n",
    "    return ''.join(get_text_from_tags(child) for child in element.children)\n",
    "\n",
    "def parse_html(file_path, clean=True):\n",
    "    with open(file_path, 'r', encoding='utf-8') as html_file:\n",
    "        soup = BeautifulSoup(html_file, 'html.parser')\n",
    "\n",
    "    texts = get_text_from_tags(soup)\n",
    "    if clean:\n",
    "        matches = re.findall(r'<mi(.*?)</mi>', texts)\n",
    "        for match in matches:\n",
    "            original_string = f'<mi{match}</mi>'\n",
    "            replaced_string = re.sub(r'<.*?>(.*?)</.*?>', r'<|\\1|>', original_string)\n",
    "            texts = texts.replace(original_string, replaced_string)\n",
    "\n",
    "    return texts\n",
    "\n",
    "# call the function with your HTML file path\n",
    "page = parse_html(f'{file_code}.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb9f95a7-690d-49a1-947e-2d1c107a5f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(prompt):\n",
    "    return f'''SYSTEM: {prompt[0]['content']}\n",
    "USER: {prompt[1]['content']}\n",
    "ASSISTANT: {prompt[2]['content']}\n",
    "USER: {prompt[3]['content']}\n",
    "ASSISTANT: '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d0568d3-d765-4c53-ac27-c67650dad1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_pattern(input_string):\n",
    "    pattern = r\"<\\|[^<\\|>]*\\|>\"\n",
    "    if re.search(pattern, input_string):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f0f85cb-e25a-4a21-81a4-c8c143a47c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_output(output):\n",
    "    pos = output.index('Do not include the angle brackets in the dictionary')\n",
    "    end_of_string = output[pos:]\n",
    "    print(end_of_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abf7ee5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "1253 prompt tokens counted.\n"
     ]
    }
   ],
   "source": [
    "text = page\n",
    "chunks = split_into_chunks(text, max_tokens=512)\n",
    "i = 0\n",
    "\n",
    "while not contains_pattern(chunks[i]):\n",
    "    i += 1\n",
    "print(i)\n",
    "\n",
    "question = chunks[i]\n",
    "\n",
    "actual_total_tokens = 0\n",
    "completion_tokens = 0\n",
    "prompt_tokens = 0\n",
    "\n",
    "prompt = [\n",
    "        {'role': 'system',\n",
    "         'content': 'You are a helpful research assistant tasked with converting long paragraphs into a JSON '\n",
    "                    'dictionary. The goal is to identify and classify each individual mathematical symbol, variable,'\n",
    "                    ' and identifier in the text marked between \"<||>\". The dictionary should store the identifiers as '\n",
    "                    'keys and their corresponding definitions as values in an array format. '},\n",
    "        {'role': 'system', 'name': 'example_user', 'content': '''A relational model is a triple <|M|>′=(<|X|>,<|R|>,\n",
    "        <|v|>), where <|X|> is a set of states, <|R|><|⊆|><|X|><|×|><|X|> is a binary relation on <|X|>, \n",
    "        and <|v|>:<|𝖯𝗋𝗈𝗉|>→2<|X|> is a valuation. Given a relational model <|M|>′, the satisfaction relation between \n",
    "        points <|x<|∈<|X<| and formulas <|φ<|∈<|ℒ<|<|𝖪𝖠<| is defined inductively by <|M|>′,<|x|>⊨<|𝖪|><|φ|>⇔ for all \n",
    "        <|y|>∈<|X|>,<|x|><|R|><|y|> implies <|M|>′,<|y|>⊨<|φ|><|M|>′,<|x|>⊨<|𝖠|><|φ|><||>⇔ for all <|y|>∈<|X|>,\n",
    "        <|M|>′,<|y|>⊨<|φ|>'''},\n",
    "        {'role': 'system', 'name': 'example_assistant', 'content': '''identifiers = {\n",
    "            \"M\": [\"Model\", \"Expertise Model\"],\n",
    "            \"M'\": \"Relational model\",\n",
    "            \"X\": \"Set of states\",\n",
    "            \"R\": \"Binary relation on X\",\n",
    "            \"v\": \"Valuation\",\n",
    "            \"𝖯𝗋𝗈𝗉\": \"Set of propositions\",\n",
    "            \"M'\": \"Relational model\",\n",
    "            \"x\": \"Point in X\",\n",
    "            \"φ\": \"Formula in 𝖪𝖠\",\n",
    "            \"ℒ_{𝖪𝖠}\": \"Set of formulas\",\n",
    "            \"𝖪\": \"Modal operator K\",\n",
    "            \"𝖠\": \"Modal operator A\",\n",
    "            \"y\": \"Point in X\",\n",
    "            \"⊨\": \"Satisfaction relation\",\n",
    "            \"⇔\": \"If and only if operator\",\n",
    "            \"∈\": \"Element of a set\",\n",
    "            \"⊆\": \"Subset of a set\",\n",
    "            \"×\": \"Cartesian product operator\",\n",
    "            \"→\": \"Function or implication operator\",\n",
    "            \"for all\": \"Universal quantifier\"\n",
    "            }'''},\n",
    "        {'role': 'user', 'content': f'Generate a JSON dictionary for the following text\\n```txt\\n{question}```. '\n",
    "                                    'Only consider the mathematical identifiers inside \"<||>\" for the dictionary. '\n",
    "                                    'Do not consider any other identifier other than those marked. Consider all the '\n",
    "                                    'identifiers individually. Do not skip any identifier, mention all the identifiers '\n",
    "                                    'inside \"<||>\" in your dictionary. Do not include the angle brackets in the '\n",
    "                                    'dictionary.'}\n",
    "    ]\n",
    "\n",
    "\n",
    "open_prompt = get_prompt(prompt)\n",
    "\n",
    "prompt_size = num_tokens_from_messages(open_prompt)\n",
    "print(f\"{prompt_size} prompt tokens counted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "612640fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TheBloke/Vicuna-33B-1-3-SuperHOT-8K-GPTQ\n",
      "Do not include the angle brackets in the dictionary.\n",
      "ASSISTANT:  identifiers = {\n",
      "\"LM\": [\"Language Model\", \"Bidirectional LSTM\"],\n",
      "\"t\": \"Timestep\",\n",
      "\"𝐰\": \"Word embedding\",\n",
      "\"𝐡\": \"Hidden state\",\n",
      "\"𝐺\": \"Hidden states\",\n",
      "\"𝐺\": \"Hidden states\",\n",
      "\"LSTM\": \"Long Short-Term Memory\",\n",
      "\"𝟐\": \"Input for LSTM\",\n",
      "\"𝟑\": \"Output for LSTM\",\n",
      "\"𝟒\": \"Forward LSTM\",\n",
      "\"𝟓\": \"Backward LSTM\",\n",
      "\"𝟕\": \"Linear transformation\",\n",
      "\"𝟖\": \"Softmax activation\",\n",
      "\"𝟗\": \"Word prediction\",\n",
      "\"𝟘\": \"Number of hidden layers\",\n",
      "\"𝟙\": \"Word at timestep t\",\n",
      "\"𝟚\": \"Activation of hidden state\",\n",
      "\"𝟛\": \"Output weight\",\n",
      "\"𝟜\": \"Left context\",\n",
      "\"𝟝\": \"Right context\",\n",
      "\"𝟞\": \"Context2vec architecture\",\n",
      "\"𝟣\": \"BiLSTM architecture\",\n",
      "\"𝟤\": \"ELMo\",\n",
      "\"𝟥\": \"Predicting a word\",\n",
      "\"𝟦\": \"Receiving a word\",\n",
      "\"𝟧\": \"Predicting both left and right context\",\n",
      "\"𝟨\": \"Predicting one context\",\n",
      "\"𝟩\": \"Summing activations\",\n",
      "\"𝟪\": \"Applying softmax\"\n",
      "}</s>\n",
      "Time taken: 0:00:23.827584\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using {model_name_or_path}\")\n",
    "start_time = datetime.now()\n",
    "\n",
    "input_ids = tokenizer(open_prompt, return_tensors='pt').input_ids.cuda()\n",
    "output = model.generate(inputs=input_ids, temperature=0.5, max_new_tokens=512, repetition_penalty=1.05)\n",
    "output_string = tokenizer.decode(output[0])\n",
    "print_output(output_string)\n",
    "\n",
    "total_time_taken = datetime.now() - start_time\n",
    "print(f\"Time taken: {total_time_taken}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "507e7993",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1633 380 1253\n"
     ]
    }
   ],
   "source": [
    "actual_total_tokens += num_tokens_from_messages(output_string)\n",
    "completion_tokens += num_tokens_from_messages(output_string) - num_tokens_from_messages(open_prompt)\n",
    "prompt_tokens += num_tokens_from_messages(open_prompt)\n",
    "print(actual_total_tokens, completion_tokens, prompt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ff56fd8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Safely convert the dictionary string to a dictionary using json.loads()\n",
    "try:\n",
    "    ind = output_string.index('Do not include the angle brackets in the dictionary.')\n",
    "    correct_output_string = modify_string(output_string[ind:])\n",
    "    dic_output_string = get_json_of_string(correct_output_string)\n",
    "    dictionary = [string_to_dict(dic_output_string)]\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    dictionary = [{}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "acbf76ad-2acb-41dc-be3d-aab9c19c15c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'LM': ['Language Model', 'Bidirectional LSTM'], 't': 'Timestep', '𝐰': 'Word embedding', '𝐡': 'Hidden state', '𝐺': ['Hidden states', 'Hidden states'], 'LSTM': 'Long Short-Term Memory', '𝟐': 'Input for LSTM', '𝟑': 'Output for LSTM', '𝟒': 'Forward LSTM', '𝟓': 'Backward LSTM', '𝟕': 'Linear transformation', '𝟖': 'Softmax activation', '𝟗': 'Word prediction', '𝟘': 'Number of hidden layers', '𝟙': 'Word at timestep t', '𝟚': 'Activation of hidden state', '𝟛': 'Output weight', '𝟜': 'Left context', '𝟝': 'Right context', '𝟞': 'Context2vec architecture', '𝟣': 'BiLSTM architecture', '𝟤': 'ELMo', '𝟥': 'Predicting a word', '𝟦': 'Receiving a word', '𝟧': 'Predicting both left and right context', '𝟨': 'Predicting one context', '𝟩': 'Summing activations', '𝟪': 'Applying softmax'}]\n"
     ]
    }
   ],
   "source": [
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d18ab90-38c2-4724-9a03-239fc73078d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_loop(prompt):\n",
    "    return f'''SYSTEM: {prompt[0]['content']}\n",
    "USER: {prompt[1]['content']}\n",
    "ASSISTANT: {prompt[2]['content']}\n",
    "SYSTEM: {prompt[3]['content']}\n",
    "USER: {prompt[4]['content']}\n",
    "ASSISTANT: '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5bab7543",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3 of 19\n",
      "Iteration 4 of 19\n",
      "Iteration 5 of 19\n",
      "Iteration 6 of 19\n",
      "Iteration 7 of 19\n",
      "\n",
      "\n",
      "\n",
      "1640 prompt tokens counted.\n",
      "\n",
      "Do not include the angle brackets in your dictionary.\n",
      "ASSISTANT:  mathematicsIdentifiers = {\n",
      "\"softmax\": [\"Softmax\", \"Activation function\"],\n",
      "\"f\": [\"Function\", \"Calculating activations\"],\n",
      "\"t\": [\"Timestep\", \"Time step\"],\n",
      "\"𝐡\": [\"Word embedding\", \"Embedding\"],\n",
      "\"L\": [\"Size\", \"Dimension\"],\n",
      "\"i\": [\"Index\", \"Indice\"],\n",
      "\"±\": [\"Plus or minus\", \"Add or subtract\"],\n",
      "\"∈\": [\"Element of a set\", \"In a set\"],\n",
      "\"≤\": [\"Less than or equal to\"],\n",
      "\"≥\": [\"Greater than or equal to\"],\n",
      "\"[]\": [\"Array indexing\", \"Accessing elements\"],\n",
      "\"[]\": [\"Array slicing\", \"Creating arrays\"],\n",
      "\"[]\": [\"Cross product\", \"Cartesian product\"],\n",
      "\"->\" [\"Implies\", \"If and only if\"]\n",
      "}</s>\n",
      "578 2893\n",
      "Actual total tokens till now: 3471\n",
      "INCORRECT DICTIONARY\n",
      "Iteration 8 of 19\n",
      "\n",
      "New dictionary\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1243 prompt tokens counted.\n",
      "\n",
      "Do not include the angle brackets in your dictionary.\n",
      "ASSISTANT: 📃 linguistic_model = {\n",
      "\"t\": \"Timestep\",\n",
      "\"h\": \"Hidden states\",\n",
      "\"fwd\": \"Forward hidden states\",\n",
      "\"bwd\": \"Backward hidden states\",\n",
      "\"c\": \"Current hidden states\",\n",
      "\"p\": \"Predictive hidden states\",\n",
      "\"a\": \"Activations\",\n",
      "\"l\": \"Layers\",\n",
      "\"v\": \"Vector representations\",\n",
      "\"w\": \"Word embeddings\",\n",
      "\"λ\": \"Lexical substituion data\",\n",
      "\"mccarthy_navigli\": \"Lexical substitution data\"\n",
      "}</s>\n",
      "718 4136\n",
      "Actual total tokens till now: 4854\n",
      "Iteration 9 of 19\n",
      "\n",
      "\n",
      "\n",
      "1407 prompt tokens counted.\n",
      "\n",
      "Do not include the angle brackets in your dictionary.\n",
      "ASSISTANT: \n",
      "identifiers = {\n",
      "\"LexSub\": \"Large dataset by Kremer et al., 2014\",\n",
      "\"Ex. (1)\": \"Example sentence 1\",\n",
      "\"Ex. (2)\": \"Example sentence 2\",\n",
      "\"h\": \"Human subjects\",\n",
      "\"a\": \"Annotated words\",\n",
      "\"t\": \"Target word\",\n",
      "\"s\": \"Substitute words\",\n",
      "\"v\": \"Vector representations\",\n",
      "\"w\": \"Word embeddings\",\n",
      "\"λ\": \"Lexical substitution data\",\n",
      "\"mccarthy_navigli\": \"Lexical substitution data\",\n",
      "\"Embedding\": \"Representation of words\",\n",
      "\"<>\": \"Average embedding\",\n",
      "\"<>\": \"Union of substitute words and target word\",\n",
      "\"<>\": \"Average embedding of substitute words\",\n",
      "\"<>\": \"Cosine similarity\",\n",
      "\"<>\": \"Top-10 neighbors\",\n",
      "\"<>\": \"Training data\",\n",
      "\"<>\": \"Words in context\",\n",
      "\"<>\": \"Paraphrases\",\n",
      "\"<>\": \"Evaluation benchmark\",\n",
      "\"<>\": \"Simple vector operations\"\n",
      "}</s>\n",
      "978 5543\n",
      "Actual total tokens till now: 6521\n",
      "Iteration 10 of 19\n",
      "\n",
      "\n",
      "\n",
      "1535 prompt tokens counted.\n",
      "\n",
      "Do not include the angle brackets in your dictionary.\n",
      "ASSISTANT: math_identifiers = {\n",
      "\"𝐰\": [\"Representation\", \"Retrieve word representations\"],\n",
      "\"𝐢\": [\"Inputs\", \"Hidden states\"],\n",
      "\"𝐬\": [\"Substitute words\"],\n",
      "\"𝐢\": [\"Inputs\", \"Predict\"],\n",
      "\"𝐫\": [\"Outputs\", \"Predict\"],\n",
      "\"𝐮\": [\"Supervised probe tasks\"],\n",
      "\"𝐁\": [\"Diagnostic model\", \"Learn to retrieve\"],\n",
      "𝐌: [\"Rate of success\", \"Measure information\"],\n",
      "𝐍: [\"Tasks\", \"Three diagnostic tasks\"],\n",
      "𝐎: [\"Word\", \"Predict\"],\n",
      "𝐏: [\"Sub\", \"Predict\"],\n",
      "𝐐: [\"Word & Sub\", \"Predict\"],\n",
      "𝑉: [\"Word\", \"Embedding\"],\n",
      "𝓅: [\"Representation\", \"Complete multi-dimensional embedding\"],\n",
      "𝓇: [\"Retain information\"],\n",
      "𝓈: [\"Probe tasks\", \"Adi et al. (2017), Conneau et al. (2018)\"],\n",
      "𝓉: [\"Train\", \"Distinct probe models\"],\n",
      "𝓊: [\"Transform\", \"Non-linear\"],\n",
      "𝓋: [\"Input vector\", \"Layer 1\"],\n",
      "𝓌: [\"Max-margin loss\", \"Optimize cosine similarity\"],\n",
      "𝓍: [\"Negative samples\", \"Five samples\"]\n",
      "}</s>\n",
      "1330 7078\n",
      "Actual total tokens till now: 8408\n",
      "INCORRECT DICTIONARY\n",
      "Iteration 11 of 19\n",
      "\n",
      "\n",
      "\n",
      "1482 prompt tokens counted.\n",
      "\n",
      "Do not include the angle brackets in your dictionary.\n",
      "ASSISTANT: Math identifiers = {\n",
      "\"𝐫\": \"Hidden states\",\n",
      "\"𝐢\": \"Bias term\",\n",
      "\"𝐮\": \"Activation function (tanh)\",\n",
      "\"W\": \"Weight matrix\",\n",
      "\"𝐚\": \"Lemmatized form\",\n",
      "\"𝐌\": \"Lexical substitution data\",\n",
      "\"𝐫\": \"Output representations\",\n",
      "\"𝐬\": \"Input representations\",\n",
      "\"𝐰\": \"Output representations\",\n",
      "\"𝐬\": \"Input representations\",\n",
      "\"𝐮\": \"Activation function (tanh)\",\n",
      "\"𝐭\": \"Training, validation, test data partitions\",\n",
      "\"𝐲: \"Scaled conjugate tanh\",\n",
      "\"𝐰\": \"Output representations\",\n",
      "\"𝐮\": \"Activation function (tanh)\",\n",
      "\"𝐸\": \"Test data\",\n",
      "\"𝐼\": \"Cosine similarity\"\n",
      "}</s>\n",
      "1554 8560\n",
      "Actual total tokens till now: 10114\n",
      "INCORRECT DICTIONARY\n",
      "Iteration 12 of 19\n",
      "\n",
      "\n",
      "\n",
      "1571 prompt tokens counted.\n",
      "\n",
      "Do not include the angle brackets in your dictionary.\n",
      "ASSISTANT: \n",
      "identifiers = {\n",
      "\"cosine scores\": \"Cosine similarities between targets and baseline representations\",\n",
      "\"10-word window\": \"Average embedding of word embeddings from a 10-word window around the target word\",\n",
      "\"unsupervised baselines\": \"Word embedding itself and average embedding of word embeddings from a 10-word window\",\n",
      "\"out-of-vocabulary words\": \"Excluded items\",\n",
      "\"punctuation\": \"Excluded items\",\n",
      "\"current hidden states\": \"Current hidden states\",\n",
      "\"predictive hidden states\": \"Hidden states for the next timestep\",\n",
      "\"probe tasks\": \"Evaluation tasks to measure the quality of the representations\",\n",
      "\"first cell\": \"Comparison with the same representation\",\n",
      "\"exceptions\": \"Models that don't meet or exceed the unsupervised baseline\",\n",
      "\"nearest neighbors\": \"Representations predicted in the first current hidden layer\",\n",
      "\"similarity\": \"Comparison of lexical and contextual vectors\",\n",
      "\"auto-encoder\": \"Decoder in an auto-encoder for reconstruction of input\",\n",
      "\"output layer\": \"Last hidden layer before the output\",\n",
      "\"Word task\": \"Task specific to word embeddings\",\n",
      "\"contextual task\": \"Task specific to contextually relevant information\"\n",
      "}</s>\n",
      "1852 10131\n",
      "Actual total tokens till now: 11983\n",
      "Iteration 13 of 19\n",
      "\n",
      "\n",
      "\n",
      "1834 prompt tokens counted.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2308 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do not include the angle brackets in your dictionary.\n",
      "ASSISTANT: \n",
      "identifiers = {\n",
      "\"embedding\": \"Representation of words\",\n",
      "\"lexical\": \"Lexical substituion data\",\n",
      "\"McCarthy-Navigli\": \"Lexical substitution data\",\n",
      "\"LexSub\": \"Large dataset by Kremer et al., 2014\",\n",
      "\"Ex.(1)\": \"Example sentence 1\",\n",
      "\"Ex.(2)\": \"Example sentence 2\",\n",
      "\"s\": \"Substitute words\",\n",
      "\"average embedding\": \"Union of substitute words and target word\",\n",
      "\"cosine similarity\": \"Cosine similarities between targets and baseline representations\",\n",
      "\"10-word window\": \"Average embedding of word embeddings from a 10-word window around the target word\",\n",
      "\"unsupervised baselines\": \"Word embedding itself and average embedding of word embeddings from a 10-word window\",\n",
      "\"out-of-vocabulary words\": \"Excluded items\",\n",
      "\"reconstructing\": \"Decoder in an auto-encoder for reconstruction of input\",\n",
      "\"output layer\": \"Last hidden layer before the output\",\n",
      "\"Word task\": \"Task specific to word embeddings\",\n",
      "\"contextual task\": \"Task specific to contextually relevant information\",\n",
      "\"high performance\": \"High accuracy in reconstructing word embedding\",\n",
      "\"hidden layers\": \"Hidden states\",\n",
      "\"current layers\": \"Current hidden states\",\n",
      "\"predictive layers\": \"Hidden states for the next timestep\",\n",
      "\"retrieved\": \"Obtained from hidden states\",\n",
      "\"correlation\": \"Pearson's ρ\",\n",
      "\"ρ\": \"Pearson's ρ\",\n",
      "\"alignment\": \"Similarity between contextual and lexical representations\",\n",
      "\"ambiguity\": \"Morphosyntactic ambiguity\",\n",
      "\"challenges\": \"Semantic distinctions\",\n",
      "\"clearer contextual cues\": \"Easier to discriminate\",\n",
      "\"less represented\": \"Lower frequency usage\",\n",
      "\"diverge\": \"Different from word embedding\",\n",
      "\"rely\": \"Heavily dependent on word embedding\"\n",
      "}</s>\n",
      "2326 11965\n",
      "Actual total tokens till now: 14291\n",
      "Iteration 14 of 19\n",
      "\n",
      "New dictionary\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1282 prompt tokens counted.\n",
      "\n",
      "Do not include the angle brackets in your dictionary.\n",
      "ASSISTANT: math_identifiers = {\n",
      "\"a\": \"a\",\n",
      "\"v\": \"v\",\n",
      "\"g\": \"g\",\n",
      "\"ctxt\": \"context\",\n",
      "\"1\": \"1\",\n",
      "\"2\": \"2\",\n",
      "\"3\": \"3\",\n",
      "\"41\": \"41\",\n",
      "\"43\": \"43\",\n",
      "\"29\": \"29\",\n",
      "\"show\": \"show\",\n",
      "\"abstracts\": \"abstracts\",\n",
      "\"embedding\": \"embedding\",\n",
      "\"states\": \"states\",\n",
      "\"layers\": \"layers\",\n",
      "\"output\": \"output\",\n",
      "\"input\": \"input\",\n",
      "\"current\": \"current\",\n",
      "\"predicted\": \"predicted\",\n",
      "\"activations\": \"activations\",\n",
      "\"aspects\": \"aspects\",\n",
      "\"suggest\": \"suggest\",\n",
      "\"hypothesis\": \"hypothesis\",\n",
      "\"retrieve\": \"retrieve\",\n",
      "\"representation\": \"representation\",\n",
      "\"performance\": \"performance\",\n",
      "\"produced\": \"produced\",\n",
      "\"underspecified\": \"underspecified\",\n",
      "\"capture\": \"capture\",\n",
      "\"integrate\": \"integrate\",\n",
      "\"processing\": \"processing\",\n",
      "\"objective\": \"objective\",\n",
      "\"focus\": \"focus\",\n",
      "\"sense\": \"sense\",\n",
      "\"shed\": \"shed\",\n",
      "\"observation\": \"observation\",\n",
      "\"relationship\": \"relationship\",\n",
      "\"consequence\": \"consequence\",\n",
      "\"noun\": \"noun\",\n",
      "\"expect\": \"expect\",\n",
      "\"experiment\": \"experiment\",\n",
      "\"example\": \"example\",\n",
      "\"average\": \"average\",\n",
      "\"task\": \"task\",\n",
      "\"input-output\": \"input-output\",\n",
      "\"contingency\": \"contingency\",\n",
      "\"conducted\": \"conducted\",\n",
      "\"extrinsic\": \"extrinsic\",\n",
      "\"intrinsic\": \"intrinsic\",\n",
      "\"behavior\": \"behavior\",\n",
      "\"model\": \"model\",\n",
      "\"mechanism\": \"mechanism\",\n",
      "\"conclusion\": \"conclusion\",\n",
      "\"concept\": \"concept\",\n",
      "\"conduct\": \"conduct\",\n",
      "\"conjunction\": \"conjunction\",\n",
      "\"consequence\": \"consequence\",\n",
      "2838 13247\n",
      "Actual total tokens till now: 16085\n",
      "Iteration 15 of 19\n",
      "Iteration 16 of 19\n",
      "Iteration 17 of 19\n",
      "Iteration 18 of 19\n",
      "Iteration 19 of 19\n",
      "Iteration 20 of 19\n",
      "Iteration 21 of 19\n",
      "\n",
      "\n",
      "\n",
      "1718 prompt tokens counted.\n",
      "\n",
      "Do not include the angle brackets in your dictionary.\n",
      "ASSISTANT: \n",
      "identifiers = {\n",
      "\"tahn\": \"non-linearity\",\n",
      "\"Paszke\": \"author of reference\",\n",
      "\"i\": \"instance\",\n",
      "\"margin\": \"margin\",\n",
      "\"cos\": \"cosine function\",\n",
      "\"r\": \"result vector\",\n",
      "\"r^\": \"result vector\",\n",
      "\"L\": \"loss function\",\n",
      "\"embedding\": \"embedding\",\n",
      "\"1\": \"initial learning rate\",\n",
      "\"2\": \"batch size\",\n",
      "\"41\": \"hyperparameter\",\n",
      "\"3\": \"negative target\",\n",
      "\"43\": \"negative samples\",\n",
      "\"29\": \"training epoch\",\n",
      "\"1\": \"initial learning rate\",\n",
      "\"3\": \"batch size\",\n",
      "\"2\": \"hyperparameter\"\n",
      "}</s>\n",
      "3005 14965\n",
      "Actual total tokens till now: 17970\n",
      "Time taken: 0:02:51.939290\n",
      "Total time taken: 0:03:15.766819\n",
      "17970 3005 14965\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.now()\n",
    "number_of_dictionaries = 0\n",
    "for chunk in chunks:\n",
    "    print(f\"Iteration {i} of {len(chunks)}\")\n",
    "    i += 1\n",
    "    if chunk == question:\n",
    "        continue\n",
    "    if not contains_pattern(chunk):\n",
    "        continue\n",
    "    question = chunk\n",
    "    \n",
    "    if prompt_size > 1600:\n",
    "        number_of_dictionaries += 1\n",
    "        print(\"\\nNew dictionary\\n\")\n",
    "        dictionary.append({})\n",
    "    \n",
    "    prompt = [\n",
    "            {'role': 'system',\n",
    "             'content': 'You are a helpful research assistant tasked with converting long paragraphs into a JSON '\n",
    "                        'dictionary. '\n",
    "                        'The goal is to identify and classify each individual mathematical symbol, variable, '\n",
    "                        'and identifier in the text marked between \"<||>\"'\n",
    "                        'The dictionary should store the identifiers as keys and their corresponding definitions as '\n",
    "                        'values in an array format. '},\n",
    "            {'role': 'system', 'name': 'example_user', 'content': '''A relational model is a triple <|M|>′=(<|X|>,<|R|>,\n",
    "            <|v|>), where <|X|> is a set of states, <|R|><|⊆|><|X|><|×|><|X|> is a binary relation on <|X|>, \n",
    "            and <|v|>:<|𝖯𝗋𝗈𝗉|>→2<|X|> is a valuation. Given a relational model <|M|>′, the satisfaction relation \n",
    "            between points <|x<|∈<|X<| and formulas <|φ<|∈<|ℒ<|<|𝖪𝖠<| is defined inductively by <|M|>′,\n",
    "            <|x|>⊨<|𝖪|><|φ|>⇔ for all <|y|>∈<|X|>,<|x|><|R|><|y|> implies <|M|>′,<|y|>⊨<|φ|><|M|>′,\n",
    "            <|x|>⊨<|𝖠|><|φ|><||>⇔ for all <|y|>∈<|X|>,<|M|>′,<|y|>⊨<|φ|>'''},\n",
    "            {'role': 'system', 'name': 'example_assistant', 'content': '''identifiers = {\n",
    "            \"M\": [\"Model\", \"Expertise Model\"],\n",
    "            \"M'\": \"Relational model\",\n",
    "            \"X\": \"Set of states\",\n",
    "            \"R\": \"Binary relation on X\",\n",
    "            \"v\": \"Valuation\",\n",
    "            \"𝖯𝗋𝗈𝗉\": \"Set of propositions\",\n",
    "            \"M'\": \"Relational model\",\n",
    "            \"x\": \"Point in X\",\n",
    "            \"φ\": \"Formula in 𝖪𝖠\",\n",
    "            \"ℒ_{𝖪𝖠}\": \"Set of formulas\",\n",
    "            \"𝖪\": \"Modal operator K\",\n",
    "            \"𝖠\": \"Modal operator A\",\n",
    "            \"y\": \"Point in X\",\n",
    "            \"⊨\": \"Satisfaction relation\",\n",
    "            \"⇔\": \"If and only if operator\",\n",
    "            \"∈\": \"Element of a set\",\n",
    "            \"⊆\": \"Subset of a set\",\n",
    "            \"×\": \"Cartesian product operator\",\n",
    "            \"→\": \"Function or implication operator\",\n",
    "            \"for all\": \"Universal quantifier\"\n",
    "            }'''},\n",
    "            {'role': 'system',\n",
    "             'content': f'Given is already a pre existing dictionary. Your job is to extend this dictionary. Do not '\n",
    "                        f'remove any pre existing definitions from this dictionary.'\n",
    "                        f'\\n{dictionary[number_of_dictionaries]}. If there is nothing to mention, reply with an empty '\n",
    "                        f'dictionary'},\n",
    "            {'role': 'user', 'content': f'Generate a JSON dictionary for the following text: {question}. '\n",
    "                                        'Only consider the mathematical identifiers inside \"<||>\" for the dictionary. '\n",
    "                                        'Do not consider any other identifier other than those marked. '\n",
    "                                        'Consider all the identifiers individually. Do not skip any identifier, mention'\n",
    "                                        ' all the identifiers inside \"<||>\" in your dictionary. '\n",
    "                                        'Do not include the angle brackets in your dictionary.'}\n",
    "        ]\n",
    "    \n",
    "    open_prompt = get_prompt_loop(prompt)\n",
    "    \n",
    "    prompt_size = num_tokens_from_messages(open_prompt)\n",
    "    print(f\"\\n\\n\\n{prompt_size} prompt tokens counted.\\n\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            input_ids = tokenizer(open_prompt, return_tensors='pt').input_ids.cuda()\n",
    "            output = model.generate(inputs=input_ids, temperature=0.5, max_new_tokens=512, repetition_penalty=1.05)\n",
    "            output_string = tokenizer.decode(output[0])\n",
    "            \n",
    "            actual_total_tokens += num_tokens_from_messages(output_string)\n",
    "            completion_tokens += num_tokens_from_messages(output_string) - num_tokens_from_messages(open_prompt)\n",
    "            prompt_tokens += num_tokens_from_messages(open_prompt)\n",
    "\n",
    "            ind = output_string.index('Do not include the angle brackets in your dictionary.')\n",
    "            print(output_string[ind:])\n",
    "            \n",
    "            print(completion_tokens, prompt_tokens)\n",
    "    \n",
    "            print(f\"Actual total tokens till now: {actual_total_tokens}\")\n",
    "\n",
    "            new_dictionary = {}\n",
    "\n",
    "            try:\n",
    "                correct_output_string = modify_string(output_string[ind:])\n",
    "                dic_output_string = get_json_of_string(correct_output_string)\n",
    "                new_dictionary = string_to_dict(dic_output_string)\n",
    "            except Exception as e:\n",
    "                print(\"INCORRECT DICTIONARY\", e)\n",
    "            dictionary[number_of_dictionaries] = merge_dictionaries(dictionary[number_of_dictionaries], new_dictionary)\n",
    "            \n",
    "            break\n",
    "        except Exception as e:\n",
    "            number_of_dictionaries += 1\n",
    "            dictionary.append({})\n",
    "            print(f\"Exception occurred: {e}\")\n",
    "            print(\"Retrying...\")\n",
    "total_time_taken += (datetime.now() - start_time)\n",
    "print(f\"Time taken: {datetime.now() - start_time }\")\n",
    "print(f\"Total time taken: {total_time_taken}\")\n",
    "print(actual_total_tokens, completion_tokens, prompt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04278ee2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dct = {}\n",
    "for dic in dictionary:\n",
    "    dct = merge_dictionaries(dct, dic)\n",
    "# pprint.pprint(dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d4212dbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'LM': '['Language Model', 'Bidirectional LSTM']'\n",
      "'t': '['Timestep', ['Timestep', 'Target word']]'\n",
      "'𝐺': '['Hidden states', 'Hidden states']'\n",
      "'h': '['Hidden states', 'Human subjects']'\n",
      "'a': '['Activations', 'Annotated words', 'a']'\n",
      "'v': '['Vector representations', 'v']'\n",
      "'λ': '['Lexical substituion data', 'Lexical substitution data']'\n",
      "'<>': '['Average embedding', 'Union of substitute words and target word', 'Average embedding of substitute words', 'Cosine similarity', 'Top-10 neighbors', 'Training data', 'Words in context', 'Paraphrases', 'Evaluation benchmark', 'Simple vector operations']'\n",
      "'embedding': '['Representation of words', 'embedding']'\n",
      "'1': '['1', ['initial learning rate', 'initial learning rate']]'\n",
      "'2': '['2', ['batch size', 'hyperparameter']]'\n",
      "'3': '['3', ['negative target', 'batch size']]'\n",
      "'41': '['41', 'hyperparameter']'\n",
      "'43': '['43', 'negative samples']'\n",
      "'29': '['29', 'training epoch']'\n",
      "'consequence': '['consequence', 'consequence']'\n"
     ]
    }
   ],
   "source": [
    "for key, value in dct.items():\n",
    "    if type(value) == list:\n",
    "        print(f\"'{key}': '{value}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed252bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_list(input_list):\n",
    "    output_list = []\n",
    "    for i in input_list:\n",
    "        if isinstance(i, list):\n",
    "            output_list.extend(flatten_list(i))\n",
    "        else:\n",
    "            output_list.append(i)\n",
    "    return output_list\n",
    "\n",
    "\n",
    "def remove_duplicates(input_list):\n",
    "    output_list = []\n",
    "    for item in input_list:\n",
    "        if item not in output_list:\n",
    "            output_list.append(item)\n",
    "    if len(output_list) == 1:\n",
    "        return output_list[0]\n",
    "    return output_list\n",
    "\n",
    "\n",
    "def process_value(v):\n",
    "    if isinstance(v, str):\n",
    "        new_v = v.replace('$', '')\n",
    "        while '\\\\\\\\' in new_v:\n",
    "            new_v = new_v.replace('\\\\\\\\', '\\\\').replace('\\n', '')\n",
    "    else:  # Assuming it's a list\n",
    "        new_v = flatten_list([process_value(val) for val in v])\n",
    "        \n",
    "    return remove_duplicates(new_v) if isinstance(new_v, list) else new_v\n",
    "\n",
    "\n",
    "def reduce_pairs(dictionary):\n",
    "    new_dict = {}\n",
    "    for k, v in dictionary.items():\n",
    "        # reduce key backslashes\n",
    "        new_k = k.replace('$', '')\n",
    "        while '\\\\\\\\' in new_k:\n",
    "            new_k = new_k.replace('\\\\\\\\', '\\\\')\n",
    "\n",
    "        # process value\n",
    "        new_v = process_value(v)\n",
    "\n",
    "        new_dict[new_k] = new_v\n",
    "\n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "85c8083f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dict_without_backslashes = reduce_pairs(dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ab472ef2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pprint.pprint(dict_without_backslashes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "09b426f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_json = dict_without_backslashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0dec9c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{file_code}_mcdict.json', 'r', encoding='utf-8') as f:\n",
    "    mc_dict_original = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4284b146",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to create a hex code (a binary representation of the key)\n",
    "def get_hex_code(key):\n",
    "    return key.encode().hex()\n",
    "\n",
    "mc_dict_original['_author'] = model_name_or_path\n",
    "\n",
    "# Iterate over your dictionary and fill the new one\n",
    "for key, values in parsed_json.items():\n",
    "    # Determine the base key and the affix\n",
    "    base_key = re.match(r\"^[^*'_^,(\\[]*\", key).group()\n",
    "    affix = key[len(base_key):]\n",
    "\n",
    "    hex_code = get_hex_code(base_key)\n",
    "    values = values if isinstance(values, list) else [values]\n",
    "\n",
    "    if hex_code in mc_dict_original[\"concepts\"]:\n",
    "        k = list(mc_dict_original[\"concepts\"][hex_code][\"identifiers\"].keys())[0]\n",
    "        new_identifier = []\n",
    "        for value in values:\n",
    "            mc_dict_original[\"concepts\"][hex_code][\"identifiers\"][k].append({\n",
    "                \"affixes\": [affix] if affix else [],\n",
    "                \"arity\": 0,\n",
    "                \"description\": value\n",
    "            })\n",
    "    else:\n",
    "        if hex_code not in mc_dict_original[\"concepts\"]:\n",
    "            mc_dict_original[\"concepts\"][hex_code] = {\n",
    "                \"_surface\": {\n",
    "                    \"text\": base_key,\n",
    "                    \"unicode_name\": base_key if len(base_key) != 1 else unicodedata.name(base_key)\n",
    "                },\n",
    "                \"identifiers\": {\n",
    "                    'default': []\n",
    "                }\n",
    "            }\n",
    "\n",
    "        for value in values:\n",
    "            mc_dict_original[\"concepts\"][hex_code][\"identifiers\"][\"default\"].append({\n",
    "                \"affixes\": [affix] if affix else [],\n",
    "                \"arity\": 0,\n",
    "                \"description\": value\n",
    "            })\n",
    "\n",
    "\n",
    "# Convert new dictionary to a sorted dictionary\n",
    "sorted_dict = dict(sorted(mc_dict_original[\"concepts\"].items(), key=lambda x: (len(x[0]), x[0])))\n",
    "mc_dict_original[\"concepts\"] = sorted_dict\n",
    "\n",
    "# Convert new dictionary to JSON\n",
    "json_str = json.dumps(mc_dict_original, indent=4, ensure_ascii=False)\n",
    "\n",
    "#print(json_str)\n",
    "\n",
    "with open(f'{file_code}-{model_name}_mcdict.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(mc_dict_original, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a79ec2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_from_tags(element):\n",
    "    if isinstance(element, NavigableString):\n",
    "        return element\n",
    "    if element.name == 'mi':\n",
    "        return str(element)\n",
    "    return ''.join(get_text_from_tags(child) for child in element.children)\n",
    "\n",
    "def parse_html(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as html_file:\n",
    "        soup = BeautifulSoup(html_file, 'html.parser')\n",
    "\n",
    "    texts = get_text_from_tags(soup)\n",
    "    return texts\n",
    "\n",
    "def find_mi_strings(text):\n",
    "    pattern = r'(<mi.*?</mi>)'\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "    return matches\n",
    "\n",
    "# call the function with your HTML file path\n",
    "page = parse_html(f'{file_code}.html')\n",
    "matches = find_mi_strings(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "64a0cba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_dict = mc_dict_original\n",
    "with open(f'{file_code}_anno.json', encoding='utf-8') as fp:\n",
    "    parsed_annotation = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "163356dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_index_from_char_index(message, key, char_index):\n",
    "    i = 0\n",
    "    index = -1\n",
    "    for word in message:\n",
    "        if key in word:\n",
    "            index = i\n",
    "        i += 1\n",
    "    return index\n",
    "\n",
    "def expand_string_to_tokens(message, index, num_tokens_right=25, num_tokens_left=75):\n",
    "    words = message.split()  # Split the message into words\n",
    "\n",
    "    # Start at the index where the center word is\n",
    "    left_index = right_index = index\n",
    "\n",
    "    tokens_counter_right = num_tokens_from_messages(words[right_index])\n",
    "    tokens_counter_left = num_tokens_from_messages(words[left_index])\n",
    "\n",
    "    # Expand to the left from the center index until you reach num_tokens_left\n",
    "    while tokens_counter_left < num_tokens_left and left_index > 0:\n",
    "        left_index -= 1\n",
    "        tokens_counter_left += num_tokens_from_messages(words[left_index])\n",
    "\n",
    "    # Expand to the right from the center index until you reach num_tokens_right\n",
    "    while tokens_counter_right < num_tokens_right and right_index < len(words) - 1:\n",
    "        right_index += 1\n",
    "        tokens_counter_right += num_tokens_from_messages(words[right_index])\n",
    "\n",
    "    # Combine the words back into a string and return\n",
    "    return ' '.join(words[left_index:right_index + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "01b6b971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_text(text, replacement, exception):\n",
    "    # Find all matches\n",
    "    matches = re.findall(r'<mi(.*?)</mi>', text)\n",
    "    \n",
    "    for match in matches:\n",
    "        original_string = f'<mi{match}</mi>'\n",
    "        \n",
    "        # Skip exception\n",
    "        if original_string == exception:\n",
    "            continue\n",
    "        \n",
    "        # Replace match\n",
    "        replaced_string = re.sub(r'<.*?>(.*?)</.*?>', r'\\1', original_string)\n",
    "        text = text.replace(original_string, replaced_string)\n",
    "\n",
    "    return text\n",
    "\n",
    "def get_context(match):\n",
    "    match_len = len(match)\n",
    "    new_page = replace_text(page, '', match)\n",
    "    char_index = new_page.index(match) + int(match_len/2)\n",
    "    word_index = get_word_index_from_char_index(new_page, char_index)\n",
    "    section = expand_string_to_tokens(new_page, word_index)\n",
    "    section = re.sub(r'<.*?>(.*?)</.*?>', r'<<\\1>>', section)\n",
    "    return match, section\n",
    "\n",
    "# Function to create a hex code (a binary representation of the key)\n",
    "def get_hex_code(key):\n",
    "    return key.encode('utf-8').hex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7706ec86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_trailing_tags(s):\n",
    "    parts = re.split('(<mi)', s)\n",
    "    for i in range(1, len(parts), 2):\n",
    "        if '>' not in parts[i + 1]:\n",
    "            parts[i] = ''\n",
    "            parts[i + 1] = ''\n",
    "    return ''.join(parts)\n",
    "\n",
    "def get_definition_of_id(dict_id, identifier):\n",
    "    \n",
    "    try:\n",
    "        hex_code = get_hex_code(identifier)\n",
    "        index = parsed_annotation['mi_anno'][dict_id]['concept_id']\n",
    "        key = list(parsed_dict['concepts'][hex_code]['identifiers'].keys())[0]\n",
    "        return f\"({parsed_dict['concepts'][hex_code]['identifiers'][key][index]['description']})\"\n",
    "    except Exception as e:\n",
    "        return \"\"\n",
    "\n",
    "def get_context(match):\n",
    "    key_word = page.index(match) + len(match)\n",
    "    last_index = min(len(page), key_word + 500)\n",
    "    first_index = max(0, key_word - 3000)\n",
    "    context_window = page[first_index:last_index]\n",
    "    \n",
    "    reg_matches = re.findall(r'<mi(.*?)</mi>', context_window)\n",
    "    \n",
    "    identifier = None\n",
    "    \n",
    "    for reg_match in reg_matches:\n",
    "        original_string = f'<mi{reg_match}</mi>'\n",
    "        soup = BeautifulSoup(original_string, 'html.parser')\n",
    "        \n",
    "        \n",
    "        tags = soup.find_all('mi')\n",
    "        \n",
    "        if original_string == match:\n",
    "            identifier = tags[0].text\n",
    "            continue\n",
    "\n",
    "        context_window = context_window.replace(original_string,\n",
    "                                                f\"{tags[0].text}{get_definition_of_id(tags[0].get('id'), tags[0].text)}\")\n",
    "    \n",
    "    context_window = re.sub(r'<mi.*?>(.*?)<\\/mi>', r'<<\\1>>', context_window)\n",
    "    \n",
    "    context_window = remove_trailing_tags(context_window)\n",
    "    context_window = re.sub(r'^(?!.*<mi.*).*<\\/mi>', '', context_window, flags=re.DOTALL)\n",
    "        \n",
    "    index = 0\n",
    "    for word in context_window.split():\n",
    "        if f\"<<{identifier}>>\" in word:\n",
    "            word_index = index\n",
    "        index += 1\n",
    "    \n",
    "    if word_index == -1:\n",
    "        return context_window\n",
    "    else:\n",
    "        context_window = expand_string_to_tokens(context_window, word_index)\n",
    "    return context_window\n",
    "\n",
    "#print(get_context('<mi id=\"S1.p2.1.m1.1.1.3\" xref=\"S1.p2.1.m1.1.1.3.cmml\">φ</mi>'))\n",
    "#get_context('<mi id=\"S1.p2.1.m1.1.1.2\" xref=\"S1.p2.1.m1.1.1.2.cmml\">𝖤</mi>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "62ec5036-7177-46d0-9728-571edb9f6bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_anno(prompt):\n",
    "    return f'''SYSTEM: {prompt[0]['content']}\n",
    "USER: {prompt[1]['content']}\n",
    "ASSISTANT:'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f57696ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 of 164: ASSISTANT: 1</s>\n",
      "6 269\n",
      "1\n",
      "Iteration 2 of 164: 0\n",
      "Iteration 3 of 164: ASSISTANT: 1</s>\n",
      "12 531\n",
      "1\n",
      "Iteration 4 of 164: ASSISTANT: 1</s>\n",
      "18 795\n",
      "1\n",
      "Iteration 5 of 164: ASSISTANT: 1</s>\n",
      "24 1055\n",
      "1\n",
      "Iteration 6 of 164: ASSISTANT: Based on the provided text and possible annotations, the most fitting description for the identifier \"t\" (considering the potential affix \"+\") is:\n",
      "\n",
      "index: 1, identifier: t, description: Target word</s>\n",
      "74 1315\n",
      "1\n",
      "Iteration 7 of 164: 0\n",
      "Iteration 8 of 164: 0\n",
      "Iteration 9 of 164: ASSISTANT: Based on the provided text, the most fitting description for the identifier <<t>> is <<1>>, which corresponds to the \"Target word\". In this context, the time step is not relevant, and the focus is on the target word in the sequence. Therefore, the index to be selected is <<1>>.</s>\n",
      "141 1579\n",
      "1\n",
      "Iteration 10 of 164: 0\n",
      "Iteration 11 of 164: 0\n",
      "Iteration 12 of 164: ASSISTANT: Based on the provided text, the most fitting description for the identifier <<t>> is >>>Timestep<<<.<<</s>\n",
      "170 1864\n",
      "'NoneType' object has no attribute 'group'\n",
      "Iteration 13 of 164: 0\n",
      "Iteration 14 of 164: ASSISTANT: Based on the provided text, the most fitting description for the identifier <<t>> is <<t>></s>\n",
      "194 2152\n",
      "'NoneType' object has no attribute 'group'\n",
      "Iteration 15 of 164: 0\n",
      "Iteration 16 of 164: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"t\" is index 1, which corresponds to \"Target word\".</s>\n",
      "226 2428\n",
      "1\n",
      "Iteration 17 of 164: 0\n",
      "Iteration 18 of 164: 0\n",
      "Iteration 19 of 164: 0\n",
      "Iteration 20 of 164: 0\n",
      "Iteration 21 of 164: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"t\" is index 1, which corresponds to the label \"Target word\".</s>\n",
      "260 2709\n",
      "1\n",
      "Iteration 22 of 164: 0\n",
      "Iteration 23 of 164: 0\n",
      "Iteration 24 of 164: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"t\" is index 1, which corresponds to the annotation \"Target word\".</s>\n",
      "294 3004\n",
      "1\n",
      "Iteration 25 of 164: 0\n",
      "Iteration 26 of 164: None\n",
      "Iteration 27 of 164: ASSISTANT: Based on the provided text and possible annotations, the most fitting description for the identifier <<t>> is index 1, which corresponds to \"Target word\".</s>\n",
      "329 3284\n",
      "1\n",
      "Iteration 28 of 164: None\n",
      "Iteration 29 of 164: None\n",
      "Iteration 30 of 164: 0\n",
      "Iteration 31 of 164: ASSISTANT: Based on the provided text, the most fitting description for the identifier <<t>> is <<Timestep>>, which corresponds to an index of </s>\n",
      "360 3563\n",
      "'NoneType' object has no attribute 'group'\n",
      "Iteration 32 of 164: 0\n",
      "Iteration 33 of 164: 0\n",
      "Iteration 34 of 164: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"t\" is 1, which corresponds to \"Target word\".</s>\n",
      "391 3842\n",
      "1\n",
      "Iteration 35 of 164: 0\n",
      "Iteration 36 of 164: 0\n",
      "Iteration 37 of 164: ASSISTANT: Based on the provided text and possible annotations, the most fitting description for the identifier <<t>> is index 1, which corresponds to the \"Target word\".</s>\n",
      "427 4100\n",
      "1\n",
      "Iteration 38 of 164: 0\n",
      "Iteration 39 of 164: 0\n",
      "Iteration 40 of 164: 0\n",
      "Iteration 41 of 164: 0\n",
      "Iteration 42 of 164: ASSISTANT: Based on the provided text, the most fitting description for the identifier <<t>> is <<t>>, which has an index of <<1>>.</s>\n",
      "460 4379\n",
      "1\n",
      "Iteration 43 of 164: 0\n",
      "Iteration 44 of 164: 0\n",
      "Iteration 45 of 164: 0\n",
      "Iteration 46 of 164: 0\n",
      "Iteration 47 of 164: ASSISTANT: Based on the provided text and possible annotations, the most fitting description for the identifier \"t\" is index 1, which corresponds to the label \"Target word\".</s>\n",
      "497 4683\n",
      "1\n",
      "Iteration 48 of 164: 0\n",
      "Iteration 49 of 164: 0\n",
      "Iteration 50 of 164: ASSISTANT: Based on the provided text and possible annotations, the most fitting description for the identifier \"t\" is index 1, which corresponds to the annotation \"Target word\".</s>\n",
      "534 4972\n",
      "1\n",
      "Iteration 51 of 164: 0\n",
      "Iteration 52 of 164: 0\n",
      "Iteration 53 of 164: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"t\" is index 1, which corresponds to the \"Target word\".</s>\n",
      "567 5264\n",
      "1\n",
      "Iteration 54 of 164: 0\n",
      "Iteration 55 of 164: 0\n",
      "Iteration 56 of 164: ASSISTANT: Based on the provided text and possible annotations, the most fitting description for the identifier <<t>> is index 1, which corresponds to the \"Target word\".</s>\n",
      "603 5545\n",
      "1\n",
      "Iteration 57 of 164: 0\n",
      "Iteration 58 of 164: 0\n",
      "Iteration 59 of 164: ASSISTANT: Based on the provided text and possible annotations, the most fitting description for the identifier <<t>> is index 1, which corresponds to the label \"Target word.\"</s>\n",
      "640 5822\n",
      "1\n",
      "Iteration 60 of 164: 0\n",
      "Iteration 61 of 164: 0\n",
      "Iteration 62 of 164: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"t\" is index 1, which corresponds to the \"Target word\".</s>\n",
      "673 6104\n",
      "1\n",
      "Iteration 63 of 164: 0\n",
      "Iteration 64 of 164: ASSISTANT: 1</s>\n",
      "679 6386\n",
      "1\n",
      "Iteration 65 of 164: ASSISTANT: 1</s>\n",
      "685 6640\n",
      "1\n",
      "Iteration 66 of 164: ASSISTANT: 1</s>\n",
      "691 6901\n",
      "1\n",
      "Iteration 67 of 164: ASSISTANT: 1</s>\n",
      "697 7163\n",
      "1\n",
      "Iteration 68 of 164: ASSISTANT: Based on the provided text and possible annotations, the most fitting description for the identifier \"t\" is index 1, which corresponds to \"Target word\".</s>\n",
      "732 7422\n",
      "1\n",
      "Iteration 69 of 164: ASSISTANT: 1</s>\n",
      "738 7681\n",
      "1\n",
      "Iteration 70 of 164: None\n",
      "Iteration 71 of 164: 0\n",
      "Iteration 72 of 164: None\n",
      "Iteration 73 of 164: 0\n",
      "Iteration 74 of 164: None\n",
      "Iteration 75 of 164: 0\n",
      "Iteration 76 of 164: ASSISTANT: Based on the provided text, the most fitting description for the identifier <<t>> is <<1>>, which corresponds to the \"Target word\">></s>\n",
      "771 7948\n",
      "1\n",
      "Iteration 77 of 164: None\n",
      "Iteration 78 of 164: 0\n",
      "Iteration 79 of 164: None\n",
      "Iteration 80 of 164: 0\n",
      "Iteration 81 of 164: 0\n",
      "Iteration 82 of 164: None\n",
      "Iteration 83 of 164: 0\n",
      "Iteration 84 of 164: None\n",
      "Iteration 85 of 164: None\n",
      "Iteration 86 of 164: None\n",
      "Iteration 87 of 164: None\n",
      "Iteration 88 of 164: 0\n",
      "Iteration 89 of 164: None\n",
      "Iteration 90 of 164: 0\n",
      "Iteration 91 of 164: None\n",
      "Iteration 92 of 164: None\n",
      "Iteration 93 of 164: None\n",
      "Iteration 94 of 164: None\n",
      "Iteration 95 of 164: None\n",
      "Iteration 96 of 164: None\n",
      "Iteration 97 of 164: None\n",
      "Iteration 98 of 164: None\n",
      "Iteration 99 of 164: 0\n",
      "Iteration 100 of 164: None\n",
      "Iteration 101 of 164: 0\n",
      "Iteration 102 of 164: None\n",
      "Iteration 103 of 164: 0\n",
      "Iteration 104 of 164: None\n",
      "Iteration 105 of 164: 0\n",
      "Iteration 106 of 164: None\n",
      "Iteration 107 of 164: ASSISTANT: 1</s>\n",
      "777 8231\n",
      "1\n",
      "Iteration 108 of 164: ASSISTANT: Based on the provided text and possible annotations, the most fitting description for the identifier \"v\" is 1, which corresponds to \"v\" itself.</s>\n",
      "811 8495\n",
      "1\n",
      "Iteration 109 of 164: 0\n",
      "Iteration 110 of 164: 0\n",
      "Iteration 111 of 164: 0\n",
      "Iteration 112 of 164: ASSISTANT: 1</s>\n",
      "817 8761\n",
      "1\n",
      "Iteration 113 of 164: 0\n",
      "Iteration 114 of 164: 0\n",
      "Iteration 115 of 164: ASSISTANT: Based on the provided text and possible annotations, the most fitting description for the identifier \"t\" is index 1, which corresponds to the \"Target word\".</s>\n",
      "853 9027\n",
      "1\n",
      "Iteration 116 of 164: 0\n",
      "Iteration 117 of 164: 0\n",
      "Iteration 118 of 164: None\n",
      "Iteration 119 of 164: 0\n",
      "Iteration 120 of 164: ASSISTANT: Based on the provided text and possible annotations, the most fitting description for the identifier <<t>> is >>>1<<, which corresponds to the index <<1>>.</s>\n",
      "889 9292\n",
      "1\n",
      "Iteration 121 of 164: 0\n",
      "Iteration 122 of 164: ASSISTANT: Based on the provided text and possible annotations, the most fitting description for the identifier \"t\" (timestep) is:\n",
      "\n",
      "index: 1\n",
      "identifier: t\n",
      "description: Target word\n",
      "\n",
      "So the index for the annotation is 1.</s>\n",
      "945 9553\n",
      "1\n",
      "Iteration 123 of 164: 0\n",
      "Iteration 124 of 164: 0\n",
      "Iteration 125 of 164: ASSISTANT: 1</s>\n",
      "951 9813\n",
      "1\n",
      "Iteration 126 of 164: 0\n",
      "Iteration 127 of 164: ASSISTANT: 3</s>\n",
      "957 10076\n",
      "3\n",
      "Iteration 128 of 164: 0\n",
      "Iteration 129 of 164: None\n",
      "Iteration 130 of 164: 0\n",
      "Iteration 131 of 164: None\n",
      "Iteration 132 of 164: 0\n",
      "Iteration 133 of 164: 0\n",
      "Iteration 134 of 164: None\n",
      "Iteration 135 of 164: 0\n",
      "Iteration 136 of 164: None\n",
      "Iteration 137 of 164: None\n",
      "Iteration 138 of 164: 0\n",
      "Iteration 139 of 164: ASSISTANT: Based on the provided text, the most fitting description for the identifier \"t\" is index 1, which corresponds to \"Target word\".</s>\n",
      "989 10358\n",
      "1\n",
      "Iteration 140 of 164: 0\n",
      "Iteration 141 of 164: Key does not exist in the dictionary of concepts  \n",
      "Iteration 142 of 164: ASSISTANT: 2</s>\n",
      "995 10635\n",
      "2\n",
      "Iteration 143 of 164: ASSISTANT: 1</s>\n",
      "1001 10891\n",
      "1\n",
      "Iteration 144 of 164: 0\n",
      "Iteration 145 of 164: 0\n",
      "Iteration 146 of 164: 0\n",
      "Iteration 147 of 164: ASSISTANT: 1</s>\n",
      "1007 11149\n",
      "1\n",
      "Iteration 148 of 164: 0\n",
      "Iteration 149 of 164: 0\n",
      "Iteration 150 of 164: 0\n",
      "Iteration 151 of 164: ASSISTANT: Based on the provided text and possible annotations, the most fitting description for the identifier <<r>> is <<result vector>>, which corresponds to the index '1'. The potential affix of the identifier could be <<^>>. Considering the affixes of the possible annotations, the index for the fitting description is '1'.\n",
      "\n",
      "Output: 1</s>\n",
      "1084 11407\n",
      "1\n",
      "Iteration 152 of 164: ASSISTANT: Based on the provided text and possible annotations, the most fitting description for the identifier <<r>> is the one with index '1', which corresponds to the result vector.</s>\n",
      "1121 11667\n",
      "1\n",
      "Iteration 153 of 164: 0\n",
      "Iteration 154 of 164: 0\n",
      "Iteration 155 of 164: ASSISTANT: Based on the provided text, the most fitting description for the identifier <<r>> is the one with index '1', which corresponds to the result vector.</s>\n",
      "1155 11939\n",
      "1\n",
      "Iteration 156 of 164: ASSISTANT: Based on the provided text and possible annotations, the most fitting description for the identifier <<r>> is index 1, which corresponds to the result vector.</s>\n",
      "1189 12232\n",
      "1\n",
      "Iteration 157 of 164: 0\n",
      "Iteration 158 of 164: Key does not exist in the dictionary of concepts  \n",
      "Iteration 159 of 164: 0\n",
      "Iteration 160 of 164: None\n",
      "Iteration 161 of 164: 0\n",
      "Iteration 162 of 164: ASSISTANT: Based on the provided text, the most fitting description for the identifier <<r>> is <<result vector>>, which corresponds to the annotation with index '1'.</s>\n",
      "1225 12516\n",
      "1\n",
      "Iteration 163 of 164: ASSISTANT: Based on the provided text and possible annotations, the most fitting description for the identifier <<r>> is index 1, which corresponds to the result vector.</s>\n",
      "1259 12794\n",
      "1\n",
      "Iteration 164 of 164: 0\n",
      "Annotation completed\n",
      "Time taken: 0:01:50.080838\n",
      "Total time taken: 0:01:50.080797\n",
      "14053 1259 12794\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.now()\n",
    "actual_total_tokens = 0\n",
    "completion_tokens = 0\n",
    "prompt_tokens = 0\n",
    "no_tags = 0\n",
    "no_keys = 0\n",
    "no_anno = 0\n",
    "i = 1\n",
    "for match in matches:\n",
    "    print(f\"Iteration {i} of {len(matches)}: \", end='')\n",
    "    i += 1\n",
    "    context = get_context(match)\n",
    "    match_variable = re.sub(r'<.*?>(.*?)</.*?>', r'\\1', match)\n",
    "    context_index = context.index(f\"<<{match_variable}>>\") + len(match_variable)\n",
    "    possible_affix = str(context[context_index+4:context_index+5]).replace(\"′\", \"'\")\n",
    "    soup = BeautifulSoup(match, 'html.parser')\n",
    "    mi_tag = soup.find('mi')\n",
    "    if mi_tag is not None and 'id' in mi_tag.attrs:\n",
    "        anno_id = mi_tag['id']\n",
    "    else:\n",
    "        print('TAG NOT FOUND', match)\n",
    "        no_tags += 1\n",
    "        continue\n",
    "    \n",
    "    hex_code = get_hex_code(match_variable)\n",
    "    if hex_code not in parsed_dict['concepts']:\n",
    "        match_variable = f\"{unidecode(match_variable)}\"\n",
    "        hex_code = get_hex_code(match_variable)\n",
    "        if hex_code not in parsed_dict['concepts']:\n",
    "            print(\"Key does not exist in the dictionary of concepts\", match_variable, hex_code)\n",
    "            no_keys += 1\n",
    "            continue\n",
    "    \n",
    "    if anno_id not in parsed_annotation['mi_anno']:\n",
    "        print(\"Annotation ID does not exist in annotation.json\", anno_id)\n",
    "        no_anno += 1\n",
    "        continue\n",
    "\n",
    "    k = list(parsed_dict[\"concepts\"][hex_code][\"identifiers\"].keys())[0]\n",
    "    mcdict = parsed_dict['concepts'][hex_code]['identifiers'][k]\n",
    "    \n",
    "    if len(mcdict) == 1:\n",
    "        parsed_annotation['mi_anno'][anno_id]['concept_id'] = 0\n",
    "        print('0')\n",
    "    elif len(mcdict) > 1:\n",
    "        prompt_mcdict = []\n",
    "\n",
    "        index = 0\n",
    "        for val in mcdict:\n",
    "            prompt_mcdict.append({'index': f\"{index}\", 'identifier': f\"{match_variable}{'' if len(val['affixes']) == 0 else val['affixes'][0]}\", 'description': val['description']})\n",
    "            index += 1\n",
    "            \n",
    "        prompt = [\n",
    "            {'role': 'system', 'content': 'You are a professional annotater API. Your job is to select a fitting annotation from a dictionary for a mathematical identifier.'},\n",
    "            {'role': 'user', 'content': f'''Given the following possible annotations:\\n```json\\n{prompt_mcdict}```.\n",
    "             Select the index for the most fitting description for the identifier <<{match_variable}>> from the following text.\n",
    "             The potential affix of the indentifier could be <<{possible_affix}>>. Take the affixes of the possible annotations into account.\n",
    "             Only return the value of the index and nothing else.\n",
    "             Do not add any explanation otherwise the API breaks.\n",
    "             The identifier has been marked with <<>>.\n",
    "             If you can't come up with an index, write 'None'\n",
    "             ```txt\n",
    "             {context}\n",
    "             ```'''}\n",
    "        ]\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                open_prompt = get_prompt_anno(prompt)\n",
    "                \n",
    "                input_ids = tokenizer(open_prompt, return_tensors='pt').input_ids.cuda()\n",
    "                output = model.generate(inputs=input_ids, temperature=0.5, max_new_tokens=512, repetition_penalty=1.05)\n",
    "                output_string = tokenizer.decode(output[0])\n",
    "                \n",
    "                actual_total_tokens += num_tokens_from_messages(output_string)\n",
    "                completion_tokens += num_tokens_from_messages(output_string) - num_tokens_from_messages(open_prompt)\n",
    "                prompt_tokens += num_tokens_from_messages(open_prompt)\n",
    "\n",
    "                ind = output_string.index('ASSISTANT:')\n",
    "                value = output_string[ind:]\n",
    "                print(value)\n",
    "                \n",
    "                print(completion_tokens, prompt_tokens)\n",
    "\n",
    "                try:\n",
    "                    index = int(int(re.search('\\d+', value).group()))\n",
    "                    print(index)\n",
    "                    parsed_annotation['mi_anno'][anno_id]['concept_id'] = index\n",
    "                except Exception as f:\n",
    "                    print(f)\n",
    "\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Exception occurred\\n{e}\")\n",
    "                print(\"Retrying...\")\n",
    "    else:\n",
    "        print('None')\n",
    "\n",
    "print('Annotation completed')\n",
    "\n",
    "    \n",
    "total_time_taken = (datetime.now() - start_time)\n",
    "print(f\"Time taken: {datetime.now() - start_time }\")\n",
    "print(f\"Total time taken: {total_time_taken}\")\n",
    "print(actual_total_tokens, completion_tokens, prompt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bfeab45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_annotation['_annotator'] = model_name_or_path\n",
    "with open(f'{file_code}-{model_name}_anno.json', 'w') as fp:\n",
    "    json.dump(parsed_annotation, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9180c363",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126\n"
     ]
    }
   ],
   "source": [
    "items = 0\n",
    "for key, value in parsed_annotation['mi_anno'].items():\n",
    "    if value['concept_id'] is not None:\n",
    "        #print(key, value)\n",
    "        items += 1\n",
    "print(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffe4c27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
